{"cells":[{"cell_type":"markdown","metadata":{"id":"j4Wf5bmyZ9sX"},"source":["# 📓 Data Preprocessing for KDD Cup\n","\n","Welcome to Day 2 of the practice!\n","\n","In this section, we will examine how the KDD Cup Crag dataset is structured and explore the types of questions it contains. Furthermore, we will check what kinds of search results are present in the dataset and ultimately understand why this is classified as a RAG (Retrieval-Augmented Generation) task.\n","\n","In addition, we will discuss how various elements within the dataset can be utilized in the RAG system we are going to build and proceed with the process of preprocessing the data accordingly.\n","\n","<img src=\"https://i.imgur.com/fTICSCN.png\" width=\"800\">\n","\n","This section is divided into three main stages:\n","\n","###I. Set Environment  \n","###II. Check Dataset  \n","###III. Check Search Results\n"]},{"cell_type":"markdown","metadata":{"id":"_MxLx7YMZ9sa"},"source":["## I. Set Environment\n","\n","The task in this section is to set up the various **environment configurations** necessary for the upcoming exercises. Unlike the tasks from Day 1, the content here is relatively more complex, so we have set aside dedicated time for preparation prior to the hands-on practice.\n","\n","Ultimately, we will build the RAG (Retrieval-Augmented Generation) system as outlined below. The goal of this section is to prepare the various elements needed to use this RAG system more efficiently and to understand why these elements are necessary.\n","\n","<img src=\"https://i.imgur.com/eOTAnVS.png\">\n","\n","To accomplish this task, the following steps will be taken.\n","<br/>\n","\n","#### 1. Install and Import Libraries\n","#### 2. Upload Dataset\n","#### 3. Check the Connection with the Knowledge Graph API Server\n"]},{"cell_type":"markdown","source":["### 1. Import Libraries and Setting GPU\n","\n","First, we will install and import the necessary libraries for this task.\n","\n","```Python\n","! pip install openai==1.55.3 blingfire --quiet\n","```\n","```Python\n","import os\n","os.environ[\"OPENAI_API_KEY\"] = \"sk-...\" #Insert your openai api key\n","\n","import openai\n","```"],"metadata":{"id":"yrx4kNH8nPSK"}},{"cell_type":"code","source":["### YOUR CODE HERE ###\n","\n","! pip install openai==1.55.3 blingfire --quiet"],"metadata":{"id":"Sjy4Ic6znhcu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### YOUR CODE HERE ###\n","\n","import os\n","os.environ[\"OPENAI_API_KEY\"] = \"sk-...\" #Insert your openai api key\n","\n","import openai"],"metadata":{"id":"gD44b3YRnlRt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["You can check if the ₩OPENAI_API_KEY₩ is properly set as an environment variable by using the following command.\n","\n","```Python\n","! echo $OPENAI_API_KEY\n","```"],"metadata":{"id":"um2Ynqld5mIa"}},{"cell_type":"code","source":["### YOUR CODE HERE ###\n","\n","! echo $OPENAI_API_KEY"],"metadata":{"id":"0w6z6sud2nuW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 2. DownLoad Data\n","\n","This time, we will go through the process of downloading the dataset into Colab. While it is possible to upload files to Colab using a simple drag-and-drop method, this approach would take a very long time because the dataset size is 5 GB.\n","\n","Therefore, instead of uploading, you can add the CRAG dataset folder (which we have shared in advance) as a shortcut to your Google Drive. By doing so, you will be able to access the dataset directly from Colab. For detailed instructions, please refer to the diagram below.\n","\n","<img src=\"https://i.imgur.com/mP7O56L.png\">  \n","\n","<img src=\"https://i.imgur.com/aJu01H4.png\">\n","\n","By doing this, you will be able to access the CRAG dataset shared folder from your Google Drive. Since Google Colab does not allow access to folders outside of your own Drive, we have opted to move the dataset from the shared folder into your personal Drive for use.\n","\n","Now, we need to connect your Google Drive to Colab. This can be done using the following code:\n","\n","```Python\n","from google.colab import drive\n","\n","drive.mount('/content/drive')\n","```\n","\n","Once the mount is successfully completed, you can click on the folder icon on the left panel to verify that the CRAG dataset has been uploaded and is accessible.\n","\n","<img src=\"https://i.imgur.com/Qfwn9G8.png\" width=\"300\">\n"],"metadata":{"id":"rcPNw3hll6Ij"}},{"cell_type":"code","source":["### YOUR CODE HERE ###\n","\n","from google.colab import drive\n","\n","drive.mount('/content/drive')"],"metadata":{"id":"6XDRv9mcigV7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["If there are no issues with mounting, you should be able to see that the `crag_task_1_dev_v4_release.jsonl.bz2` file has been uploaded.\n","\n","The dataset is composed of jsonl files, which stands for JSON Lines. It is a text-based file format that stores structured data in a way similar to JSON, but it uses newline characters to separate individual JSON values. This means that each line in the file contains a single structured data record.\n","\n","Now, let’s look at an example to ensure that the CRAG dataset has been properly uploaded. The CRAG dataset is structured in JSON format, where each JSON object should contain both a question and an answer.\n","\n","To improve readability, let’s format and print the JSON content for better visibility. Here’s an example code snippet:\n","\n","```Python\n","import json\n","import bz2\n","\n","def pretty_json_print(data):\n","    json_string = json.dumps(data, indent=4)\n","    lines = json_string.splitlines()\n","    formatted_lines = \"\\n\\n\".join(lines)\n","    print(formatted_lines)\n","```\n","\n","```Python\n","file_path = '/content/drive/MyDrive/CRAG dataset/crag_task_1_dev_v4_release.jsonl.bz2'\n","\n","with bz2.open(file_path, 'rt') as file:\n","    for line in file:\n","        try:\n","            data = json.loads(line.strip())\n","            pretty_json_print(data)\n","        except json.JSONDecodeError as e:\n","            print(f\"Error decoding JSON: {e}\")\n","        break\n","```"],"metadata":{"id":"c_xPguxFn2Wa"}},{"cell_type":"markdown","source":["If you are unsure about the current path, you can use the following command to check it or directly locate the `crag_task_1_dev_v4_release.jsonl` file as shown in the diagram:\n","\n","```Python\n","! pwd\n","```\n","\n","<img src=\"https://i.imgur.com/BQ3RPoa.png\" width=300>\n"],"metadata":{"id":"Emq0XlsZGNxj"}},{"cell_type":"code","source":["! pwd"],"metadata":{"id":"L5RaLfejg_2U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### YOUR CODE HERE ###\n","\n","import json\n","import bz2\n","\n","def pretty_json_print(data):\n","    json_string = json.dumps(data, indent=4)\n","    lines = json_string.splitlines()\n","    formatted_lines = \"\\n\\n\".join(lines)\n","    print(formatted_lines)"],"metadata":{"id":"AwR3Lel_Iepm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### YOUR CODE HERE ###\n","\n","file_path = '/content/drive/MyDrive/CRAG dataset/crag_task_1_dev_v4_release.jsonl.bz2'\n","\n","with bz2.open(file_path, 'rt') as file:\n","    for line in file:\n","        try:\n","            data = json.loads(line.strip())\n","            pretty_json_print(data)\n","        except json.JSONDecodeError as e:\n","            print(f\"Error decoding JSON: {e}\")\n","        break"],"metadata":{"id":"sw7R2KdSpK4r"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Upon inspection, it was found that the data’s question is stored under the key query, while the corresponding answer is stored under the key answer.\n","\n","Additionally, there are several other elements that make up a single data point. A more detailed explanation of these components will be provided in the **Check Dataset** section."],"metadata":{"id":"IhLC4c1NJnfs"}},{"cell_type":"markdown","metadata":{"id":"k3W_CgXgZ9se"},"source":["### 3. Check the Connection with the Knowledge Graph API Server\n","\n","Now, let’s check how to connect to the Knowledge Graph (KG) via an API and verify that the connection is successful. This step is essential for querying the KG using the API later.\n","\n","First, let’s import the necessary libraries to confirm connectivity:\n","\n","```Python\n","import os\n","import requests\n","```"]},{"cell_type":"code","source":["### YOUR CODE HERE ###\n","\n","import os\n","import requests"],"metadata":{"id":"bCiKhRTsqxBu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Before verifying the connection, let me briefly explain how the Knowledge Graph is currently structured and why we need to access it through an API server.\n","\n","The Knowledge Graph we will use today is extremely large, making it very difficult to install and run directly in Colab. To address this, our teaching assistants have uploaded the required Knowledge Graph to a specific server for your convenience. Your task is to connect to this server and retrieve information from the Knowledge Graph.\n","\n","In this case, you will interact with the Knowledge Graph through a predefined API. Using APIs with specific functions, you will be able to obtain the corresponding information from the Knowledge Graph.  \n","<br/>\n","\n","<img src=\"https://i.imgur.com/WwvESuj.png\" width=500>\n","\n","<br/>\n","\n","The basic concept of this process is server and client. Server and client can be thought of as follows:  \n","\n","*   **Client**: The side that sends requests based on specific needs\n","*   **Server**: The side that receives the requests, processes them, and sends back a response\n","\n","In this situation, you are the client, using the API to interact with the Knowledge Graph we have set up and the API server that can process your requests through the Knowledge Graph. Therefore, it may be difficult for you to see the Knowledge Graph in its direct form.\n","\n","<br/>\n","\n","<img src=\"https://i.imgur.com/fxQ5sFF.png\" width=500>\n","\n","\n","Now, we will proceed to check if we can successfully connect to the server hosting the Knowledge Graph. Using the code below, we will verify if communication between the server and the client over the internet is possible.  \n","\n","```Python\n","external_server_url = \"http://x.x.x.x:port\" # given external url of KG api server\n","\n","try:\n","  response = requests.get(f\"{external_server_url}/\")\n","  if response.status_code == 200:\n","      print(response.json())\n","  else:\n","      print({\"error\": f\"Failed to connect. Status code: {response.status_code}\"})\n","except requests.exceptions.RequestException as e:\n","  print({\"error\": str(e)})\n","```"],"metadata":{"id":"nPADzZXZliSf"}},{"cell_type":"code","source":["### YOUR CODE HERE ###\n","\n","external_server_url = \"http://x.x.x.x:port\" # given external url of KG api server\n","\n","try:\n","  response = requests.get(f\"{external_server_url}/\")\n","  if response.status_code == 200:\n","      print(response.json())\n","  else:\n","      print({\"error\": f\"Failed to connect. Status code: {response.status_code}\"})\n","except requests.exceptions.RequestException as e:\n","  print({\"error\": str(e)})"],"metadata":{"id":"CYNRCtUJ4DPc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The URL and port used here represent the external IPv4 address of the server hosting the Knowledge Graph and the allowed port for access, respectively. To connect to the server, you need to know its location (IP address), and even with that information, proper access can only be achieved if the correct port is specified and controlled via firewall settings.\n","\n","Later, we will explain how to set up a Knowledge Graph in a virtual environment, check the IP address, and configure firewall settings so that you can perform this process yourself.  "],"metadata":{"id":"Zsv7Airahigf"}},{"cell_type":"markdown","source":["## II. Check Dataset\n","\n","Through the above process, we have confirmed the ability to upload and determine the usability of the dataset and API required for future exercises. Now, let’s specifically examine what the CRAG dataset contains and what types of questions it includes.\n","\n","First, let’s take a closer look at what the CRAG dataset is. The CRAG dataset is a factual question-answering benchmark consisting of question-answer pairs and mock APIs to simulate web and Knowledge Graph (KG) searches. In other words, it is fundamentally composed of questions and their corresponding answers.\n","\n","To understand how this data is structured, we will review randomly sampled examples. To make the dataset easier to work with, we will first convert it into a list of dictionaries for accessibility.\n","\n","```Python\n","file_path = '/content/drive/MyDrive/CRAG dataset/crag_task_1_dev_v4_release.jsonl'\n","\n","dataset = []\n","\n","with open(file_path, 'r', encoding='utf-8') as file:\n","    for line in file:\n","        try:\n","            data = json.loads(line.strip())\n","            dataset.append(data)\n","        except json.JSONDecodeError as e:\n","            print(f\"Error decoding JSON: {e}\")\n","```"],"metadata":{"id":"U8r7tRdutIFo"}},{"cell_type":"code","source":["### YOUR CODE HERE ###\n","\n","file_path = '/content/drive/MyDrive/CRAG dataset/crag_task_1_dev_v4_release.jsonl'\n","\n","dataset = []\n","\n","with open(file_path, 'r', encoding='utf-8') as file:\n","    for line in file:\n","        try:\n","            data = json.loads(line.strip())\n","            dataset.append(data)\n","        except json.JSONDecodeError as e:\n","            print(f\"Error decoding JSON: {e}\")"],"metadata":{"id":"ns8xqdNyvonp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This step may take some time (**1–5 minutes**).\n","\n","<img src=\"https://i.gifer.com/B6Qs.gif\" width=\"150\">\n"],"metadata":{"id":"WDqS6d1Ev2Yr"}},{"cell_type":"markdown","source":["A single data point follows the data schema outlined below.  \n","<br/>\n","\n","| Field Name             | Type          | Description                                                                                                                                                           |\n","|------------------------|---------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n","| `interaction_id`       | string        | A unique identifier for each example.                                                                                                                                |\n","| `query_time`           | string        | Date and time when the query and the web search occurred.                                                                                                            |\n","| `domain`               | string        | Domain label for the query. Possible values: \"finance\", \"music\", \"movie\", \"sports\", \"open\". \"Open\" includes any factual queries not among the previous four domains. |\n","| `question_type`        | string        | Type label about the query. Possible values include: \"simple\", \"simple_w_condition\", \"comparison\", \"aggregation\", \"set\", \"false_premise\", \"post-processing\", \"multi-hop\".      |\n","| `static_or_dynamic`    | string        | Indicates whether the answer to a question changes and the expected rate of change. Possible values: \"static\", \"slow-changing\", \"fast-changing\", and \"real-time\".    |\n","| `query`                | string        | The question for RAG to answer.                                                                                                                                       |\n","| `answer`               | string        | The gold standard answer to the question.                                                                                                                             |\n","| `alt_ans`  | list        | Other valid gold standard answers to the question.                                                                                                                    |\n","| `split`                | integer       | Data split indicator, where 0 is for validation and 1 is for the public test.                                                                                         |\n","| `search_results`       | list of JSON  | Contains up to `k` HTML pages for each query (`k=5` for Task #1 and `k=50` for Task #3), including page name, URL, snippet, full HTML, and last modified time.         |  \n","\n","<br/>\n","\n","From the schema, it is clear that this data point already includes search_results, which contain documents that can be retrieved when performing a query.\n","\n","The details of search_results are as follows:\n","\n","<br/>  \n","\n","\n","| Key                  | Type   | Description                                             |\n","|----------------------|--------|---------------------------------------------------------|\n","| `page_name`          | string | The name of the webpage.                                |\n","| `page_url`           | string | The URL of the webpage.                                 |\n","| `page_snippet`       | string | A short paragraph describing the major content of the page. |\n","| `page_result`        | string | The full HTML of the webpage.                           |\n","| `page_last_modified` | string | The time when the page was last modified.               |\n","\n","\n","<br/>\n","\n","We will revisit each element in detail later. What is important here is that, based on the above information, we can categorize data points into the following three classifications.  \n","\n","**1.   Domain**  \n","\n","**2.   Question Type**  \n","\n","**3.   Dynamism**\n","\n","Now, let’s examine the types of data present based on these elements.\n","\n","Additionally, since the CRAG dataset is designed for a question-answering task, we will use an LLM to check how it generates answers for the given questions. To achieve this, we will define a function that can pass questions to the LLM.  \n","\n","\n","```Python\n","def generate_answer(question):\n","    messages = [\n","        {\n","            \"role\": \"user\",\n","            \"content\": question,\n","        },\n","    ]\n","\n","    response = openai.chat.completions.create(\n","        model=\"gpt-3.5-turbo\",  \n","        messages=messages,\n","    )\n","\n","    return response.choices[0].message.content\n","```\n"],"metadata":{"id":"KBWXXU1CwXAb"}},{"cell_type":"code","source":["### YOUR CODE HERE ###\n","\n","def generate_answer(question):\n","    messages = [\n","        {\n","            \"role\": \"user\",\n","            \"content\": question,\n","        },\n","    ]\n","\n","    response = openai.chat.completions.create(\n","        model=\"gpt-3.5-turbo\",\n","        messages=messages,\n","    )\n","\n","    return response.choices[0].message.content"],"metadata":{"id":"skjciCQ5qzFa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 1. Domain\n","\n","First, let’s check what types of questions exist by domain. The CRAG dataset consists of a total of five question domains.  \n","\n","\n","```Python\n","unique_domains = {}\n","for item in dataset:\n","    if 'domain' in item:\n","        domain_value = item['domain']\n","        if domain_value not in unique_domains:\n","            unique_domains[domain_value] = item\n","\n","print(\"Unique domains and their example items:\\n\")\n","for domain, example_item in unique_domains.items():\n","    print(f\"Domain: {domain}\")\n","    question = example_item['query']\n","    answer = example_item['answer']\n","    print(f\"Example question: {question}\")\n","    print(f\"Example answer: {answer}\\n\")\n","```"],"metadata":{"id":"iTcP18oNvMQN"}},{"cell_type":"code","source":["### YOUR CODE HERE ###\n","\n","unique_domains = {}\n","for item in dataset:\n","    if 'domain' in item:\n","        domain_value = item['domain']\n","        if domain_value not in unique_domains:\n","            unique_domains[domain_value] = item\n","\n","print(\"Unique domains and their example items:\\n\")\n","for domain, example_item in unique_domains.items():\n","    print(f\"Domain: {domain}\")\n","    question = example_item['query']\n","    answer = example_item['answer']\n","    print(f\"Example question: {question}\")\n","    print(f\"Example answer: {answer}\\n\")"],"metadata":{"id":"eTI0cUhkwOLJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["By reviewing the examples, you will understand what it means for a question to belong to a specific domain.\n","\n","Now, let’s proceed to pass these questions to the LLM and verify whether the model generates accurate answers. If the LLM alone cannot produce the correct response, we will recognize the need for approaches like RAG to assist the LLM in answering effectively.\n","\n","In other words, this process can be viewed as a way to assess the difficulty level of the questions.  \n","\n","```Python\n","wanted_domain = 'movie' # you can change this domain as you want\n","\n","wanted_question = unique_domains[wanted_domain]['query']\n","wanted_answer = unique_domains[wanted_domain]['answer']\n","\n","gpt_answer = generate_answer(wanted_question)\n","\n","print(f\"Question: \\n{wanted_question}\\n\")\n","print(\"------------------------------------------------------------\")\n","print(f\"GPT's answer to question: \\n{gpt_answer}\\n\")\n","print(\"------------------------------------------------------------\")\n","print(f\"Real Answer: \\n{wanted_answer}\")\n","```"],"metadata":{"id":"a5p3pwD_28uG"}},{"cell_type":"code","source":["### YOUR CODE HERE ###\n","\n","wanted_domain = 'movie' # you can change this domain as you want\n","\n","wanted_question = unique_domains[wanted_domain]['query']\n","wanted_answer = unique_domains[wanted_domain]['answer']\n","\n","gpt_answer = generate_answer(wanted_question)\n","\n","print(f\"Question: \\n{wanted_question}\\n\")\n","print(\"------------------------------------------------------------\")\n","print(f\"GPT's answer to question: \\n{gpt_answer}\\n\")\n","print(\"------------------------------------------------------------\")\n","print(f\"Real Answer: \\n{wanted_answer}\")"],"metadata":{"id":"RgBxi2Ae28ct"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["By reviewing the examples, you will understand what it means for a question to belong to a specific domain.\n","\n","Now, let’s check how many questions are included in each domain and examine the data distribution.  \n","\n","```Python\n","import matplotlib.pyplot as plt\n","from collections import Counter\n","\n","domain_counts = Counter(item['domain'] for item in dataset if 'domain' in item)\n","\n","plt.figure(figsize=(10, 6))\n","plt.bar(domain_counts.keys(), domain_counts.values())\n","plt.xlabel('Domain')\n","plt.ylabel('Count')\n","plt.title('Number of Data Items per Domain')\n","plt.xticks(rotation=45, ha='right')\n","plt.tight_layout()\n","plt.show()\n","```"],"metadata":{"id":"Dr_1giQm031z"}},{"cell_type":"code","source":["### YOUR CODE HERE ###\n","\n","import matplotlib.pyplot as plt\n","from collections import Counter\n","\n","domain_counts = Counter(item['domain'] for item in dataset if 'domain' in item)\n","\n","plt.figure(figsize=(10, 6))\n","plt.bar(domain_counts.keys(), domain_counts.values())\n","plt.xlabel('Domain')\n","plt.ylabel('Count')\n","plt.title('Number of Data Items per Domain')\n","plt.xticks(rotation=45, ha='right')\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"0dWkyfE_yXWJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We are checking each domain with just one example, so it will give you more accurate insight if you check it with more diverse examples."],"metadata":{"id":"FCLimkaAyouz"}},{"cell_type":"markdown","source":["### 2. Question Type\n","\n","This time, we will extract examples for each question type to see what kinds of questions belong to each category.\n","\n","The question types are broadly divided into eight categories.  \n","\n","</br>\n","\n","<img src=\"https://i.imgur.com/bdabIYY.png\" width=600>\n","\n","<br/>\n","\n","```Python\n","unique_question_types = {}\n","for item in dataset:\n","    if 'question_type' in item:\n","        question_type = item['question_type']\n","        if question_type not in unique_question_types:\n","            unique_question_types[question_type] = item\n","\n","print(\"Unique question_type and their example items:\\n\")\n","for question_type, example_item in unique_question_types.items():\n","    print(f\"Question type: {question_type}\")\n","    question = example_item['query']\n","    answer = example_item['answer']\n","    print(f\"Example question: {question}\")\n","    print(f\"Example answer: {answer}\\n\")\n","```\n"],"metadata":{"id":"z44P9aUJ1LP9"}},{"cell_type":"code","source":["### YOUR CODE HERE ###\n","\n","unique_question_types = {}\n","for item in dataset:\n","    if 'question_type' in item:\n","        question_type = item['question_type']\n","        if question_type not in unique_question_types:\n","            unique_question_types[question_type] = item\n","\n","print(\"Unique question_type and their example items:\\n\")\n","for question_type, example_item in unique_question_types.items():\n","    print(f\"Question type: {question_type}\")\n","    question = example_item['query']\n","    answer = example_item['answer']\n","    print(f\"Example question: {question}\")\n","    print(f\"Example answer: {answer}\\n\")"],"metadata":{"id":"f71hHZ4rSSyr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["It would be good to check each question and experience what kind of question types there are. Also, as above, if you check how many data exist for each question type, you will know what question type to prepare for.\n","\n","\n","```Python\n","wanted_type = 'simple' # you can change this type as you want\n","\n","wanted_question = unique_question_types[wanted_type]['query']\n","wanted_answer = unique_question_types[wanted_type]['answer']\n","\n","gpt_answer = generate_answer(wanted_question)\n","\n","print(f\"Question: \\n{wanted_question}\\n\")\n","print(\"------------------------------------------------------------\")\n","print(f\"GPT's answer to question: \\n{gpt_answer}\\n\")\n","print(\"------------------------------------------------------------\")\n","print(f\"Real Answer: \\n{wanted_answer}\")\n","```\n","```Python\n","question_type_counts = Counter([item['question_type'] for item in dataset])\n","\n","plt.figure(figsize=(10, 6))\n","plt.bar(question_type_counts.keys(), question_type_counts.values())\n","plt.xlabel('Question Type')\n","plt.ylabel('Count')\n","plt.title('Number of Data Items per Question Type')\n","plt.xticks(rotation=45, ha='right')\n","plt.tight_layout()\n","plt.show()\n","```"],"metadata":{"id":"vsczfcTQUVNo"}},{"cell_type":"code","source":["### YOUR CODE HERE ###\n","\n","wanted_type = 'simple' # you can change this type as you want\n","\n","wanted_question = unique_question_types[wanted_type]['query']\n","wanted_answer = unique_question_types[wanted_type]['answer']\n","\n","gpt_answer = generate_answer(wanted_question)\n","\n","print(f\"Question: \\n{wanted_question}\\n\")\n","print(\"------------------------------------------------------------\")\n","print(f\"GPT's answer to question: \\n{gpt_answer}\\n\")\n","print(\"------------------------------------------------------------\")\n","print(f\"Real Answer: \\n{wanted_answer}\")"],"metadata":{"id":"UHew38_oARe-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### YOUR CODE HERE ###\n","\n","question_type_counts = Counter([item['question_type'] for item in dataset])\n","\n","plt.figure(figsize=(10, 6))\n","plt.bar(question_type_counts.keys(), question_type_counts.values())\n","plt.xlabel('Question Type')\n","plt.ylabel('Count')\n","plt.title('Number of Data Items per Question Type')\n","plt.xticks(rotation=45, ha='right')\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"OuMWFTBL9LIC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 3. Dynamism\n","\n","Dynamism refers to how frequently the answer to a question changes.  \n","\n","For example, a question like “What is the name of the first human to go to the moon?” has a static answer that does not change.   \n","In contrast, a question like “Who is the President of the United States?” can have an answer that changes relatively frequently.\n","\n","In this case, the faster the answer changes, the more difficult it may be for the LLM to generate an accurate response. Let’s analyze the dataset statistics to verify whether this expectation holds true.\n","\n","```Python\n","unique_dynamism = {}\n","for item in dataset:\n","    if 'static_or_dynamic' in item:\n","        question_type = item['static_or_dynamic']\n","        if question_type not in unique_dynamism:\n","            unique_dynamism[question_type] = item\n","\n","print(\"Unique dynamism and their example items:\\n\")\n","for question_type, example_item in unique_dynamism.items():\n","    print(f\"Dynamism: {question_type}\")\n","    question = example_item['query']\n","    answer = example_item['answer']\n","    print(f\"Example question: {question}\")\n","    print(f\"Example answer: {answer}\\n\")\n","```"],"metadata":{"id":"N04g56LQzlLh"}},{"cell_type":"code","source":["### YOUR CODE HERE ###\n","\n","unique_dynamism = {}\n","for item in dataset:\n","    if 'static_or_dynamic' in item:\n","        question_type = item['static_or_dynamic']\n","        if question_type not in unique_dynamism:\n","            unique_dynamism[question_type] = item\n","\n","print(\"Unique dynamism and their example items:\\n\")\n","for question_type, example_item in unique_dynamism.items():\n","    print(f\"Dynamism: {question_type}\")\n","    question = example_item['query']\n","    answer = example_item['answer']\n","    print(f\"Example question: {question}\")\n","    print(f\"Example answer: {answer}\\n\")"],"metadata":{"id":"ZLNzIix5tC8r"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Similarly, let’s ask the LLM example questions and also examine the overall statistics to gain insights.\n","\n","```Python\n","wanted_type = 'static' # you can change this type as you want\n","\n","wanted_question = unique_dynamism[wanted_type]['query']\n","wanted_answer = unique_dynamism[wanted_type]['answer']\n","\n","gpt_answer = generate_answer(wanted_question)\n","\n","print(f\"Question: \\n{wanted_question}\\n\")\n","print(\"------------------------------------------------------------\")\n","print(f\"GPT's answer to question: \\n{gpt_answer}\\n\")\n","print(\"------------------------------------------------------------\")\n","print(f\"Real Answer: \\n{wanted_answer}\")\n","```\n","```Python\n","question_type_counts = Counter([item['static_or_dynamic'] for item in dataset])\n","\n","plt.figure(figsize=(10, 6))\n","plt.bar(question_type_counts.keys(), question_type_counts.values())\n","plt.xlabel('Dynamism')\n","plt.ylabel('Count')\n","plt.title('Number of Data Items per Question Type')\n","plt.xticks(rotation=45, ha='right')\n","plt.tight_layout()\n","plt.show()\n","```"],"metadata":{"id":"7O3nZyZu0XyV"}},{"cell_type":"code","source":["### YOUR CODE HERE ###\n","\n","wanted_type = 'static' # you can change this type as you want\n","\n","wanted_question = unique_dynamism[wanted_type]['query']\n","wanted_answer = unique_dynamism[wanted_type]['answer']\n","\n","gpt_answer = generate_answer(wanted_question)\n","\n","print(f\"Question: \\n{wanted_question}\\n\")\n","print(\"------------------------------------------------------------\")\n","print(f\"GPT's answer to question: \\n{gpt_answer}\\n\")\n","print(\"------------------------------------------------------------\")\n","print(f\"Real Answer: \\n{wanted_answer}\")"],"metadata":{"id":"eKH2oCv_tDb6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### YOUR CODE HERE ###\n","\n","question_type_counts = Counter([item['static_or_dynamic'] for item in dataset])\n","\n","plt.figure(figsize=(10, 6))\n","plt.bar(question_type_counts.keys(), question_type_counts.values())\n","plt.xlabel('Dynamism')\n","plt.ylabel('Count')\n","plt.title('Number of Data Items per Question Type')\n","plt.xticks(rotation=45, ha='right')\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"UW-_0dzkXAQN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Oltar1myZ9se"},"source":["## III. Check Search Results\n","\n","In this section, we will explore how to preprocess the various components that make up the dataset and modify the data accordingly.  \n","\n","Preprocessing is the process of extracting and preparing data so that it can be used effectively, especially when the data contains unnecessary content or includes information we do not need alongside the relevant information.\n","\n","In the data schema we examined earlier, there is both information of interest and irrelevant data. Furthermore, within the `search_results`, there is often a significant amount of unnecessary information mixed in with what we need. If we feed all of this information to the LLM, it may fail to produce the kind of answer we are looking for.  \n","\n","Therefore, in this section, we will understand the importance of this process and consider how to resolve these issues. Specifically, we will go through the following steps:\n","\n"," <br/>  \n","#### 1. Analysing the search result schema\n","#### 2. Understanding the reason why we need to parsing search result\n"]},{"cell_type":"markdown","source":["### 1. Analysing the search result schema\n","\n","In this section, we will examine the `search results` for each data point in the CRAG dataset in more detail. Since the search results are the data that the RAG will reference during retrieval, analyzing them more closely is expected to provide valuable insights for building the RAG system.\n","\n","Let’s start by examining a single data point as an example. While I will describe a pre-selected data point for explanation purposes, you can randomly extract data from the CRAG dataset using the code below. To focus on the search results, let’s print only the question, answer, and search results from the data.\n","\n","```Python\n","data_index = 2617\n","\n","example_data = dataset[2617]\n","#random_data = random.choice(dataset)\n","\n","print(example_data['query'])\n","print(example_data['answer'])\n","pretty_json_print(example_data['search_results'])\n","```"],"metadata":{"id":"9X0FDeNeFQyz"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Um0swyp_Z9sf"},"outputs":[],"source":["### YOUR CODE HERE ###\n","\n","data_index = 2617\n","\n","example_data = dataset[2617]\n","# random_data = random.choice(dataset)\n","\n","print(example_data['query'])\n","print(example_data['answer'])\n","pretty_json_print(example_data['search_results'])"]},{"cell_type":"markdown","source":["There are a total of five search results. These include information from Wikipedia as well as other web pages. Each search result may:  \n","\n","The schema for each search result is structured as follows:  \n","\n","</br>\n","\n","| Key                  | Type   | Description                                             |\n","|----------------------|--------|---------------------------------------------------------|\n","| `page_name`          | string | The name of the webpage.                                |\n","| `page_url`           | string | The URL of the webpage.                                 |\n","| `page_snippet`       | string | A short paragraph describing the major content of the page. |\n","| `page_result`        | string | The full HTML of the webpage.                           |\n","| `page_last_modified` | string | The time when the page was last modified.               |\n","\n","</br>\n","\n","From the schema above, the key pieces of information that are most relevant for the LLM are:\n","\n","`page_name`, `page_snippet`, `page_result`  \n"],"metadata":{"id":"8hccTzGW1AFY"}},{"cell_type":"markdown","metadata":{"id":"tLMEiSoxZ9sf"},"source":["### 2. Understanding the reason why we need to parsing search result\n","\n","From the above results, it appears that the snippet is highly information-intensive and efficient for use in RAG.\n","\n","However, there are cases where the snippet does not exist, and since it is a summary of the full paragraph, specific details may be lost. This can lead to problems where the LLM does not receive the necessary information.\n","\n","This can be verified through the example below.\n","\n","```Python\n","for page in example_data['search_results']:\n","  print(f\"Length of title: {len(page['page_name'])} \\n {page['page_name']}\\n\\n\")\n","  print(f\"Length of snippet: {len(page['page_snippet'])}\\n\")\n","  print(f\"Length of result: {len(page['page_result'])}\\n\")\n","  print(\"------------------------------\")\n","```"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AGjjL17TZ9sf"},"outputs":[],"source":["### YOUR CODE HERE ###\n","\n","for page in example_data['search_results']:\n","  print(f\"Length of title: {len(page['page_name'])} \\n {page['page_name']}\\n\\n\")\n","  print(f\"Length of snippet: {len(page['page_snippet'])}\\n\")\n","  print(f\"Length of result: {len(page['page_result'])}\\n\")\n","  print(\"------------------------------\")"]},{"cell_type":"markdown","source":["Therefore, we need to ensure that the RAG process can reference the contents of `page_result`.\n","\n","However, upon examining page_result, it becomes clear that it differs from natural language. The content in page_result is directly scraped HTML files from the respective websites. As a result, page_result contains not only the main text but also additional HTML code.\n","\n","Thus, we need to remove the HTML tags and extract only the text. However, the extracted text is often very long, so it must be split into appropriately sized chunks to make it easier to search later. Additionally, by splitting the text into chunks, we can efficiently retrieve only the relevant parts of the text, which makes this process an essential step in data preprocessing.\n","\n","The above process can be performed using the following code. To extract the required text from HTML, we can use the `BeautifulSoup` library.  \n","\n","\n","```Python\n","from bs4 import BeautifulSoup\n","from blingfire import text_to_sentences_and_offsets\n","\n","all_chunks = []\n","\n","for html_text in example_data['search_results']:\n","    soup = BeautifulSoup(html_text[\"page_result\"], features=\"lxml\")\n","    text = soup.get_text(\" \", strip=True)\n","    if not text:\n","        all_chunks.append(\"\")\n","    else:\n","        _, offsets = text_to_sentences_and_offsets(text)\n","\n","        chunks = []\n","\n","        for start, end in offsets:\n","            chunk = text[start:end][:4000]\n","            all_chunks.append(chunk)\n","\n","print(all_chunks[:1])\n","```"],"metadata":{"id":"O9pBTeo-2bji"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"OZIUi51lZ9sf"},"outputs":[],"source":["### YOUR CODE HERE ###\n","\n","from bs4 import BeautifulSoup\n","from blingfire import text_to_sentences_and_offsets\n","\n","all_chunks = []\n","\n","for html_text in example_data['search_results']:\n","    soup = BeautifulSoup(html_text[\"page_result\"], features=\"lxml\")\n","    text = soup.get_text(\" \", strip=True)\n","    if not text:\n","        all_chunks.append(\"\")\n","    else:\n","        _, offsets = text_to_sentences_and_offsets(text)\n","\n","        chunks = []\n","\n","        for start, end in offsets:\n","            chunk = text[start:end][:4000]\n","            all_chunks.append(chunk)\n","\n","print(all_chunks[:1])"]},{"cell_type":"markdown","source":["By running the code below to check the average length of the chunks, you will see that the size of the text information we need to handle has been significantly reduced. Moving forward, we will split the `search_results` in this manner and perform additional searches within these chunks.\n","\n","```Python\n","total_length = 0.0\n","total_num = 0.0\n","\n","for chunk in all_chunks:\n","  total_length += len(chunk)\n","  total_num += 1.0\n","\n","print(\"Average length of chunk: \", total_length/total_num)\n","```"],"metadata":{"id":"teZskMYtMq92"}},{"cell_type":"code","source":["### YOUR CODE HERE ###\n","\n","total_length = 0.0\n","total_num = 0.0\n","\n","for chunk in all_chunks:\n","  total_length += len(chunk)\n","  total_num += 1.0\n","\n","print(\"Average length of chunk: \", total_length/total_num)"],"metadata":{"id":"c_SmExH3Mamo"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3.11.4 ('agents')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"},"vscode":{"interpreter":{"hash":"7d153714b979d5e6d08dd8ec90712dd93bff2c9b6c1f0c118169738af3430cd4"}},"colab":{"provenance":[{"file_id":"https://github.com/truera/trulens/blob/main/trulens_eval/examples/quickstart/llama_index_quickstart.ipynb","timestamp":1711653872147}]}},"nbformat":4,"nbformat_minor":0}