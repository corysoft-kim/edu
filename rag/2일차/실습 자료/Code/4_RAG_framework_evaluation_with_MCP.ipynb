{"cells":[{"cell_type":"markdown","metadata":{"id":"TXSKbyvh2QaU"},"source":["#📓 RAG Framework Evaluation and Upgrade\n","\n","\n","<img src=\"https://i.imgur.com/fTICSCN.png\">\n","\n","In this exercise, you will have the opportunity to evaluate the various RAG components you have built earlier. In the previous Day 1 session, we only performed evaluation using the LLM. However, this time, we will utilize various evaluation methods commonly used in the NLP field.\n","\n","\n","### I. Evaluate RAG  \n","### II. Upgrade KG Query Stage with MCP\n","  \n","Okay. Now we know what we have to do for this final section.  \n","However, we need to know additional evaluation metric for CRAG dataset:\n","\n","`Exact Accuracy`  \n","`Accuracy`  \n","`Hallucination`   \n","`Missing`  \n","\n"]},{"cell_type":"markdown","source":["## 0. New evaluation metrics for CRAG dataset\n","\n","In most cases, datasets designed for specific tasks are presented along with **evaluation metrics** that can be used for performance evaluation. Similarly, the CRAG dataset provides evaluation metrics that should be used when measuring the performance of LLMs on this dataset.  \n","\n","Therefore, before proceeding with the evaluation, let’s first check which evaluation methods the creators of the CRAG dataset intended to use. Specifically, we will examine the four answer classification criteria they proposed, understand how these are evaluated, and clarify what each criterion means.  \n","\n","<br/>\n","\n","We follow the steps below:  \n","\n","#### 1. What is the new evaluation metrics for CRAG dataset?\n","#### 2. How to evalute RAG following new evaluation metrics?\n"],"metadata":{"id":"pGvoJusEMYuq"}},{"cell_type":"markdown","source":["### 1. What is the new evaluation metrics for CRAG dataset?\n","  \n","The creators of the CRAG dataset evaluated RAG based on the following four elements:\n","\n","<img src=\"https://i.imgur.com/0hxmPdi.png\">\n","\n","Simply put, they classified responses that were identical to their predefined answers as the most ideal case. Responses with similar meanings but containing minor errors were classified as the next most ideal case.\n","\n","The important point is that, under the CRAG dataset’s evaluation criteria, everything else is not simply classified as “incorrect.” Instead, the creators expect the LLM to admit when it does not know the answer. Incorrect answers are those containing errors, and the more these answers occur, the worse the model’s performance is considered. However, answers classified as Missing (indicating no answer) do not negatively or positively impact the model’s performance.\n","\n","By understanding these four classification criteria, you will gain valuable insights when analyzing evaluation results on the CRAG dataset.\n"],"metadata":{"id":"HrqPxwa4OOe8"}},{"cell_type":"markdown","source":["### 2. How to evalute RAG following new evaluation metrics?\n","  \n","Since the evaluation criteria mentioned above cannot be measured automatically, the evaluation must be conducted using an LLM. This can be done using methods similar to Trulens. However, as the evaluation results can vary depending on the prompt used, we will use the default prompt provided by the creators of the CRAG dataset.\n","\n","The evaluation prompt is as follows. Based on its content, we need to provide the LLM with the `question`, `model prediction`, and `ground truth answers`. The LLM will then generate a response by performing the evaluation according to the instructions.\n","\n","```Python\n","INSTRUCTIONS = \"\"\"\n","# Task:\n","You are given a Question, a model Prediction, and a list of Ground Truth answers, judge whether the model Prediction matches any answer from the list of Ground Truth answers. Follow the instructions step by step to make a judgement.\n","1. If the model prediction matches any provided answers from the Ground Truth Answer list, \"Accuracy\" should be \"True\"; otherwise, \"Accuracy\" should be \"False\".\n","2. If the model prediction says that it couldn't answer the question or it doesn't have enough information, \"Accuracy\" should always be \"False\".\n","3. If the Ground Truth is \"invalid question\", \"Accuracy\" is \"True\" only if the model prediction is exactly \"invalid question\".\n","# Output:\n","Respond with only a single JSON string with an \"Accuracy\" field which is \"True\" or \"False\".\n","\"\"\"\n","```\n","To help us understand this with a simple example, please install and import the following library:\n","\n","```Python\n","!pip install openai==1.55.3 --quiet\n","!pip install llama-index==0.12.2 --quiet\n","!pip install llama-index-embeddings-huggingface==0.4.0 --quiet\n","!pip install packaging==23.2 langchain nltk>=3.8.1 streamlit==1.35.0 watchdog kubernetes==26.1.0 --quiet\n","!pip install blingfire beautifulsoup4 sentence-transformers ray --quiet\n","!pip install scikit-learn --quiet\n","!pip install tqdm tiktoken --quiet\n","!pip uninstall numpy -y\n","!pip install numpy==2.0.2\n","!pip uninstall pandas scipy transformers -y\n","!pip install pandas scipy transformers --quiet\n","```"],"metadata":{"id":"6ouB4AuyGWsk"}},{"cell_type":"code","source":["### YOUR CODE HERE ###\n","\n","INSTRUCTIONS = \"\"\"\n","# Task:\n","You are given a Question, a model Prediction, and a list of Ground Truth answers, judge whether the model Prediction matches any answer from the list of Ground Truth answers. Follow the instructions step by step to make a judgement.\n","1. If the model prediction matches any provided answers from the Ground Truth Answer list, \"Accuracy\" should be \"True\"; otherwise, \"Accuracy\" should be \"False\".\n","2. If the model prediction says that it couldn't answer the question or it doesn't have enough information, \"Accuracy\" should always be \"False\".\n","3. If the Ground Truth is \"invalid question\", \"Accuracy\" is \"True\" only if the model prediction is exactly \"invalid question\".\n","# Output:\n","Respond with only a single JSON string with an \"Accuracy\" field which is \"True\" or \"False\".\n","\"\"\""],"metadata":{"id":"j1Ba1UdyofiA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### YOUR CODE HERE ###\n","\n","!pip install openai==1.55.3 --quiet\n","!pip install llama-index==0.12.2 --quiet\n","!pip install llama-index-embeddings-huggingface==0.4.0 --quiet\n","!pip install packaging==23.2 langchain nltk>=3.8.1 streamlit==1.35.0 watchdog kubernetes==26.1.0 --quiet\n","!pip install blingfire beautifulsoup4 sentence-transformers ray --quiet\n","!pip install scikit-learn --quiet\n","!pip install tqdm tiktoken --quiet\n","!pip uninstall numpy -y\n","!pip install numpy==2.0.2\n","!pip uninstall pandas scipy transformers -y\n","!pip install pandas scipy transformers --quiet"],"metadata":{"id":"lpCFzMHMK9rV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["You can drag-and-drop the import code file into the workspace. This will allow you to import the necessary functions from that file for this practice. However, please ensure that the file is in the same folder as the currently running code for the import to succeed.  \n","\n","\n","```Python\n","import os\n","os.environ[\"OPENAI_API_KEY\"] = \"sk-...\" #Insert your openai api key\n","\n","import openai\n","import json\n","import random\n","import bz2\n","from tqdm import tqdm\n","from import_function import LlamaIndexRetriever, Reader, KGQueryEngine\n","```\n"],"metadata":{"id":"prM0Bc25vzts"}},{"cell_type":"code","source":["### YOUR CODE HERE ###\n","\n","import os\n","os.environ[\"OPENAI_API_KEY\"] = \"sk-...\" #Insert your openai api key\n","\n","import openai\n","import json\n","import random\n","import bz2\n","from tqdm import tqdm\n","from import_function import LlamaIndexRetriever, Reader, KGQueryEngine"],"metadata":{"id":"zHI_1XMjoffG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now, to proceed with the evaluation, let’s mount your Google Drive as before to make the dataset accessible in Colab. Run the code below. Depending on your computer environment, this may take a little time.  \n","\n","```Pyhon\n","from google.colab import drive\n","\n","drive.mount('/content/drive')\n","```\n","```Pyhon\n","file_path = '/content/drive/MyDrive/CRAG dataset/crag_task_1_dev_v4_release.jsonl.bz2'\n","\n","dataset = []\n","\n","with bz2.open(file_path, 'rt') as file:\n","    for line in file:\n","        try:\n","            data = json.loads(line.strip())\n","            dataset.append(data)\n","            if len(dataset) > 500:\n","              break\n","        except json.JSONDecodeError as e:\n","            print(f\"Error decoding JSON: {e}\")\n","```"],"metadata":{"id":"6ljCv864h1Ak"}},{"cell_type":"code","source":["### YOUR CODE HERE ###\n","\n","from google.colab import drive\n","\n","drive.mount('/content/drive')"],"metadata":{"id":"kpzSrQ8khTDV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### YOUR CODE HERE ###\n","\n","file_path = '/content/drive/MyDrive/CRAG dataset/crag_task_1_dev_v4_release.jsonl.bz2'\n","\n","dataset = []\n","\n","with bz2.open(file_path, 'rt') as file:\n","    for line in file:\n","        try:\n","            data = json.loads(line.strip())\n","            dataset.append(data)\n","            if len(dataset) > 500:\n","              break\n","        except json.JSONDecodeError as e:\n","            print(f\"Error decoding JSON: {e}\")"],"metadata":{"id":"wlwx9IrniR8R"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Please note that, depending on your computer environment, this process might take some time.  \n","\n","<img src=\"https://i.gifer.com/B6Qs.gif\" width=\"150\">\n","\n","Thank you for your understanding.\n"],"metadata":{"id":"oLQ_G4XgvdqJ"}},{"cell_type":"markdown","source":["Next, to obtain the model prediction by asking the LLM a question, we will use the following simple code to generate a response:\n","\n","```Python\n","def generate_answer(user_prompt, system_prompt = \"You are a helpful assistant.\"):\n","    messages = [\n","        {\n","            \"role\": \"system\",\n","            \"content\": system_prompt\n","        },\n","        {\n","            \"role\": \"user\",\n","            \"content\": user_prompt,\n","        },\n","    ]\n","\n","    response = openai.chat.completions.create(\n","        model=\"gpt-3.5-turbo\",  \n","        messages=messages,\n","    )\n","\n","    return response.choices[0].message.content\n","```"],"metadata":{"id":"fSQrIM8pg16P"}},{"cell_type":"code","source":["### YOUR CODE HERE ###\n","\n","def generate_answer(user_prompt, system_prompt = \"You are a helpful assistant.\"):\n","    messages = [\n","        {\n","            \"role\": \"system\",\n","            \"content\": system_prompt\n","        },\n","        {\n","            \"role\": \"user\",\n","            \"content\": user_prompt,\n","        },\n","    ]\n","\n","    response = openai.chat.completions.create(\n","        model=\"gpt-3.5-turbo\",\n","        messages=messages,\n","    )\n","\n","    return response.choices[0].message.content"],"metadata":{"id":"vtUejo1HiYUL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now, let’s proceed with the evaluation. The evaluation will be conducted using randomly selected data points.\n","\n","```Python\n","random_data = random.choice(dataset)\n","\n","test_question = random_data['query']\n","test_answer = random_data['answer']\n","test_answer_candidate = random_data['alt_ans']\n","all_answers = [test_answer] + test_answer_candidate\n","\n","model_prediction = generate_answer(user_prompt=test_question)\n","```\n","```Python\n","context_template = f\"\"\"Question: {test_question}\n","List of Ground Truth answers: {all_answers}\n","Model Prediction: {model_prediction}\n","\"\"\"\n","\n","print(context_template)\n","\n","evaluation_result = generate_answer(user_prompt=context_template, system_prompt=INSTRUCTIONS)\n","\n","print(evaluation_result)\n","```"],"metadata":{"id":"PDAnfRGcjFm4"}},{"cell_type":"code","source":["### YOUR CODE HERE ###\n","\n","random_data = random.choice(dataset)\n","\n","test_question = random_data['query']\n","test_answer = random_data['answer']\n","test_answer_candidate = random_data['alt_ans']\n","all_answers = [test_answer] + test_answer_candidate\n","\n","model_prediction = generate_answer(user_prompt=test_question)"],"metadata":{"id":"D3sX7arBYmKy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### YOUR CODE HERE ###\n","\n","context_template = f\"\"\"Question: {test_question}\n","List of Ground Truth answers: {all_answers}\n","Model Prediction: {model_prediction}\n","\"\"\"\n","\n","print(context_template)\n","\n","evaluation_result = generate_answer(user_prompt=context_template, system_prompt=INSTRUCTIONS)\n","\n","print(evaluation_result)"],"metadata":{"id":"duUtRsdclc4u"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Once the LLM evaluation results are generated, we need a function to extract the relevant results. Using the parser below, we can extract the classification results effectively:\n","\n","```Python\n","def parse_response(response):\n","    try:\n","        response = response.lower()\n","        model_resp = json.loads(response)\n","        answer = -1\n","        if \"accuracy\" in model_resp and (\n","            (\n","              model_resp[\"accuracy\"] is True\n","            )\n","            or\n","            (\n","                isinstance(model_resp[\"accuracy\"], str)\n","                and model_resp[\"accuracy\"].lower() == \"true\"\n","            )\n","        ):\n","            answer = 1\n","        else:\n","            raise ValueError(f\"Could not parse answer from response: {model_resp}\")\n","\n","        return answer\n","    except:\n","        return -1\n","\n","test_text = \"\"\"{\n","    \"Accuracy\": \"True\"\n","}\"\"\"\n","\n","print(parse_response(test_text))\n","```"],"metadata":{"id":"_0M-z3sILAKk"}},{"cell_type":"code","source":["### YOUR CODE HERE ###\n","\n","def parse_response(response):\n","    try:\n","        response = response.lower()\n","        model_resp = json.loads(response)\n","        answer = -1\n","        if \"accuracy\" in model_resp and (\n","            (\n","              model_resp[\"accuracy\"] is True\n","            )\n","            or\n","            (\n","                isinstance(model_resp[\"accuracy\"], str)\n","                and model_resp[\"accuracy\"].lower() == \"true\"\n","            )\n","        ):\n","            answer = 1\n","        else:\n","            raise ValueError(f\"Could not parse answer from response: {model_resp}\")\n","\n","        return answer\n","    except:\n","        return -1\n","\n","test_text = \"\"\"{\n","    \"Accuracy\": \"True\"\n","}\"\"\"\n","\n","print(parse_response(test_text))"],"metadata":{"id":"_f_nvWPgLDwb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Here is the function that processes the LLM-generated response, converts it into JSON format, and checks for the accuracy attribute. If accuracy is true, it returns 1; otherwise, it returns -1.\n","\n","This function can be used to perform the evaluation.\n","\n","```Python\n","def CRAG_evaluation(question, ground_truth, prediction):\n","  context_template = f\"\"\"Question: {question}\n","  List of Ground Truth answers: {ground_truth}\n","  Model Prediction: {prediction}\n","  \"\"\"\n","  \n","  evaluation_result = generate_answer(user_prompt=context_template, system_prompt=INSTRUCTIONS)\n","\n","  eval_res = parse_response(evaluation_result)\n","\n","  return eval_res\n","```"],"metadata":{"id":"vT1tBWWULVg7"}},{"cell_type":"code","source":["### YOUR CODE HERE ###\n","\n","def CRAG_evaluation(question, ground_truth, prediction):\n","  context_template = f\"\"\"Question: {question}\n","  List of Ground Truth answers: {ground_truth}\n","  Model Prediction: {prediction}\n","  \"\"\"\n","\n","  evaluation_result = generate_answer(user_prompt=context_template, system_prompt=INSTRUCTIONS)\n","\n","  eval_res = parse_response(evaluation_result)\n","\n","  return eval_res"],"metadata":{"id":"Fk5g7mdlM9i6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"C8sCpV0D2QaY"},"source":["## I. Evaluate RAG\n","\n","So far, we have evaluated the performance of the retriever to determine which retriever can be more effective. While using the recall score as the evaluation metric makes it difficult to measure semantic similarity, we have seen that it is convenient for large-scale automatic evaluation.\n","\n","In this section, we aim to evaluate the RAG system using multiple approaches. After selecting the best retriever based on the method described above, we now need to integrate it with the Reader to build a complete RAG system and verify its overall performance.\n","We follow the steps below:  \n","\n","#### 1. Define RAG and Evaluation Metric\n","#### 2. Evaluate through CRAG Evaluation Method"]},{"cell_type":"markdown","source":["### 1. Define RAG\n","Now, we will define RAG class.\n","\n","\n","```Python\n","external_kg_server = \"http://x.x.x.x:port\"   #need to change\n","\n","class RAG:\n","    def __init__(self, server=None):\n","        self.retriever = LlamaIndexRetriever()\n","        self.kg_query_engine = KGQueryEngine(server=server)\n","        self.reader = Reader()\n","\n","    def retrieve(self, query, search_results, topk):\n","        retrieved_results = self.retriever.retrieve(query, search_results, topk)\n","\n","        kg_results = self.kg_query_engine.query(query)\n","\n","        combined_results = [kg_results]\n","        combined_results.extend(retrieved_results)\n","\n","        return combined_results\n","\n","    def generate_response(self, query, retrieved_results):\n","        answer = self.reader.generate_response(query, retrieved_results)\n","        return answer\n","\n","    def inference(self, query, search_results, topk):\n","        retrieved_results = self.retrieve(query, search_results, topk)\n","        answer = self.generate_response(query, retrieved_results)\n","        return {\n","            \"retrieved_results\": retrieved_results,\n","            \"answer\": answer\n","        }\n","\n","rag = RAG(server=external_kg_server)\n","```"],"metadata":{"id":"5BMJPmxtKQe2"}},{"cell_type":"code","source":["### YOUR CODE HERE ###\n","\n","external_kg_server = \"http://x.x.x.x:port\"   #need to change\n","\n","class RAG:\n","    def __init__(self, server=None):\n","        self.retriever = LlamaIndexRetriever()\n","        self.kg_query_engine = KGQueryEngine(server=server)\n","        self.reader = Reader()\n","\n","    def retrieve(self, query, search_results, topk):\n","        retrieved_results = self.retriever.retrieve(query, search_results, topk)\n","\n","        kg_results = self.kg_query_engine.query(query)\n","\n","        combined_results = [kg_results]\n","        combined_results.extend(retrieved_results)\n","\n","        return combined_results\n","\n","    def generate_response(self, query, retrieved_results):\n","        answer = self.reader.generate_response(query, retrieved_results)\n","        return answer\n","\n","    def inference(self, query, search_results, topk):\n","        retrieved_results = self.retrieve(query, search_results, topk)\n","        answer = self.generate_response(query, retrieved_results)\n","        return {\n","            \"retrieved_results\": retrieved_results,\n","            \"answer\": answer\n","        }\n","\n","rag = RAG(server=external_kg_server)"],"metadata":{"id":"0Ivpbmd2wQBi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###2. Evaluate through CRAG Evaluation Method\n","\n","This time, we will evaluate the results using the evaluation metrics proposed in the CRAG dataset. As explained at the very beginning of this session, the model’s predictions are categorized into four classes for evaluation purposes.\n","\n","####1.   **Perfect**: Correctly answers the question and contains no hallucination\n","####2.   **Acceptable**: Provide a useful answer to the question but may contain minor errors\n","####3.   **Missing**: The response is \"I don't know\", \"I'm sorry I can't find ...\".\n","####4.   **Incorrect**: The response provides wrong or irrelevant infromation to answer the question.\n","\n","The model predictions categorized above are then linearized in the following manner to evaluate the final performance of the RAG system.\n","\n","<img src=\"https://i.imgur.com/TDQ5eI4.png\">\n","\n","Here, let’s proceed to evaluate the validation set using the function we just defined.\n","\n","We designed our Graph RAG to handle only questions within the finance domain, so we will also select test dataset questions that belong to the finance domain.\n","\n","```Python\n","finance_test_dataset_ids = []\n","\n","for data in dataset:\n","    if data['domain'] == 'finance':\n","        finance_test_dataset_ids.append(data['interaction_id'])\n","\n","    if len(finance_test_dataset_ids) >= 10:\n","        break\n","```\n","\n","```Python\n","n_miss, n_correct, n_correct_exact = 0, 0, 0\n","\n","for data in tqdm(dataset):\n","  if data['interaction_id'] not in finance_test_dataset_ids:\n","    continue\n","\n","  question = data['query']\n","  ground_truth_lowercase = str(data['answer']).strip().lower()\n","  web_search_results = data['search_results']\n","\n","  prediction_lowercase = rag.inference(question, web_search_results, 5)['answer'].lower()\n","\n","  if prediction_lowercase == ground_truth_lowercase:\n","      n_correct_exact += 1\n","      continue\n","  elif \"i don't know\" in prediction_lowercase:\n","      n_miss += 1\n","      continue\n","\n","  acceptable = CRAG_evaluation(question, ground_truth_lowercase, prediction_lowercase)\n","\n","  if acceptable == 1:\n","    n_correct += 1\n","\n","n_hallucinate = (len(finance_test_dataset_ids) - n_correct_exact - n_correct - n_miss)\n","\n","CRAG_score = n_correct_exact + 0.5*n_correct - n_hallucinate\n","\n","print(\"\\n\\n\")\n","print(\"Number of correct answers:\", n_correct)\n","print(\"Number of exact correct answers:\", n_correct_exact)\n","print(\"Number of missed answers:\", n_miss)\n","print(\"Number of hallucinated answers:\", n_hallucinate)\n","print(\"CRAG score:\", CRAG_score)\n","```"],"metadata":{"id":"N41wQyiiDaV9"}},{"cell_type":"code","source":["### YOUR CODE HERE ###\n","\n","finance_test_dataset_ids = []\n","\n","for data in dataset:\n","    if data['domain'] == 'finance':\n","        finance_test_dataset_ids.append(data['interaction_id'])\n","\n","    if len(finance_test_dataset_ids) >= 10:\n","        break"],"metadata":{"id":"U2B7owVkHHau"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### YOUR CODE HERE ###\n","\n","n_miss, n_correct, n_correct_exact = 0, 0, 0\n","\n","for data in tqdm(dataset):\n","  if data['interaction_id'] not in finance_test_dataset_ids:\n","    continue\n","\n","  question = data['query']\n","  ground_truth_lowercase = str(data['answer']).strip().lower()\n","  web_search_results = data['search_results']\n","\n","  prediction_lowercase = rag.inference(question, web_search_results, 5)['answer'].lower()\n","\n","  if prediction_lowercase == ground_truth_lowercase:\n","      n_correct_exact += 1\n","      continue\n","  elif \"i don't know\" in prediction_lowercase:\n","      n_miss += 1\n","      continue\n","\n","  acceptable = CRAG_evaluation(question, ground_truth_lowercase, prediction_lowercase)\n","\n","  if acceptable == 1:\n","    n_correct += 1\n","\n","n_hallucinate = (len(finance_test_dataset_ids) - n_correct_exact - n_correct - n_miss)\n","\n","CRAG_score = n_correct_exact + 0.5*n_correct - n_hallucinate\n","\n","print(\"\\n\\n\")\n","print(\"Number of correct answers:\", n_correct)\n","print(\"Number of exact correct answers:\", n_correct_exact)\n","print(\"Number of missed answers:\", n_miss)\n","print(\"Number of hallucinated answers:\", n_hallucinate)\n","print(\"CRAG score:\", CRAG_score)"],"metadata":{"id":"rnQRw9sbMPFa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## II. Upgrade KG Query Stage with MCP\n","\n","So far, we have integrated RAG with a knowledge graph, enabling the LLM to retrieve structured knowledge through KGQueryEngine. In this approach, the retrieval was based on a decision tree to invoke API calls, and queries were constructed using the LLM.\n","\n","However, is our KGQueryEngine truly reliable in practical settings? Can the retrieved results be used effectively by the LLM? More importantly, how does this method compare to the emerging MCP-based retrieval approach?\n","\n","To answer these questions, we will upgrade our RAG system and evaluate both methods through the following steps:\n","\n","####1. Error Case Analysis\n","####2. Implement MCP-based Tool Calls\n","\n","This structured comparison will help us determine whether the shift toward MCP tools—now gaining popularity in LLM applications—is practically justified."],"metadata":{"id":"f-E3pWUz-dbV"}},{"cell_type":"markdown","source":["###1. Error Case Analysis\n","\n","Previously, we built Graph RAG and confirmed that incorporating a knowledge graph, rather than relying solely on RAG, can improve performance for certain examples.\n","\n","However, it remains uncertain whether our `KGQueryEngine` functions correctly for all question-answer pairs. We have yet to analyze all questions within the finance domain.\n","\n","Beyond questions that require information such as EPS, there can be various other questions within the finance domain. For example, consider the following questions:\n","\n","```python\n","with bz2.open(file_path, 'rt') as file:\n","  for line in file:\n","    data = json.loads(line.strip())\n","\n","    if data['interaction_id'] == \"7a77679e-d88b-4acf-9532-94e32233950b\":\n","      question = data['query']\n","      gold_answer = data['answer']\n","      search_results = data['search_results']\n","      query_time = data['query_time']\n","      break\n","      \n","print(\"Question: \", question)\n","print(\"Gold Answer: \", gold_answer)\n","print(\"Query Time: \", query_time)\n","```"],"metadata":{"id":"ZB0LHFGO9tc-"}},{"cell_type":"code","source":["### YOUR CODE HERE ###\n","\n","with bz2.open(file_path, 'rt') as file:\n","  for line in file:\n","    data = json.loads(line.strip())\n","\n","    if data['interaction_id'] == \"7a77679e-d88b-4acf-9532-94e32233950b\":\n","      question = data['query']\n","      gold_answer = data['answer']\n","      search_results = data['search_results']\n","      query_time = data['query_time']\n","      break\n","\n","print(\"Question: \", question)\n","print(\"Gold Answer: \", gold_answer)\n","print(\"Query Time: \", query_time)"],"metadata":{"id":"uWQRtg6ukr6T"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let’s examine the results generated by our KGQueryEngine for this question.\n","\n","To do so, we need to recall how we perform retrievals on the knowledge graph. We use an LLM to generate queries that serve as inputs for APIs connected to the knowledge graph.\n","\n","However, the LLM models provided by OpenAI inherently exhibit randomness, meaning that the same query is not always generated consistently. As a result, our KGQueryEngine does not always return identical results.\n","\n","Therefore, it is important to repeat the same process multiple times to identify trends in the generated outputs.\n","\n","Let’s explore the irregularity of GPT and our query generation examples through the following case study.\n","\n","\n","```Python\n","kg_query_engine = KGQueryEngine(server=external_kg_server)\n","\n","generated_queries = []\n","\n","for i in range(3):\n","    generated_query = kg_query_engine.generate_query(question)[0]\n","    print(generated_query)\n","    generated_queries.append(generated_query)\n","```"],"metadata":{"id":"-SilvoccE2ih"}},{"cell_type":"code","source":["### YOUR CODE HERE ###\n","\n","kg_query_engine = KGQueryEngine(server=external_kg_server)\n","\n","generated_queries = []\n","\n","for i in range(3):\n","    generated_query = kg_query_engine.generate_query(question)[0]\n","    print(generated_query)\n","    generated_queries.append(generated_query)"],"metadata":{"id":"orxfsZZmnQXF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Select queries that explicitly specify company names, metrics, and other relevant details, and examine the results of knowledge graph retrieval\n","\n","```python\n","i=0 #change this index\n","\n","kg_results = kg_query_engine.get_finance_kg_results(generated_queries[i])\n","\n","json_strings = kg_results.split(\"<DOC>\\n\")\n","\n","json_strings = [s.replace(\"'\", '\"') for s in json_strings]\n","\n","parsed_json = [json.loads(js) for js in json_strings]\n","\n","for idx, data in enumerate(parsed_json):\n","    print(json.dumps(data, indent=4))\n","\n","len(kg_results)\n","```"],"metadata":{"id":"7F5_JRsrRkx1"}},{"cell_type":"code","source":["### YOUR CODE HERE ###\n","\n","i=0 #change this index\n","\n","kg_results = kg_query_engine.get_finance_kg_results(generated_queries[i])\n","\n","json_strings = kg_results.split(\"<DOC>\\n\")\n","\n","json_strings = [s.replace(\"'\", '\"') for s in json_strings]\n","\n","parsed_json = [json.loads(js) for js in json_strings]\n","\n","for idx, data in enumerate(parsed_json):\n","    print(json.dumps(data, indent=4))\n","\n","len(kg_results)"],"metadata":{"id":"1jEVAx8_VYO2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Upon reviewing the search results, we observed that an extremely long string was retrieved.  \n","\n","Such lengthy search results can lead to the following issues:  \n","\n","1. **Excessive unnecessary information may cause hallucinations.**  \n","2. **Even if the necessary information is retrieved, the LLM may fail to recognize it properly.**  \n","3. **When combined with search results from `search_results`, the total context length may exceed the LLM’s limit, leading to inference errors.**  \n","\n","Let’s analyze how our RAG responds to this issue. Here, we will focus on handling search results from the knowledge graph, excluding the search process from `search_results`.  \n","\n","Therefore, let’s declare a new RAG class as follows and use it for evaluation.\n","\n","```python\n","class RAGwithoutSR:\n","    def __init__(self, server=None):\n","        self.retriever = LlamaIndexRetriever()\n","        self.kg_query_engine = KGQueryEngine(server=server)\n","        self.reader = Reader()\n","\n","    def retrieve(self, query, search_results, topk):\n","        kg_results = self.kg_query_engine.query(query)\n","\n","        return [kg_results]\n","\n","    def generate_response(self, query, retrieved_results):\n","        answer = self.reader.generate_response(query, retrieved_results)\n","        return answer\n","\n","    def inference(self, query, search_results, topk):\n","        retrieved_results = self.retrieve(query, search_results, topk)\n","        answer = self.generate_response(query, retrieved_results)\n","        return {\n","            \"retrieved_results\": retrieved_results,\n","            \"answer\": answer\n","        }\n","\n","rag_kg = RAGwithoutSR(server=external_kg_server)\n","```\n","```python\n","for i in range(3):\n","    rag_output = rag_kg.inference(question, search_results, 5)\n","    print(rag_output['answer'])\n","```"],"metadata":{"id":"81zakW4_SGAO"}},{"cell_type":"code","source":["### YOUR CODE HERE ###\n","\n","class RAGwithoutSR:\n","    def __init__(self, server=None):\n","        self.retriever = LlamaIndexRetriever()\n","        self.kg_query_engine = KGQueryEngine(server=server)\n","        self.reader = Reader()\n","\n","    def retrieve(self, query, search_results, topk):\n","        kg_results = self.kg_query_engine.query(query)\n","\n","        return [kg_results]\n","\n","    def generate_response(self, query, retrieved_results):\n","        answer = self.reader.generate_response(query, retrieved_results)\n","        return answer\n","\n","    def inference(self, query, search_results, topk):\n","        retrieved_results = self.retrieve(query, search_results, topk)\n","        answer = self.generate_response(query, retrieved_results)\n","        return {\n","            \"retrieved_results\": retrieved_results,\n","            \"answer\": answer\n","        }\n","\n","rag_kg = RAGwithoutSR(server=external_kg_server)"],"metadata":{"id":"ulCPvSsASDxd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### YOUR CODE HERE ###\n","\n","for i in range(3):\n","    rag_output = rag_kg.inference(question, search_results, 5)\n","    print(rag_output['answer'])"],"metadata":{"id":"T7ZRCC4-i2cB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###2. Implement MCP-based Tool Calls\n","\n","In this step, we replace the existing KGQueryEngine logic with MCP-based tool calls.\n","\n","**Model Context Protocol (MCP)** is a standardized interface that allows large language models to interact with external tools or data services via a structured client-server protocol. Each function on the server is registered as a tool, and the LLM can invoke these tools by sending structured requests through the MCP client.\n","\n","<img src=\"https://i.imgur.com/P1g0TPh.png\">\n","\n","Therefore, if an external data source can be accessed by the LLM through MCP, this interaction can be viewed as a form of RAG. This is especially relevant in our scenario, where the knowledge graph is only partially accessible via APIs. In such cases, instead of relying on a pre-defined decision tree, it might be more effective to let the LLM decide which tools to use dynamically.\n","\n","In other words, the LLM itself selects the appropriate tool to retrieve specific information from the knowledge graph.\n","\n","To enable this, it is crucial to define which tools are available to the LLM. This is handled on the MCP server side, where each tool is registered along with a description that helps the model understand what it can do.\n","\n","<img src=\"https://i.imgur.com/EiQfJxQ.png\" width=500>\n","\n","The MCP server consists of core tool definitions and server-side settings that together define the server’s behavior.  \n","However, because the MCP server requires an event loop to run, it cannot be executed within this Colab notebook environment. Thus, we exclude server execution from this exercise, but you can refer to the attached code for implementation details.\n","\n","We can leverage the same tools we used previously, such as llamaindex, to construct the MCP client. Refer to the code below to see how the MCP client can be implemented.\n","\n","Now, let’s proceed to implement the MCP client. This client is responsible for retrieving tool information from the server and forwarding tool invocation requests on behalf of the LLM.\n","\n","First, let's install and import what to need.\n","\n","```python\n","! pip install llama-index-tools-mcp --quiet\n","! pip install llama-index-llms-openai --quiet\n","```\n","```python\n","from llama_index.tools.mcp import BasicMCPClient, McpToolSpec\n","from llama_index.llms.openai import OpenAI\n","from llama_index.core.agent.workflow import FunctionAgent, ToolCallResult, ToolCall\n","from llama_index.core.workflow import Context\n","\n","import dotenv\n","```"],"metadata":{"id":"KFX8YSjFrUfC"}},{"cell_type":"code","source":["### YOUR CODE HERE ###\n","\n","! pip install llama-index-tools-mcp==0.2.5 --quiet\n","! pip install llama-index-llms-openai==0.4.4 --quiet"],"metadata":{"id":"-jeEA5wjhBl8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### YOUR CODE HERE ###\n","\n","from llama_index.tools.mcp import BasicMCPClient, McpToolSpec\n","from llama_index.core.agent.workflow import FunctionAgent, ToolCallResult, ToolCall\n","from llama_index.core.workflow import Context\n","\n","import dotenv"],"metadata":{"id":"7CIZHHtVhXtg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Second, apart from this, the current reader is not considering query_time. Therefore, let's use the revised version to consider query_time.\n","\n","\n","```python\n","from openai import OpenAI\n","\n","oai_client = OpenAI()\n","\n","class Reader:\n","  def __init__(self):\n","\n","    self.system_prompt = \"\"\"\n","    You are provided with a question, the time at which the question is asked, and various references.\n","    Your task is to answer the question succinctly, using the fewest words possible.\n","    If the references do not contain the necessary information to answer the question, respond with 'I don't know'.\n","    There is no need to explain the reasoning behind your answers.\n","    \"\"\"\n","\n","  def generate_response(self, question: str, query_time: str, top_k_chunks: list) -> str:\n","    \"\"\"\n","    Generate answer from context.\n","    \"\"\"\n","    llm_input = self.prompt_generator(question, query_time, top_k_chunks)\n","    completion = oai_client.chat.completions.create(\n","    model=\"gpt-3.5-turbo\",\n","    temperature=0,\n","    messages=\n","    llm_input\n","    ).choices[0].message.content\n","    return completion\n","\n","  def prompt_generator(self, query, query_time, top_k_chunks):\n","    user_message = \"\"\n","    references = \"\"\n","\n","    if len(top_k_chunks) > 0:\n","        references += \"# References \\n\"\n","        # Format the top sentences as references in the model's prompt template.\n","        for chunk_id, chunk in enumerate(top_k_chunks):\n","            references += f\"- {chunk.strip()}\\n\"\n","\n","\n","    user_message += f\"{references}\\n------\\n\\n\"\n","    user_message\n","    user_message += f\"Using only the references listed above, answer the following question: \\n\"\n","    user_message += f\"Question: {query}\\n\"\n","    user_message += f\"Query Time: {query_time}\\n\"\n","\n","    llm_input = [\n","    {\"role\": \"system\", \"content\": self.system_prompt},\n","    {\"role\": \"user\", \"content\": user_message},\n","    ]\n","\n","    return llm_input\n","```"],"metadata":{"id":"ucKTVlqjBzJ-"}},{"cell_type":"code","source":["### YOUR CODE HERE ###\n","\n","from openai import OpenAI\n","\n","oai_client = OpenAI()\n","\n","class Reader:\n","  def __init__(self):\n","\n","    self.system_prompt = \"\"\"\n","    You are provided with a question, the time at which the question is asked, and various references.\n","    Your task is to answer the question succinctly, using the fewest words possible.\n","    If the references do not contain the necessary information to answer the question, respond with 'I don't know'.\n","    There is no need to explain the reasoning behind your answers.\n","    \"\"\"\n","\n","  def generate_response(self, question: str, query_time: str, top_k_chunks: list) -> str:\n","    \"\"\"\n","    Generate answer from context.\n","    \"\"\"\n","    llm_input = self.prompt_generator(question, query_time, top_k_chunks)\n","    completion = oai_client.chat.completions.create(\n","    model=\"gpt-3.5-turbo\",\n","    temperature=0,\n","    messages=\n","    llm_input\n","    ).choices[0].message.content\n","    return completion\n","\n","  def prompt_generator(self, query, query_time, top_k_chunks):\n","    user_message = \"\"\n","    references = \"\"\n","\n","    if len(top_k_chunks) > 0:\n","        references += \"# References \\n\"\n","        # Format the top sentences as references in the model's prompt template.\n","        for chunk_id, chunk in enumerate(top_k_chunks):\n","            references += f\"- {chunk.strip()}\\n\"\n","\n","\n","    user_message += f\"{references}\\n------\\n\\n\"\n","    user_message\n","    user_message += f\"Using only the references listed above, answer the following question: \\n\"\n","    user_message += f\"Question: {query}\\n\"\n","    user_message += f\"Query Time: {query_time}\\n\"\n","\n","    llm_input = [\n","    {\"role\": \"system\", \"content\": self.system_prompt},\n","    {\"role\": \"user\", \"content\": user_message},\n","    ]\n","\n","    return llm_input"],"metadata":{"id":"usfEGeitCKQI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### MCP Server Overview\n","\n","The **MCP (Model-Context-Program) Server** serves as the execution and data layer that interfaces directly with the LLM. It exposes customized functionality and structured contextual information through three main elements: **Resources**, **Tools**, and **Prompts**. This enables LLMs to make informed, context-aware decisions and perform real-world operations safely and efficiently.\n","\n","---\n","\n","### MCP Context Components\n","---\n","\n","### 1. Resource\n","\n","A **Resource** provides structured, external data that the LLM can reference during reasoning or decision-making. It represents external objects—such as datasets, files, graph nodes, or static configuration tables—that are read-only from the model's perspective.\n","\n","**Key Properties:**\n","- `name`: Human-readable name shown to users or in debugging.\n","- `description`: A short summary explaining the resource's purpose.\n","- `mime_type`: The MIME type of the content (e.g., `text/plain`, `application/json`). This describes the content format.\n","- `uri`: A globally unique identifier for the resource.\n","- `content`: The actual data or a pointer to the data (e.g., a JSON object, string, or file reference).\n","\n","**Usage Notes:**\n","- Resources are **not invoked** like functions.\n","- They are **fetched by URI**, and remain **static** so that the LLM can repeatedly refer to them without reloading.\n","- Resources improve LLM accuracy by anchoring it to reliable external context.\n","\n","---\n","\n","### 2. Tool\n","\n","A **Tool** is a callable function registered on the MCP server that the LLM can use to interact with the outside world. These tools bridge natural language instructions and real-world effects by turning a model’s intent into executable actions.\n","\n","**Typical Use Cases:**\n","- Executing database queries\n","- Fetching live data from APIs\n","- Performing calculations or summarizations\n","- Searching documents or files\n","\n","**How It Works:**\n","1. The user provides a natural language input (e.g., “Please calculate 3 + 7”).\n","2. The MCP client supplies the LLM with a list of available tools and their schema.\n","3. The LLM chooses the appropriate tool and extracts input arguments (e.g., `x=3`, `y=7`).\n","4. The tool invocation is generated and passed to the MCP server.\n","5. The tool is executed on the server, and the result is returned.\n","6. The LLM incorporates the result into its context to generate a final user response.\n","\n","**Implementation:**\n","Tools are registered using the `@mcp.tool()` decorator in Python, and must conform to the function signature and type schema required by the MCP framework.\n","\n","---\n","\n","### 3. Prompt\n","\n","A **Prompt** in MCP serves as a reusable, templated instruction that guides the LLM’s reasoning in a structured way. Prompts help the model:\n","\n","- Interpret ambiguous user queries\n","- Structure downstream tool invocations\n","- Extract relevant content from complex input\n","\n","**Prompt Usage Scenarios:**\n","- Generate structured summaries from unstructured context\n","- Classify user intent\n","- Disambiguate time references like “this year” or “last quarter”\n","\n","Prompts act as templates or procedural guides that the LLM fills in dynamically based on user input and context. While not executable like tools, they play a key role in shaping how the LLM prepares inputs and interprets outputs.\n","\n","---\n","\n","### Summary\n","\n","| Component | Purpose |\n","|----------|---------|\n","| `Resource` | Static external data that LLM can reference |\n","| `Tool`     | Callable function to perform real-world actions |\n","| `Prompt`   | Structured, reusable guide for reasoning or template generation |\n","\n","Together, these elements form the foundation of the **MCP Server**, enabling LLMs to operate safely, reliably, and contextually in real applications."],"metadata":{"id":"-Usj8YMd7xhb"}},{"cell_type":"markdown","source":["### MCP Client Overview\n","\n","In this section, we focus on the **MCP Client**, which serves as the interface between the LLM runtime (e.g., LlamaIndex) and an external **MCP Server**.\n","\n","### What is the MCP Client?\n","\n","The MCP Client is responsible for:\n","\n","- Connecting to the MCP Server over HTTP or SSE.\n","- Fetching available **tools**, **resources**, and **prompts** exposed by the server.\n","- Translating natural language requests into structured tool invocations.\n","- Managing context and integrating responses from the server back into the LLM workflow.\n","\n","It essentially acts as a **bridge** that allows the language model to interact with external APIs and structured context in a modular and dynamic way.\n","\n","```python\n","external_mcp_server = \"http://x.x.x.x:port/sse\" #change to correct uri\n","\n","mcp_client = BasicMCPClient(external_mcp_server)\n","mcp_tool = McpToolSpec(client=mcp_client)\n","\n","tools = await mcp_tool.to_tool_list_async()\n","print(\"\\n=== Available Tools ===\\n\")\n","for tool in tools:\n","    print(f\"🔧 Name       : {tool.metadata.name}\")\n","    print(f\"   Description: {tool.metadata.description}\\n\")\n","```\n","```python\n","resources = await mcp_tool.fetch_resources()\n","\n","print(\"\\n=== Available Resources ===\\n\")\n","for resource in resources:\n","    print(f\"📦 URI        : {resource.uri}\")\n","    print(f\"   Name       : {resource.name}\")\n","    print(f\"   Description: {resource.description}\")\n","    print(f\"   MIME Type  : {resource.mimeType}\\n\")\n","```"],"metadata":{"id":"ZeLWPNEY_wvE"}},{"cell_type":"code","source":["### YOUR CODE HERE ###\n","\n","external_mcp_server = \"http://x.x.x.x:port/sse\" #change to correct uri\n","\n","mcp_client = BasicMCPClient(external_mcp_server)\n","mcp_tool = McpToolSpec(client=mcp_client)\n","\n","tools = await mcp_tool.to_tool_list_async()\n","print(\"\\n=== Available Tools ===\\n\")\n","for tool in tools:\n","    print(f\"🔧 Name       : {tool.metadata.name}\")\n","    print(f\"   Description: {tool.metadata.description}\\n\")"],"metadata":{"id":"tO9W9l5IDN38"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["resources = await mcp_tool.fetch_resources()\n","\n","print(\"\\n=== Available Resources ===\\n\")\n","for resource in resources:\n","    print(f\"📦 URI        : {resource.uri}\")\n","    print(f\"   Name       : {resource.name}\")\n","    print(f\"   Description: {resource.description}\")\n","    print(f\"   MIME Type  : {resource.mimeType}\\n\")"],"metadata":{"id":"jV2Gzwt_DSKj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 1. KG Query Engine Initialization with MCP Client\n","\n","In this step, we initialize an **LLM agent** that can interact with tools registered on the MCP Server.\n","\n","- `BasicMCPClient` connects to the MCP Server at the specified URL.\n","- `McpToolSpec` wraps available tools for the agent to use.\n","- `FunctionAgent` is created with:\n","  - The `GPT` model\n","  - A system prompt guiding tool-based reasoning\n","  - A dynamic list of MCP tools\n","\n","The agent is now ready to handle user queries by calling tools exposed via the MCP interface.\n","\n","```python\n","SYSTEM_PROMPT = \"\"\"\\\n","You are an AI assistant for Tool Calling.\n","\n","Before you help a user, you need to work with tools to interact with Our Knowledge Graph\n","\"\"\"\n","```\n","\n","```python\n","from llama_index.llms.openai import OpenAI\n","\n","class KGQueryEngineWithMCP:\n","    def __init__(self, mcp_tool_spec: McpToolSpec, model: str, llm = None):\n","        self.llm = llm or OpenAI(model=model, temperature=0)\n","        self.mcp_tool_spec = mcp_tool_spec\n","        self.agent: Optional[FunctionAgent] = None\n","        self.agent_context: Optional[Context] = None\n","\n","    async def init_agent(self):\n","        tools = await self.mcp_tool_spec.to_tool_list_async()\n","        self.agent = FunctionAgent(\n","            name=\"Agent\",\n","            description=\"An agent that can work with Our Knowledge Graph api.\",\n","            tools=tools,\n","            llm=self.llm,\n","            system_prompt=SYSTEM_PROMPT,\n","        )\n","        self.agent_context = Context(self.agent)\n","\n","    async def query(self, question: str, verbose: bool = False) -> str:\n","        if self.agent is None or self.agent_context is None:\n","            raise RuntimeError(\"Agent not initialized. Call `await init_agent()` first.\")\n","\n","        handler = self.agent.run(question, ctx=self.agent_context)\n","        async for event in handler.stream_events():\n","            if verbose and type(event) == ToolCall:\n","                print(f\"Calling tool {event.tool_name} with kwargs {event.tool_kwargs}\")\n","            elif verbose and type(event) == ToolCallResult:\n","                print(f\"Tool {event.tool_name} returned {event.tool_output}\")\n","\n","        response = await handler\n","        return str(response)\n","\n","kg_engine = KGQueryEngineWithMCP(mcp_tool, model='gpt-3.5-turbo')\n","await kg_engine.init_agent()\n","```\n"],"metadata":{"id":"5H_8j2k5DUG2"}},{"cell_type":"code","source":["### YOUR CODE HERE ###\n","\n","SYSTEM_PROMPT = \"\"\"\\\n","You are an AI assistant for Tool Calling.\n","\n","Before you help a user, you need to work with tools to interact with Our Knowledge Graph\n","\"\"\""],"metadata":{"id":"sGHEWJ3w1dMm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### YOUR CODE HERE ###\n","\n","from llama_index.llms.openai import OpenAI\n","\n","class KGQueryEngineWithMCP:\n","    def __init__(self, mcp_tool_spec: McpToolSpec, model: str, llm = None):\n","        self.llm = llm or OpenAI(model=model, temperature=0)\n","        self.mcp_tool_spec = mcp_tool_spec\n","        self.agent: Optional[FunctionAgent] = None\n","        self.agent_context: Optional[Context] = None\n","\n","    async def init_agent(self):\n","        tools = await self.mcp_tool_spec.to_tool_list_async()\n","        self.agent = FunctionAgent(\n","            name=\"Agent\",\n","            description=\"An agent that can work with Our Knowledge Graph api.\",\n","            tools=tools,\n","            llm=self.llm,\n","            system_prompt=SYSTEM_PROMPT,\n","        )\n","        self.agent_context = Context(self.agent)\n","\n","    async def query(self, question: str, verbose: bool = False) -> str:\n","        if self.agent is None or self.agent_context is None:\n","            raise RuntimeError(\"Agent not initialized. Call `await init_agent()` first.\")\n","\n","        handler = self.agent.run(question, ctx=self.agent_context)\n","        async for event in handler.stream_events():\n","            if verbose and type(event) == ToolCall:\n","                print(f\"Calling tool {event.tool_name} with kwargs {event.tool_kwargs}\")\n","            elif verbose and type(event) == ToolCallResult:\n","                print(f\"Tool {event.tool_name} returned {event.tool_output}\")\n","\n","        response = await handler\n","        return str(response)\n","\n","kg_engine = KGQueryEngineWithMCP(mcp_tool, model='gpt-3.5-turbo')\n","await kg_engine.init_agent()"],"metadata":{"id":"_dCqrHvuEwX5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This function executes an LLM agent `FunctionAgent` using a given user message and shared workflow context `Context`.    \n","It streams intermediate tool call events in real time and returns the final response from the agent.\n","\n","1. Starts the agent with the input message and context.\n","2. Streams events while the agent is running.\n","   - Logs tool invocations and results if `verbose=True`.\n","3. Awaits the final result and returns it as a string.\n","\n","This function helps monitor the reasoning and tool execution steps taken by the agent in a transparent, asynchronous manner.\n","\n","Through the code below, let's see how the MCP-based RAG is solving the 'error case seen above\n","\n","```python\n","question = \"\"\"Query: which company distribute more dividends this year, muj or  tcbio?\n","Query time: 2024/02/23\"\"\"\n","\n","response = await handle_user_message(question, agent, agent_context, verbose=True)\n","print(\"Agent: \", response)\n","```"],"metadata":{"id":"8YgNGTaDun4b"}},{"cell_type":"code","source":["### YOUR CODE HERE ###\n","\n","print(\"Question: \", question)\n","\n","response = await kg_engine.query(\n","    question,\n","    verbose=True,\n",")\n","print(response)"],"metadata":{"id":"E4NZVatRulQQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's examine the result of the code above.  \n","Did the MCP agent generate a correct answer? Probably not.\n","\n","The reason becomes clear when we look at the question that was passed to the LLM.  \n","The LLM did not receive any information about the **query time**, which is crucial for answering this type of question.  \n","Since the timing context is highly important here, let’s include the query time and try the question again.\n","\n","```python\n","question_with_time = \"Query: \" + question + \"\\nQuery time: \" + query_time\n","print(\"Question: \", question_with_time)\n","\n","response = await kg_engine.query(\n","    question_with_time,\n","    verbose=True,\n",")\n","print(response)\n","```"],"metadata":{"id":"fiSmEsHf9wFc"}},{"cell_type":"code","source":["### YOUR CODE HERE ###\n","\n","question_with_time = \"Query: \" + question + \"\\nQuery time: \" + query_time\n","print(\"Question: \", question_with_time)\n","\n","response = await kg_engine.query(\n","    question_with_time,\n","    verbose=True,\n",")\n","print(response)"],"metadata":{"id":"aFx9fztC-Eyk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Was the generated result good enough?\n","\n","Some users may find it insufficient, while others may encounter outright errors.  \n","In most cases, these issues arise because the LLM used in MCP exceeded its **context limit** due to the large amount of input.\n","\n","There are several possible solutions to this problem, but one practical approach is to switch to a model with a **larger context window**.\n","\n","This time, let's proceed by using the `gpt-4.1-mini` model, which supports a larger input context and is better suited for handling longer queries.\n","\n","```python\n","kg_engine = KGQueryEngineWithMCP(mcp_tool, model='gpt-4.1-mini')\n","await kg_engine.init_agent()\n","```\n","```python\n","print(\"Question: \": question_with_time)\n","\n","response = await kg_engine.query(\n","    question_with_time,\n","    verbose=True,\n",")\n","print(response)\n","```"],"metadata":{"id":"FEUkAPX5-F1k"}},{"cell_type":"code","source":["### YOUR CODE HERE ###\n","\n","kg_engine = KGQueryEngineWithMCP(mcp_tool, model='gpt-4.1-mini')\n","await kg_engine.init_agent()"],"metadata":{"id":"1vcdDwIC-zmp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### YOUR CODE HERE ###\n","\n","print(\"Question: \", question_with_time)\n","\n","response = await kg_engine.query(\n","    question_with_time,\n","    verbose=True,\n",")\n","print(response)"],"metadata":{"id":"o4CENCgd-z8l"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 2. Structuring a RAG Class for MCP-based Interaction\n","\n","To modularize the code above and enable scalable, query-driven interaction with the MCP Server, we can refactor it into a unified `RAG` class.\n","\n","### Design Motivation\n","\n","- MCP-based clients require **rich, well-structured input** to maximize tool usage accuracy.\n","- The CRAG dataset provides us with structured components such as:\n","  - Query (question)\n","  - Context (retrieved passages)\n","  - Metadata (timestamps, entities, etc.)\n","\n","By encapsulating this into a class, we can:\n","1. **Generate precise questions** from structured CRAG input\n","2. **Run those questions through the MCP-connected agent**\n","3. **Return clean, tool-integrated answers**\n","\n","---\n","\n","### Key Components\n","\n","| Method | Purpose |\n","|--------|---------|\n","| `__init__` | Initialize MCP client, tools, and LLM agent |\n","| `retrieve(query, query_time)` | Convert CRAG sample into a natural-language query with max context and retrieve from knowledge graph |\n","| `generate_response(query, query_time, )` | Perform LLM reference based on searched results to obtain LLM response |\n","| `inference(query, query_time)` | Combine the above two methods into a method that can perform QATask in RAG |\n","\n","```python\n","class RAGwithMCP:\n","    def __init__(self, server=None):\n","        self.mcp_application = KGQueryEngineWithMCP(server=server)\n","        self.reader = Reader()\n","\n","    async def retrieve(self, query: str, query_time: str, search_results: list, topk: int):\n","        await self.mcp_application.init_agent()\n","\n","        full_query = f\"\"\"Query: {query}\n","Query time: {query_time}\"\"\"\n","\n","        mcp_result = await self.mcp_application.query(full_query, verbose=False)\n","        return mcp_result\n","\n","    def generate_response(self, query: str, query_time: str, retrieved_results: str):\n","        answer = self.reader.generate_response(query, query_time, retrieved_results)\n","        return answer\n","\n","    async def inference(self, query: str, search_results: list, query_time: str, topk: int):\n","        retrieved_results = await self.retrieve(query, query_time, search_results, topk)\n","        answer = self.generate_response(query, query_time, retrieved_results)\n","        return {\n","            \"retrieved_results\": retrieved_results,\n","            \"answer\": answer\n","        }\n","\n","rag_mcp = RAGwithMCP(server=external_mcp_server)\n","```"],"metadata":{"id":"fcxcrnVZxRL2"}},{"cell_type":"code","source":["### YOUR CODE HERE ###\n","\n","from llama_index.llms.openai import OpenAI\n","\n","class RAGwithMCP:\n","    def __init__(self, mcp_tool):\n","        self.mcp_application = KGQueryEngineWithMCP(mcp_tool, model='gpt-4.1-mini')\n","        self.reader = Reader()\n","\n","    async def retrieve(self, query: str, query_time: str, search_results: list, topk: int):\n","        await self.mcp_application.init_agent()\n","\n","        full_query = f\"\"\"Query: {query}\n","Query time: {query_time}\"\"\"\n","\n","        mcp_result = await self.mcp_application.query(full_query, verbose=False)\n","        return mcp_result\n","\n","    def generate_response(self, query: str, query_time: str, retrieved_results: str):\n","        answer = self.reader.generate_response(query, query_time, retrieved_results)\n","        return answer\n","\n","    async def inference(self, query: str, search_results: list, query_time: str, topk: int):\n","        retrieved_results = await self.retrieve(query, query_time, search_results, topk)\n","        answer = self.generate_response(query, query_time, retrieved_results)\n","        return {\n","            \"retrieved_results\": retrieved_results,\n","            \"answer\": answer\n","        }\n","\n","rag_mcp = RAGwithMCP(mcp_tool)"],"metadata":{"id":"r_KauNO-P_NF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### YOUR CODE HERE ###\n","\n","result = await rag_mcp.inference(\n","    query=question,\n","    search_results=[],\n","    query_time=query_time,\n","    topk=5\n",")\n","\n","print(\"Retrieved Results:\\n\", result[\"retrieved_results\"])\n","print(\"------------------\")\n","print(\"Final Answer:\\n\", result[\"answer\"])"],"metadata":{"id":"yj0MxJ8lE4Go"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### YOUR CODE HERE ###\n","\n","for i in range(3):\n","    rag_output = await rag_mcp.inference(\n","        query=question,\n","        search_results=[],\n","        query_time=query_time,\n","        topk=5\n","    )\n","    print(rag_output['answer'].lower())"],"metadata":{"id":"8F13OdcbQynI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["In this notebook, we explored how MCP (Model Context Protocol) can be used to build a flexible and modular tool-augmented RAG pipeline.\n","\n","While MCP allows LLMs to dynamically select and invoke tools using a standardized interface, real-world usage has revealed several practical limitations:\n","\n","- **Context Overflow**: Tool outputs are inserted directly into the model's input context. If the result is too long, it may exceed the model’s token limit and lead to failure.\n","- **Limited Error Recovery**: When tool execution fails or exceeds limits, LLMs often cannot recover or retry unless explicitly guided to do so.\n","- **Debugging Difficulty**: Since tool selection and reasoning are tightly coupled inside the model, it is difficult to trace what went wrong without detailed logs or event streaming.\n","- **Latency and Reliability**: Each tool call requires a round-trip to the server. In multi-step workflows, this can introduce significant delay and failure points.\n","- **Loss of Developer Control**: Tool behavior is driven by the LLM’s interpretation of the prompt and available tools, making behavior harder to predict or constrain.\n","\n","Understanding these trade-offs is key to using MCP effectively in real-world LLM applications."],"metadata":{"id":"oLaIQSGWGX4K"}}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.18"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}