{"cells":[{"cell_type":"markdown","metadata":{"id":"TXSKbyvh2QaU"},"source":["#üìì TASK #1: WEB-BASED RETRIEVAL SUMMARIZATION\n","\n","In this section, we will tackle the first task of the KDD Cup: Web-based Retrieval Summarization. Since the KDD Cup CRAG benchmark fundamentally focuses on RAG (Retrieval-Augmented Generation), our approach will also be based on the RAG framework.\n","\n","Before building the RAG system, let‚Äôs first clarify what problem we need to solve. At first, participants receive 5 web pages per question, potentially containing relevant information. And the objective is to measure the systems' capability to identify and condense this information into accurate answers.\n","\n","<br/>\n","\n","<img src=\"https://i.imgur.com/jlNdBmD.png\">\n","\n","By looking at the diagram above, you will get an idea of what problem we need to solve. Additionally, since you have already reviewed the input data in previous sessions, you are well aware of the types of data you will be working with.\n","\n","As you may recall, the CRAG dataset contains many challenging questions, and as we observed earlier, it is difficult for the LLM alone to solve these problems effectively. Therefore, we will explore how these types of problems can be addressed using the RAG framework.\n","\n","Specifically, This practice class will be comprised of four sections.  \n","  \n","### I. Implementing a Retriever\n","### II. Implementing a Reader\n","### III. Implementing a RAG\n","### IV. Error case analysis"]},{"cell_type":"markdown","metadata":{"id":"pGvoJusEMYuq"},"source":["## I. Implementing a Retriever\n","\n","Before building the RAG system, the first essential component we need is the **Retriever**. As you are already familiar, the retriever is a crucial element for building an effective RAG. If the retriever successfully retrieves a sufficient amount of relevant information and passes it to the LLM, the probability of the LLM generating the correct answer will significantly increase.\n","\n","In the previous session, we only experimented with the default retriever provided by `LlamaIndex` and made minor adjustments, such as modifying the chunk size.   \n","This time, however, we will define the retriever in a more low-level manner and explore its use step by step.\n","\n","This section is divided into the following four stages.\n","\n","1. Preparing Python Packages\n","2. Implementing a Chunk Extractor\n","3. Implementing a Retriever\n","4. Implementing a Retriever with LlamaIndex\n","\n"]},{"cell_type":"markdown","metadata":{"id":"HrqPxwa4OOe8"},"source":["\n","\n","### 1. Preparing Python Packages\n","\n","As always, we will start by installing and importing the necessary libraries for use.  \n","\n","At this point, we will also set the values for the global variables that will be needed later. The significance of these values will be explained in detail in the following steps.\n","\n","```Python\n","!pip install openai==1.55.3 --quiet\n","!pip install llama-index --quiet\n","!pip install llama-index-readers-wikipedia wikipedia --quiet\n","!pip install llama-index-llms-openai --quiet\n","!pip install llama-index-embeddings-huggingface --quiet\n","!pip install packaging==23.2 openai --quiet\n","!pip install langchain nltk>=3.8.1 streamlit==1.35.0 watchdog kubernetes==26.1.0 --quiet\n","\n","!pip install blingfire beautifulsoup4 sentence-transformers ray --quiet\n","!pip install textwrap3 --quiet\n","!pip install scikit-learn --quiet\n","!pip uninstall numpy -y\n","!pip install numpy==1.26.4 --quiet\n","```\n","```Python\n","import numpy as np\n","import ray\n","import bz2\n","import json\n","import torch\n","from blingfire import text_to_sentences_and_offsets\n","from collections import defaultdict\n","from typing import Any, Dict, List\n","from bs4 import BeautifulSoup\n","import os\n","import openai\n","\n","os.environ[\"OPENAI_API_KEY\"] = \"sk-...\" #copy your api key\n","\n","from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Document, get_response_synthesizer\n","from llama_index.readers.wikipedia import WikipediaReader\n","from llama_index.core.node_parser import SentenceSplitter\n","\n","import textwrap\n","```\n","\n","```Python\n","# Define the number of context sentences to consider for generating an answer.\n","NUM_CONTEXT_SENTENCES = 20\n","# Set the maximum length for each context sentence (in characters).\n","MAX_CONTEXT_SENTENCE_LENGTH = 1000\n","# Set the maximum context references length (in characters).\n","MAX_CONTEXT_REFERENCES_LENGTH = 4000\n","# Sentence Transformer Parameters\n","SENTENTENCE_TRANSFORMER_BATCH_SIZE = 128 # TUNE THIS VARIABLE depending on the size of your embedding model and GPU mem available\n","```\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"fTJAIMKnOfe0"},"outputs":[],"source":["### YOUR CODE HERE ###\n","\n","!pip install openai==1.55.3 --quiet\n","!pip install llama-index --quiet\n","!pip install llama-index-readers-wikipedia wikipedia --quiet\n","!pip install llama-index-llms-openai --quiet\n","!pip install llama-index-embeddings-huggingface --quiet\n","!pip install packaging==23.2 openai --quiet\n","!pip install langchain nltk>=3.8.1 streamlit==1.35.0 watchdog kubernetes==26.1.0 --quiet\n","\n","!pip install blingfire beautifulsoup4 sentence-transformers ray --quiet\n","!pip install textwrap3 --quiet\n","!pip install scikit-learn --quiet\n","!pip uninstall numpy -y\n","!pip install numpy==1.26.4 --quiet"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S8Ig_PSVGW_G"},"outputs":[],"source":["### YOUR CODE HERE ###\n","\n","import numpy as np\n","import ray\n","import bz2\n","import json\n","from blingfire import text_to_sentences_and_offsets\n","from collections import defaultdict\n","from typing import Any, Dict, List\n","from bs4 import BeautifulSoup\n","import os\n","import openai\n","\n","os.environ[\"OPENAI_API_KEY\"] = \"sk-...\" #copy your api key\n","\n","from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Document, get_response_synthesizer\n","from llama_index.readers.wikipedia import WikipediaReader\n","from llama_index.core.node_parser import SentenceSplitter\n","\n","import textwrap"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OcrU0dgdjLIE"},"outputs":[],"source":["### YOUR CODE HERE ###\n","\n","# Define the number of context sentences to consider for generating an answer.\n","NUM_CONTEXT_SENTENCES = 20\n","# Set the maximum length for each context sentence (in characters).\n","MAX_CONTEXT_SENTENCE_LENGTH = 1000\n","# Set the maximum context references length (in characters).\n","MAX_CONTEXT_REFERENCES_LENGTH = 4000\n","# Sentence Transformer Parameters\n","SENTENTENCE_TRANSFORMER_BATCH_SIZE = 128 # TUNE THIS VARIABLE depending on the size of your embedding model and GPU mem available"]},{"cell_type":"markdown","metadata":{"id":"6ouB4AuyGWsk"},"source":["### 2. Implementing a Chunk Extractor\n","\n","This time, we will define and use a `Chunk Extractor`. As you observed during the first practice session, the Chunk Extractor is a function needed to split the search results into appropriately sized pieces for use.\n","\n","Since search results are essentially `HTML` files, we will first define the `parse_htmls` function to remove HTML tags. Then, we will define the `extract_chunks` function, which splits the text extracted from the HTML into chunks. To avoid losing information at the chunk boundaries, the text will be split at the sentence level.\n","\n","```Python\n","def parse_htmls(search_results):\n","    all_documents = []\n","    \n","    # Process each HTML text from the search results to extract text content.\n","    for html_text in search_results:\n","\n","        # Parse the HTML content using BeautifulSoup\n","        soup = BeautifulSoup(html_text[\"page_result\"], features=\"lxml\")\n","        text = soup.get_text(\" \", strip=True)  # Use space as a separator, strip whitespaces\n","        all_documents.append(text)\n","    \n","    return all_documents\n","\n","def extract_chunks(all_documents):\n","    # Initialize a list to hold all extracted sentences from the search results.\n","    all_chunks = []\n","\n","    for document in all_documents:\n","\n","        if not document:\n","            # If no document is extracted, add an empty string as a placeholder.\n","            all_chunks.append(\"\")\n","        else:\n","\n","            # Extract offsets of sentences from the document\n","            _, offsets = text_to_sentences_and_offsets(document)\n","\n","            # Initialize a list to store sentences\n","            chunks = []\n","\n","            # Iterate through the list of offsets and extract sentences\n","            for start, end in offsets:\n","                # Extract the sentence and limit its length\n","                chunk = document[start:end][:MAX_CONTEXT_SENTENCE_LENGTH]\n","                all_chunks.append(chunk)\n","\n","    return all_chunks\n","```"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lpCFzMHMK9rV"},"outputs":[],"source":["### YOUR CODE HERE ###\n","\n","def parse_htmls(search_results):\n","    all_documents = []\n","\n","    # Process each HTML text from the search results to extract text content.\n","    for html_text in search_results:\n","\n","        # Parse the HTML content using BeautifulSoup\n","        soup = BeautifulSoup(html_text[\"page_result\"], features=\"lxml\")\n","        text = soup.get_text(\" \", strip=True)  # Use space as a separator, strip whitespaces\n","        all_documents.append(text)\n","\n","    return all_documents\n","\n","def extract_chunks(all_documents):\n","    # Initialize a list to hold all extracted sentences from the search results.\n","    all_chunks = []\n","\n","    for document in all_documents:\n","\n","        if not document:\n","            # If no document is extracted, add an empty string as a placeholder.\n","            all_chunks.append(\"\")\n","        else:\n","\n","            # Extract offsets of sentences from the document\n","            _, offsets = text_to_sentences_and_offsets(document)\n","\n","            # Initialize a list to store sentences\n","            chunks = []\n","\n","            # Iterate through the list of offsets and extract sentences\n","            for start, end in offsets:\n","                # Extract the sentence and limit its length\n","                chunk = document[start:end][:MAX_CONTEXT_SENTENCE_LENGTH]\n","                all_chunks.append(chunk)\n","\n","    return all_chunks"]},{"cell_type":"markdown","metadata":{"id":"onEEe6nv-J4A"},"source":["Now, let‚Äôs use the two functions to process the data by loading the search results and splitting them into chunks. As mentioned earlier, you will need to mount your Google Drive to access the dataset.\n","\n","Run the code below to test the example:\n","\n","```\n","from google.colab import drive\n","\n","drive.mount('/content/drive')\n","```\n","```Python\n","dataset_path = \"/content/drive/MyDrive/CRAG dataset/crag_task_1_dev_v4_release.jsonl\"\n","with open(dataset_path, \"rt\") as file:\n","    for line in file:\n","        item = json.loads(line)\n","        \n","        # Get documents\n","        all_documents = parse_htmls(item[\"search_results\"])\n","        \n","        # Get chunks\n","        all_chunks = extract_chunks(all_documents)\n","        \n","        print(\"=========== Document ===========\")\n","        print(\"# of Document Characters: \", len(all_documents[0]))\n","        print()\n","        print(all_documents[0])\n","        print()\n","        print(\"=========== Chunk ===========\")\n","        print(\"# of Chunk Characters: \", len(all_chunks[0]))\n","        print()\n","        print(all_chunks[0])\n","        print()\n","        break\n","```"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"le4XrVc1mP3A"},"outputs":[],"source":["### YOUR CODE HERE ###\n","\n","from google.colab import drive\n","\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OW7a67_BkYaB"},"outputs":[],"source":["### YOUR CODE HERE ###\n","\n","dataset_path = \"/content/drive/MyDrive/CRAG dataset/crag_task_1_dev_v4_release.jsonl\"\n","with open(dataset_path, \"rt\") as file:\n","    for line in file:\n","        item = json.loads(line)\n","\n","        # Get documents\n","        all_documents = parse_htmls(item[\"search_results\"])\n","\n","        # Get chunks\n","        all_chunks = extract_chunks(all_documents)\n","\n","        print(\"=========== Document ===========\")\n","        print(\"# of Document Characters: \", len(all_documents[0]))\n","        print()\n","        print(all_documents[0])\n","        print()\n","        print(\"=========== Chunk ===========\")\n","        print(\"# of Chunk Characters: \", len(all_chunks[0]))\n","        print()\n","        print(all_chunks[0])\n","        print()\n","        break"]},{"cell_type":"markdown","metadata":{"id":"98LPBiYQ-NKu"},"source":["As a result of the test, we observed that the length of the text was reduced by nearly **100 times** after splitting it into chunks compared to using the full search results. This suggests that chunks can effectively extract only the relevant parts from the `search_results` and pass them to the LLM efficiently.\n","\n","Of course, we cannot guarantee that the retriever will always retrieve relevant results. However, if we use entire documents as the retrieval unit, it will take a long time to compute embeddings, and information loss may occur during that process.\n","\n","Additionally, the length of the chunk characters may not exactly match the value of `MAX_CONTEXT_SENTENCE_LENGTH`. This is because we used the text_to_sentences_and_offsets function to ensure that chunks are formed without splitting sentences."]},{"cell_type":"markdown","metadata":{"id":"HZNPYuw0K9KC"},"source":["### 3. Implementing a Retriever\n","\n","This time, we will implement a **Retriever** using the Chunk Extractor we defined earlier, without relying on AI frameworks like LlamaIndex.  \n","\n","For this implementation, the following components are required:\n","\n","<br/>\n","\n","1.\t**Chunk extractor**: Used to split the input search_results into chunks.\n","2.\t**Embedding model**: Used to generate embeddings for the chunks and the query.\n","3.\t**Similarity metric**: Measures the similarity between embeddings. We will use cosine similarity here.\n","\n","Using these components, let‚Äôs implement the `BaseRetriever` with the following code:\n","\n","```Python\n","class BaseRetriever:\n","    def __init__(self,):\n","        self.client = openai.OpenAI(api_key = os.environ[\"OPENAI_API_KEY\"])\n","\n","    def embed_text(self, texts):\n","        \"\"\"Generate embeddings using OpenAI's embedding model.\"\"\"\n","        if isinstance(texts, str):\n","            texts = [texts]\n","\n","        response = self.client.embeddings.create(\n","            model=\"text-embedding-3-small\",\n","            input=texts\n","        )\n","\n","        # Extract embeddings correctly from the response object\n","        embeddings = [np.array(item.embedding) for item in response.data]  # Adjust based on actual attributes\n","        return np.array(embeddings)\n","\n","    def retrieve(self, query, search_results, topk):\n","        # Get documents\n","        all_documents = parse_htmls(search_results)\n","\n","        # Get chunks\n","        all_chunks = extract_chunks(all_documents)\n","\n","        # Generate embeddings for all chunks and the query.\n","        all_embeddings = self.embed_text(all_chunks)\n","        query_embedding = self.embed_text(query)[0]  # Single query embedding\n","\n","        # Calculate cosine similarity between query and sentence embeddings, and select the top sentences.\n","        cosine_scores = np.dot(all_embeddings, query_embedding) / (\n","            np.linalg.norm(all_embeddings, axis=1) * np.linalg.norm(query_embedding)\n","        )\n","        top_k_indices = (-cosine_scores).argsort()[:topk]\n","        top_k_chunks = np.array(all_chunks)[top_k_indices]\n","\n","        return top_k_chunks\n","```\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D3sX7arBYmKy"},"outputs":[],"source":["### YOUR CODE HERE ###\n","\n","class BaseRetriever:\n","    def __init__(self,):\n","        self.client = openai.OpenAI(api_key = os.environ[\"OPENAI_API_KEY\"])\n","\n","    def embed_text(self, texts):\n","        \"\"\"Generate embeddings using OpenAI's embedding model.\"\"\"\n","        if isinstance(texts, str):\n","            texts = [texts]\n","\n","        response = self.client.embeddings.create(\n","            model=\"text-embedding-3-small\",\n","            input=texts\n","        )\n","\n","        # Extract embeddings correctly from the response object\n","        embeddings = [np.array(item.embedding) for item in response.data]  # Adjust based on actual attributes\n","        return np.array(embeddings)\n","\n","    def retrieve(self, query, search_results, topk):\n","        # Get documents\n","        all_documents = parse_htmls(search_results)\n","\n","        # Get chunks\n","        all_chunks = extract_chunks(all_documents)\n","\n","        # Generate embeddings for all chunks and the query.\n","        all_embeddings = self.embed_text(all_chunks)\n","        query_embedding = self.embed_text(query)[0]  # Single query embedding\n","\n","        # Calculate cosine similarity between query and sentence embeddings, and select the top sentences.\n","        cosine_scores = np.dot(all_embeddings, query_embedding) / (\n","            np.linalg.norm(all_embeddings, axis=1) * np.linalg.norm(query_embedding)\n","        )\n","        top_k_indices = (-cosine_scores).argsort()[:topk]\n","        top_k_chunks = np.array(all_chunks)[top_k_indices]\n","\n","        return top_k_chunks"]},{"cell_type":"markdown","metadata":{"id":"4R9KzDF7DPtm"},"source":["The retriever determines the final chunks to return through three main steps.  \n","\n","1.\t**First Step**: The retriever takes the query, the search_results, and a variable topk (which determines how many chunks to return) as inputs. It then extracts chunks from the `search_results`.\n","2.\t**Second Step**: The extracted chunks are converted into embeddings using an embedding model. Since the chunks are in a list format, the embedding results will also be returned as a list. At the same time, the query is also converted into an embedding.\n","3. **Third Step**: **Cosine similarity** between the query‚Äôs embedding and the chunks‚Äô embeddings is calculated to determine which chunks have the highest similarity to the query.\n","\n","Through this process, our `BaseRetriever` retrieves and returns the `topk` chunks with the highest similarity.  \n","\n","Here is the code to verify the process:\n","\n","```\n","retriever = BaseRetriever()\n","topk = 5\n","dataset_path = \"/content/drive/MyDrive/CRAG dataset/crag_task_1_dev_v4_release.jsonl\"\n","\n","with open(dataset_path, \"rt\") as file:\n","    for line in file:\n","        item = json.loads(line)\n","        print(f\"query: {item['query']}\")\n","        print()\n","        retrieved_results = retriever.retrieve(item['query'], item['search_results'], topk)\n","        break\n","\n","print(\"retrieved results:\")\n","print()\n","for rank, retrieved_result in enumerate(retrieved_results):\n","    print(f\"rank {rank+1}: {retrieved_result}\")\n","    print()\n","```\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5685qgMxnQPs"},"outputs":[],"source":["### YOUR CODE HERE ###\n","\n","retriever = BaseRetriever()\n","topk = 5\n","dataset_path = \"/content/drive/MyDrive/CRAG dataset/crag_task_1_dev_v4_release.jsonl\"\n","\n","with open(dataset_path, \"rt\") as file:\n","    for line in file:\n","        item = json.loads(line)\n","        print(f\"query: {item['query']}\")\n","        print()\n","        retrieved_results = retriever.retrieve(item['query'], item['search_results'], topk)\n","        break\n","\n","print(\"retrieved results:\")\n","print()\n","for rank, retrieved_result in enumerate(retrieved_results):\n","    print(f\"rank {rank+1}: {retrieved_result}\")\n","    print()"]},{"cell_type":"markdown","metadata":{"id":"zFogBOkgfTkl"},"source":["### 3. Implementing a Retriever with Llama Index\n","\n","You may recall that in Day 1 practice, we defined a retriever using `LlamaIndex`.\n","\n","In this exercise, we will again define a retriever using LlamaIndex. To create a retriever with `LlamaIndex`, we must first build an index. To build the index, we need to decide which data to use ‚Äì in this case, we will use the `search_results`.\n","\n","Follow the code below to declare the retriever:\n","\n","```Python\n","from llama_index.core.schema import Document\n","from llama_index.core.node_parser import SentenceSplitter\n","from llama_index.core import VectorStoreIndex, Settings\n","from llama_index.embeddings.openai import OpenAIEmbedding\n","\n","Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")\n","\n","class LlamaIndexRetriever:\n","  def __init__(self):\n","      self.parser = SentenceSplitter(chunk_size=512, chunk_overlap=0)\n","\n","  def retrieve(self, query, search_results, topk):\n","      documents = []\n","\n","      for document in parse_htmls(search_results):\n","        if not document:\n","            # If no text is extracted, add an empty string as a placeholder.\n","            documents.append(Document(text=\"\"))\n","        else:\n","            documents.append(Document(text=document))\n","\n","      # Split documents into chunks & Create vector index\n","      base_index = VectorStoreIndex.from_documents(documents = documents, transformations=[self.parser])\n","\n","      # Execute query\n","      base_retriever = base_index.as_retriever(similarity_top_k=topk)\n","\n","      retrieved_nodes = base_retriever.retrieve(query)\n","\n","      retrieved_results = [retrieved_node.node.get_content().strip() for retrieved_node in retrieved_nodes]\n","\n","      return retrieved_results\n","```\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ypqI8b9gmcP5"},"outputs":[],"source":["### YOUR CODE HERE ###\n","\n","from llama_index.core.schema import Document\n","from llama_index.core.node_parser import SentenceSplitter\n","from llama_index.core import VectorStoreIndex, Settings\n","from llama_index.embeddings.openai import OpenAIEmbedding\n","\n","Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")\n","\n","class LlamaIndexRetriever:\n","  def __init__(self):\n","      self.parser = SentenceSplitter(chunk_size=512, chunk_overlap=0)\n","\n","  def retrieve(self, query, search_results, topk):\n","      documents = []\n","\n","      for document in parse_htmls(search_results):\n","        if not document:\n","            # If no text is extracted, add an empty string as a placeholder.\n","            documents.append(Document(text=\"\"))\n","        else:\n","            documents.append(Document(text=document))\n","\n","      # Split documents into chunks & Create vector index\n","      base_index = VectorStoreIndex.from_documents(documents = documents, transformations=[self.parser])\n","\n","      # Execute query\n","      base_retriever = base_index.as_retriever(similarity_top_k=topk)\n","\n","      retrieved_nodes = base_retriever.retrieve(query)\n","\n","      retrieved_results = [retrieved_node.node.get_content().strip() for retrieved_node in retrieved_nodes]\n","\n","      return retrieved_results"]},{"cell_type":"markdown","metadata":{"id":"V-Phoua0EeGk"},"source":["By leveraging an external AI framework like LlamaIndex, we can see that the code has become significantly more concise and streamlined.\n","\n","Now, let‚Äôs practice using the same approach with an example to verify how it works in action!\n","\n","```\n","retriever = LlamaIndexRetriever()\n","topk = 5\n","dataset_path = \"/content/drive/MyDrive/CRAG dataset/crag_task_1_dev_v4_release.jsonl\"\n","\n","with open(dataset_path, \"rt\") as file:\n","    for line in file:\n","        item = json.loads(line)\n","        print(f\"query: {item['query']}\")\n","        print()\n","        retrieved_results = retriever.retrieve(item['query'], item['search_results'], topk)\n","        break\n","\n","print(\"retrieved results:\")\n","print()\n","for rank, retrieved_result in enumerate(retrieved_results):\n","    print(f\"rank {rank}: {retrieved_result}\")\n","    print()\n","```"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9jDeBe4XpAca"},"outputs":[],"source":["### YOUR CODE HERE ###\n","\n","retriever = LlamaIndexRetriever()\n","topk = 5\n","dataset_path = \"/content/drive/MyDrive/CRAG dataset/crag_task_1_dev_v4_release.jsonl\"\n","\n","with open(dataset_path, \"rt\") as file:\n","    for line in file:\n","        item = json.loads(line)\n","        print(f\"query: {item['query']}\")\n","        print()\n","        retrieved_results = retriever.retrieve(item['query'], item['search_results'], topk)\n","        break\n","\n","print(\"retrieved results:\")\n","print()\n","for rank, retrieved_result in enumerate(retrieved_results):\n","    print(f\"rank {rank}: {retrieved_result}\")\n","    print()"]},{"cell_type":"markdown","metadata":{"id":"FbeB2pmznh3X"},"source":["## II. Implementing a Reader\n","\n","In this section, we will design the **Reader**.\n","\n","What are the most important considerations when creating a Reader? The most crucial factor is likely the choice of LLM. Factors such as model size, performance on reasoning benchmarks, cost, and other considerations are typically part of the configuration.\n","\n","However, since we have limited options for the LLMs we can use in this practice session, this will not be a consideration for us here.\n","\n","So, what‚Äôs the next most important factor? **Prompt design**. It is well known that well-designed prompts lead to better results from the LLM.\n","\n","Moreover, setting an appropriate prompt becomes even more critical for the CRAG dataset. In this task, the LLM must be able to answer ‚ÄúI don‚Äôt know‚Äù if it encounters something it is unsure about or cannot answer confidently. To achieve this, the prompt must be specifically designed to guide the LLM to behave in this manner.\n","\n","Therefore, this exercise will be conducted in the following three main stages:\n","\n","1. Design a Prompt Template\n","2. Implement a Prompt Generator\n","3. Implement a Reader"]},{"cell_type":"markdown","metadata":{"id":"AV1ZcVEiVaJ2"},"source":["### 1. Design a Prompt Template\n","\n","To design an effective prompt template, we need to carefully consider certain factors.\n","\n","1.\tThe response must be generated based on the given question and references.\n","2.\tIn the CRAG benchmark, answers should not be too long or verbose. During evaluation, only the first 75 tokens are used for scoring, so the response needs to be concise.\n","3.\tThe LLM must be able to recognize questions it cannot answer and respond with ‚ÄúI don‚Äôt know‚Äù.\n","\n","Taking these factors into account, we can draft the following `system_prompt`:\n","\n","```Python\n","\n","system_prompt = \"\"\"\n","You are provided with a question and various references.\n","Your task is to answer the question succinctly, using the fewest words possible.\n","If the references do not contain the necessary information to answer the question, respond with 'I don't know'.\n","There is no need to explain the reasoning behind your answers.\n","\"\"\"\n","```"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ObtbXbQBpFIp"},"outputs":[],"source":["### YOUR CODE HERE ###\n","\n","system_prompt = \"\"\"\n","You are provided with a question and various references.\n","Your task is to answer the question succinctly, using the fewest words possible.\n","If the references do not contain the necessary information to answer the question, respond with 'I don't know'.\n","There is no need to explain the reasoning behind your answers.\n","\"\"\""]},{"cell_type":"markdown","metadata":{"id":"tbP_U8iXWgNK"},"source":["### 2. Implement a Prompt Generator\n","\n","Above, we created a system prompt. Now, we need to build a prompt generator that takes the question and reference, combines them into one, and formats it so it can be passed to the LLM.\n","\n","Below is an example of a `prompt_generator` that takes a question and reference, combines them for delivery to the LLM:\n","\n","```Python\n","def prompt_generator(query, top_k_chunks, system_prompt):\n","    user_message = \"\"\n","    references = \"\"\n","\n","    if len(top_k_chunks) > 0:\n","        references += \"# References \\n\"\n","        # Format the top sentences as references in the model's prompt template.\n","        for chunk_id, chunk in enumerate(top_k_chunks):\n","            references += f\"- {chunk.strip()}\\n\"\n","\n","    references = references[:MAX_CONTEXT_REFERENCES_LENGTH]\n","    # Limit the length of references to fit the model's input size.\n","\n","    user_message += f\"{references}\\n------\\n\\n\"\n","    user_message += f\"Using only the references listed above, answer the following question: \\n\"\n","    user_message += f\"Question: {query}\\n\"\n","\n","    llm_input = [\n","      {\"role\": \"system\", \"content\": system_prompt},\n","      {\"role\": \"user\", \"content\": user_message},\n","    ]\n","\n","    return llm_input\n","\n","```"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9uniz5hrXrGI"},"outputs":[],"source":["### YOUR CODE HERE ###\n","\n","def prompt_generator(query, top_k_chunks, system_prompt):\n","    user_message = \"\"\n","    references = \"\"\n","\n","    if len(top_k_chunks) > 0:\n","        references += \"# References \\n\"\n","        # Format the top sentences as references in the model's prompt template.\n","        for chunk_id, chunk in enumerate(top_k_chunks):\n","            references += f\"- {chunk.strip()}\\n\"\n","\n","    references = references[:MAX_CONTEXT_REFERENCES_LENGTH]\n","    # Limit the length of references to fit the model's input size.\n","\n","    user_message += f\"{references}\\n------\\n\\n\"\n","    user_message += f\"Using only the references listed above, answer the following question: \\n\"\n","    user_message += f\"Question: {query}\\n\"\n","\n","    llm_input = [\n","      {\"role\": \"system\", \"content\": system_prompt},\n","      {\"role\": \"user\", \"content\": user_message},\n","    ]\n","\n","    return llm_input"]},{"cell_type":"markdown","metadata":{"id":"UDbMms2ApXZm"},"source":["### 3. Implement a Reader\n","\n","Now that we have created a function to generate the necessary prompts for the Reader, we will proceed to define the Reader itself and set up the components needed for RAG creation.\n","\n","Follow the code below to implement it.\n","\n","```Python\n","from openai import OpenAI\n","\n","oai_client = OpenAI()\n","\n","class Reader:\n","  def __init__(self):\n","\n","    self.system_prompt = \"\"\"\n","    You are provided with a question and various references.\n","    Your task is to answer the question succinctly, using the fewest words possible.\n","    If the references do not contain the necessary information to answer the question, respond with 'I don't know'.\n","    There is no need to explain the reasoning behind your answers.\n","    \"\"\"\n","\n","  def generate_response(self, query: str, top_k_chunks: list) -> str:\n","      \"\"\"\n","      Generate answer from context.\n","      \"\"\"\n","      llm_input = self.prompt_generator(query, top_k_chunks)\n","      completion = oai_client.chat.completions.create(\n","      model=\"gpt-3.5-turbo\",\n","      temperature=0,\n","      messages=\n","      llm_input\n","      ).choices[0].message.content\n","      return completion\n","\n","  def prompt_generator(self, query, top_k_chunks):\n","      user_message = \"\"\n","      references = \"\"\n","\n","      if len(top_k_chunks) > 0:\n","          references += \"# References \\n\"\n","          # Format the top sentences as references in the model's prompt template.\n","          for chunk_id, chunk in enumerate(top_k_chunks):\n","              references += f\"- {chunk.strip()}\\n\"\n","      \n","      references = references[:MAX_CONTEXT_REFERENCES_LENGTH]\n","      # Limit the length of references to fit the model's input size.\n","\n","      user_message += f\"{references}\\n------\\n\\n\"\n","      user_message\n","      user_message += f\"Using only the references listed above, answer the following question: \\n\"\n","      user_message += f\"Question: {query}\\n\"\n","\n","      llm_input = [\n","        {\"role\": \"system\", \"content\": self.system_prompt},\n","        {\"role\": \"user\", \"content\": user_message},\n","      ]\n","\n","      return llm_input\n","```\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J59VqE5-al91"},"outputs":[],"source":["### YOUR CODE HERE ###\n","\n","from openai import OpenAI\n","\n","oai_client = OpenAI()\n","\n","class Reader:\n","  def __init__(self):\n","\n","    self.system_prompt = \"\"\"\n","    You are provided with a question and various references.\n","    Your task is to answer the question succinctly, using the fewest words possible.\n","    If the references do not contain the necessary information to answer the question, respond with 'I don't know'.\n","    There is no need to explain the reasoning behind your answers.\n","    \"\"\"\n","\n","  def generate_response(self, query: str, top_k_chunks: list) -> str:\n","      \"\"\"\n","      Generate answer from context.\n","      \"\"\"\n","      llm_input = self.prompt_generator(query, top_k_chunks)\n","      completion = oai_client.chat.completions.create(\n","      model=\"gpt-3.5-turbo\",\n","      temperature=0,\n","      messages=\n","      llm_input\n","      ).choices[0].message.content\n","      return completion\n","\n","  def prompt_generator(self, query, top_k_chunks):\n","      user_message = \"\"\n","      references = \"\"\n","\n","      if len(top_k_chunks) > 0:\n","          references += \"# References \\n\"\n","          # Format the top sentences as references in the model's prompt template.\n","          for chunk_id, chunk in enumerate(top_k_chunks):\n","              references += f\"- {chunk.strip()}\\n\"\n","\n","      references = references[:MAX_CONTEXT_REFERENCES_LENGTH]\n","      # Limit the length of references to fit the model's input size.\n","\n","      user_message += f\"{references}\\n------\\n\\n\"\n","      user_message\n","      user_message += f\"Using only the references listed above, answer the following question: \\n\"\n","      user_message += f\"Question: {query}\\n\"\n","\n","      llm_input = [\n","        {\"role\": \"system\", \"content\": self.system_prompt},\n","        {\"role\": \"user\", \"content\": user_message},\n","      ]\n","\n","      return llm_input"]},{"cell_type":"markdown","metadata":{"id":"0uBdH4d9Sr-S"},"source":["Now, let‚Äôs check the results through an actual example.\n","\n","```\n","reader = Reader()\n","dataset_path = \"/content/drive/MyDrive/CRAG dataset/crag_task_1_dev_v4_release.jsonl\"\n","\n","with open(dataset_path, \"rt\") as file:\n","    for line in file:\n","        item = json.loads(line)\n","        print(f\"query: {item['query']}\")\n","        print(f\"ground truth: {item['answer']}\")\n","        print()\n","        answer = reader.generate_response(item['query'], [])\n","        break\n","\n","print(f\"answer: {answer}\")\n","```\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xaPEFeZ92hh6"},"outputs":[],"source":["### YOUR CODE HERE ###\n","\n","reader = Reader()\n","dataset_path = \"/content/drive/MyDrive/CRAG dataset/crag_task_1_dev_v4_release.jsonl\"\n","\n","with open(dataset_path, \"rt\") as file:\n","    for line in file:\n","        item = json.loads(line)\n","        print(f\"query: {item['query']}\")\n","        print(f\"ground truth: {item['answer']}\")\n","        print()\n","        answer = reader.generate_response(item['query'], [])\n","        break\n","\n","print(f\"answer: {answer}\")"]},{"cell_type":"markdown","metadata":{"id":"KVHLibfMGMy_"},"source":["## III. Implementing a RAG\n","\n","At this point, we have defined both the Reader and the Retriever, and we have verified their inputs and outputs.\n","\n","Now, let‚Äôs combine these two components into a functional RAG system that we can use.\n","\n","```\n","class RAG:\n","    def __init__(self):\n","        self.retriever = LlamaIndexRetriever()\n","        self.reader = Reader()\n","  \n","    def inference(self, query, search_results, topk):\n","        # 1. retrieve relevant chunks\n","        retrieved_results = self.retriever.retrieve(query, search_results, topk)\n","\n","        # 2. answer the question based on the retrieved chunks\n","        answer = self.reader.generate_response(query, retrieved_results)\n","\n","        return answer, retrieved_results\n","\n","```\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ikmQ42W1PRUt"},"outputs":[],"source":["### YOUR CODE HERE ###\n","\n","class RAG:\n","    def __init__(self):\n","        self.retriever = LlamaIndexRetriever()\n","        self.reader = Reader()\n","\n","    def inference(self, query, search_results, topk):\n","        # 1. retrieve relevant chunks\n","        retrieved_results = self.retriever.retrieve(query, search_results, topk)\n","\n","        # 2. answer the question based on the retrieved chunks\n","        answer = self.reader.generate_response(query, retrieved_results)\n","\n","        return answer, retrieved_results"]},{"cell_type":"markdown","metadata":{"id":"zByVLutpTnXX"},"source":["Let‚Äôs now verify whether the RAG system we defined works as intended or not.\n","\n","Using the code below, we will test the system on a total of 10 data points. You can check each result yourself and evaluate whether the RAG performs well enough.\n","\n","```\n","rag = RAG()\n","topk = 5\n","dataset_path = \"/content/drive/MyDrive/CRAG dataset/crag_task_1_dev_v4_release.jsonl\"\n","\n","repeat = 0\n","with open(dataset_path, \"rt\") as file:\n","    for line in file:\n","        if repeat > 9:\n","          break\n","        \n","        item = json.loads(line)\n","        print(f\"query: {item['query']}\")\n","        print()\n","        answer = rag.inference(item['query'], item['search_results'], topk)[0]\n","        print(f\"predicted answer: {answer}\")\n","        print(f\"ground truth answer: {item['answer']}\")\n","        print()\n","        repeat += 1\n","```\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-i6vlXeI3Mmp"},"outputs":[],"source":["### YOUR CODE HERE ###\n","\n","rag = RAG()\n","topk = 5\n","dataset_path = \"/content/drive/MyDrive/CRAG dataset/crag_task_1_dev_v4_release.jsonl\"\n","\n","repeat = 0\n","with open(dataset_path, \"rt\") as file:\n","    for line in file:\n","        if repeat > 9:\n","          break\n","\n","        item = json.loads(line)\n","        print(f\"query: {item['query']}\")\n","        print()\n","        answer = rag.inference(item['query'], item['search_results'], topk)[0]\n","        print(f\"predicted answer: {answer}\")\n","        print(f\"ground truth answer: {item['answer']}\")\n","        print()\n","        repeat += 1"]},{"cell_type":"markdown","metadata":{"id":"rNNOCLNt3Ypx"},"source":["## IV. Error case analysis\n","\n","Some of you may be satisfied with the experimental results above, while others may not. However, few would believe that the RAG system produced the correct answer for all questions.\n","\n","Therefore, before formally evaluating the RAG system, we will check which questions it answered incorrectly and try to understand why those results occurred. To do this, we need to classify the data into two categories:\n","\n","1.\tQuestions the RAG answered correctly.\n","2.\tQuestions the RAG answered incorrectly.\n","\n","Ultimately, before moving on to Task 2 in the next session, we will execute the RAG implemented for Task 1 and analyze which queries the system struggles to answer correctly.\n","\n","To begin, let‚Äôs check how well the Reader alone performs on the following questions, without using search results.\n","<br/>  \n","Question: **In 2004, which animated film was recognized with the best animated feature film oscar?**.   \n","Answer: **Finding Nemo**\n","<br/>\n","\n","\n","```\n","dataset_path = \"/content/drive/MyDrive/CRAG dataset/crag_task_1_dev_v4_release.jsonl\"\n","\n","repeat = 0\n","with open(dataset_path, \"rt\") as file:\n","    for line in file:\n","        if repeat != 5:\n","          repeat += 1\n","          continue\n","        \n","        item = json.loads(line)\n","        print(f\"query: {item['query']}\")\n","        print()\n","        answer = reader.generate_response(item['query'], [])\n","        print(f\"predicted answer: {answer}\")\n","        print(f\"ground truth answer: {item['answer']}\")\n","        print()\n","        repeat += 1\n","        break\n","```\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tsvz1mgAfimM"},"outputs":[],"source":["### YOUR CODE HERE ###\n","\n","dataset_path = \"/content/drive/MyDrive/CRAG dataset/crag_task_1_dev_v4_release.jsonl\"\n","\n","repeat = 0\n","with open(dataset_path, \"rt\") as file:\n","    for line in file:\n","        if repeat != 5:\n","          repeat += 1\n","          continue\n","\n","        item = json.loads(line)\n","        print(f\"query: {item['query']}\")\n","        print()\n","        answer = reader.generate_response(item['query'], [])\n","        print(f\"predicted answer: {answer}\")\n","        print(f\"ground truth answer: {item['answer']}\")\n","        print()\n","        repeat += 1\n","        break"]},{"cell_type":"markdown","metadata":{"id":"5hAUpDSyhtVJ"},"source":["Although the correct answer is **‚ÄúFinding Nemo‚Äù**, the model generated the incorrect answer, **‚ÄúThe Incredibles‚Äù**.\n","\n","Next, let‚Äôs check the generated result when search results are utilized.\n","\n","\n","```\n","dataset_path = \"/content/drive/MyDrive/CRAG dataset/crag_task_1_dev_v4_release.jsonl\"\n","\n","rag = RAG()\n","topk = 5\n","\n","repeat = 0\n","with open(dataset_path, \"rt\") as file:\n","    for line in file:\n","        if repeat != 5:\n","          repeat += 1\n","          continue\n","        \n","        item = json.loads(line)\n","        print(f\"query: {item['query']}\")\n","        print()\n","        answer, retrieved_results = rag.inference(item['query'], item['search_results'], topk)\n","        print(f\"predicted answer: {answer}\")\n","        print(f\"ground truth answer: {item['answer']}\")\n","        print()\n","        print(\"retrieved results:\")\n","        for rank, retrieved_result in enumerate(retrieved_results):\n","            print(f\"{rank}: {retrieved_result}\")\n","        print()\n","        repeat += 1\n","        break\n","```\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ulSkV4uUiGYI"},"outputs":[],"source":["### YOUR CODE HERE ###\n","\n","dataset_path = \"/content/drive/MyDrive/CRAG dataset/crag_task_1_dev_v4_release.jsonl\"\n","\n","rag = RAG()\n","topk = 5\n","\n","repeat = 0\n","with open(dataset_path, \"rt\") as file:\n","    for line in file:\n","        if repeat != 5:\n","          repeat += 1\n","          continue\n","\n","        item = json.loads(line)\n","        print(f\"query: {item['query']}\")\n","        print()\n","        answer, retrieved_results = rag.inference(item['query'], item['search_results'], topk)\n","        print(f\"predicted answer: {answer}\")\n","        print(f\"ground truth answer: {item['answer']}\")\n","        print()\n","        print(\"retrieved results:\")\n","        for rank, retrieved_result in enumerate(retrieved_results):\n","            print(f\"{rank}: {retrieved_result}\")\n","        print()\n","        repeat += 1\n","        break"]},{"cell_type":"markdown","metadata":{"id":"FnVsb-k14ZCi"},"source":["The following queries focus on retrieving information related to finance.\n","\n","Such information is typically stored in structured data formats, such as tables or knowledge graphs. However, unstructured data sources, like web search results, often overlook the structural information inherent in tables or knowledge graphs, making it challenging to extract specific information efficiently.\n","\n","For instance, financial data such as Microsoft's ex-dividend date, P/E ratio, or earnings per share is usually presented in numeric, date, or tabular formats. In contrast, text-based data lacks the structured representation found in tables, making it harder to leverage such information.\n","\n","Let us explore whether RAG (Retrieval-Augmented Generation) can effectively answer the following queries using only web search results.\n","\n","<br/>  \n","Question: **What is the ex-dividend date of microsoft in the 1st qtr of 2024**.   \n","Answer: **The ex-dividend date of microsoft in the 1st qtr of 2024 is feb 14, 2024**\n","<br/>\n","\n","<br/>  \n","Question: **I'm looking for the p/e ratio of dks. would you happen to know what it is?**.   \n","Answer: **13.75**\n","<br/>\n","\n","<br/>  \n","Question: **What's auph's earnings per share?**.   \n","Answer: **0.4**\n","<br/>\n","\n","\n","\n","```Python\n","dataset_path = \"/content/drive/MyDrive/CRAG dataset/crag_task_1_dev_v4_release.jsonl\"\n","\n","rag = RAG()\n","topk = 5\n","\n","repeat = 0\n","with open(dataset_path, \"rt\") as file:\n","    for line in file:\n","        if repeat not in [14, 53, 64]:\n","          repeat += 1\n","          continue\n","\n","        item = json.loads(line)\n","        print(f\"query: {item['query']}\")\n","        print()\n","        answer, retrieved_results = rag.inference(item['query'], item['search_results'], topk)\n","        print(f\"predicted answer: {answer}\")\n","        print(f\"ground truth answer: {item['answer']}\")\n","        print()\n","        print(\"retrieved results:\")\n","        for rank, retrieved_result in enumerate(retrieved_results):\n","            print(f\"{rank}: {retrieved_result}\")\n","        print()\n","        repeat += 1\n","```\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Dya6MQwm8H-V"},"outputs":[],"source":["### YOUR CODE HERE ###\n","\n","dataset_path = \"/content/drive/MyDrive/CRAG dataset/crag_task_1_dev_v4_release.jsonl\"\n","\n","rag = RAG()\n","topk = 5\n","\n","repeat = 0\n","with open(dataset_path, \"rt\") as file:\n","    for line in file:\n","        if repeat not in [14, 53, 64]:\n","          repeat += 1\n","          continue\n","\n","        item = json.loads(line)\n","        print(f\"query: {item['query']}\")\n","        print()\n","        answer, retrieved_results = rag.inference(item['query'], item['search_results'], topk)\n","        print(f\"predicted answer: {answer}\")\n","        print(f\"ground truth answer: {item['answer']}\")\n","        print()\n","        print(\"retrieved results:\")\n","        for rank, retrieved_result in enumerate(retrieved_results):\n","            print(f\"{rank}: {retrieved_result}\")\n","        print()\n","        repeat += 1"]}],"metadata":{"colab":{"provenance":[{"file_id":"https://github.com/truera/trulens/blob/main/trulens_eval/examples/quickstart/quickstart.ipynb","timestamp":1711609044141}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.18"}},"nbformat":4,"nbformat_minor":0}