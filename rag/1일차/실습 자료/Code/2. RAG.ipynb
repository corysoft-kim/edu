{"cells":[{"cell_type":"markdown","metadata":{"id":"TXSKbyvh2QaU"},"source":["#ðŸ““ RAG (Retrieval-Augmented Generation) Practice\n","\n","This is today's main section! In this class, we delve into RAG, the Retrieval-Augmented Generation (RAG) technique. RAG is the process of optimizing the output of a large language model, so it references an external knowledge base outside of its training data sources before generating a response.\n","\n","Retrieval-Augmented Generation combines the power of language models with the specificity of retrieved documents to generate more accurate and relevant responses. This technique retrieves relevant documents or passages based on the input query and uses this retrieved information to inform the generation process of the language model.  \n","\n","Let's look at the picture below and remind the structure of the RAG.\n","<img src=\"https://i.imgur.com/bR4xaBd.png\">\n","###Steps in RAG\n","1. The model receives an input query to which a response is required.\n","2. It retrieves relevant documents or passages from a database that are pertinent to the input query.\n","3. The language model then generates a response, informed by both the original query and the retrieved documents.\n","\n","\n","![](https://huggingface.co/blog/assets/12_ray_rag/rag_gif.gif)\n","\n"]},{"cell_type":"markdown","source":["This practice class will be comprised of three sections.  \n","  \n","### I. Building a Prototype RAG.  \n","### II. Finding the best RAG App Configuration.\n","  \n","Okay. Now we know what we have to do.  \n","Then, is it okay to dive into practice?\n","\n","![](https://media.tenor.com/r2l6ol9HRqIAAAAj/question-mark-question.gif)"],"metadata":{"id":"duLH-aV7Evhl"}},{"cell_type":"markdown","source":["## 0. Before Dive into Practice!\n","\n","We can easily infer that models often give out strange answers to complex questions. However, this is only natural because people are the same.\n","\n","So exactly what kind of questions does LLM generate the wrong answers in? If we find out what kind of questions LLM is vulnerable to, we will have a better understanding of how to supplement LLM.\n","\n","<br/>\n","\n","We follow the steps below:  \n","\n","#### 1. Preparing for OpenAI API\n","#### 2. Make a Method for Asking Question\n","#### 3. Ask to Query Engine\n"],"metadata":{"id":"pGvoJusEMYuq"}},{"cell_type":"markdown","source":["### 1. Preparing for OpenAI API\n","  \n","We will use OpenAI LLM for this acitivity. So, we need to install openai api library, import it, and set openai api key.\n","\n","Copy the api key you have to the 'sk-...' location. Note that all api keys start with 'sk-'.\n","\n","\n","```Python\n","! pip install llama-index==0.12.2 --quiet\n","! pip install llama-index-readers-wikipedia==0.3.0 wikipedia==1.4.0 --quiet\n","! pip install llama-index-llms-openai==0.3.1 --quiet\n","! pip install openai==1.55.3 --quiet\n","! pip install trulens==1.2.0 trulens-providers-openai==1.2.0 --quiet\n","! pip install packaging==23.2 langchain nltk>=3.8.1 streamlit==1.35.0 watchdog kubernetes==26.1.0 --quiet\n","! pip uninstall numpy -y\n","! pip install numpy==2.0.2\n","```\n","```Python\n","import os\n","import openai\n","\n","os.environ[\"OPENAI_API_KEY\"] = \"sk-...\" #copy your api key\n","\n","from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Document, get_response_synthesizer\n","from llama_index.readers.wikipedia import WikipediaReader\n","from llama_index.core.node_parser import SentenceSplitter\n","\n","import textwrap\n","\n","import nltk\n","nltk.download('punkt')\n","```"],"metadata":{"id":"HrqPxwa4OOe8"}},{"cell_type":"code","source":["### YOUR CODE HERE ###\n","\n","! pip install llama-index==0.12.2 --quiet\n","! pip install llama-index-readers-wikipedia==0.3.0 wikipedia==1.4.0 --quiet\n","! pip install llama-index-llms-openai==0.3.1 --quiet\n","! pip install openai==1.55.3 --quiet\n","! pip install trulens==1.2.0 trulens-providers-openai==1.2.0 --quiet\n","! pip install packaging==23.2 langchain nltk>=3.8.1 streamlit==1.35.0 watchdog kubernetes==26.1.0 --quiet\n","! pip uninstall numpy -y\n","! pip install numpy==2.0.2"],"metadata":{"id":"fTJAIMKnOfe0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### YOUR CODE HERE ###\n","\n","import os\n","import openai\n","\n","os.environ[\"OPENAI_API_KEY\"] = \"sk-...\" #copy your api key\n","\n","from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Document, get_response_synthesizer\n","from llama_index.readers.wikipedia import WikipediaReader\n","from llama_index.core.node_parser import SentenceSplitter\n","\n","import textwrap\n","\n","import nltk\n","nltk.download('punkt')"],"metadata":{"id":"S8Ig_PSVGW_G"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 2. Make a Method for Asking Question\n","  \n","Using the OpenAI API, we will create a function that receives a question and returns the response of the OpenAI LLM.\n","\n","```Python\n","def generate_answer(question):\n","    messages = [\n","        {\n","            \"role\": \"system\",\n","            \"content\": \"You are a helpful assistant.\"\n","        },\n","        {\n","            \"role\": \"user\",\n","            \"content\": question,\n","        },\n","    ]\n","\n","    response = openai.chat.completions.create(\n","        model=\"gpt-3.5-turbo\",  \n","        messages=messages,\n","    )\n","    \n","    return response.choices[0].message.content\n","```\n"],"metadata":{"id":"6ouB4AuyGWsk"}},{"cell_type":"code","source":["### YOUR CODE HERE ###\n","\n","def generate_answer(question):\n","    messages = [\n","        {\n","            \"role\": \"system\",\n","            \"content\": \"You are a helpful assistant.\"\n","        },\n","        {\n","            \"role\": \"user\",\n","            \"content\": question,\n","        },\n","    ]\n","\n","    response = openai.chat.completions.create(\n","        model=\"gpt-3.5-turbo\",\n","        messages=messages,\n","    )\n","\n","    return response.choices[0].message.content"],"metadata":{"id":"lpCFzMHMK9rV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 3. Ask to Query Engine\n","\n","Let's pose some questions to the OpenAI LLM. We will ask the following types of questions:\n","\n","#### 1. Question about **old information** vs Question about **recent information**\n","#### 2. Question that **require only simple information** vs Question that **require complex reasoning**\n","#### 3. Question that are **too vague** vs Question that **clearly specify the desired task**\n","\n","\n","<br/>\n","\n","#### 1. Question about **old information** vs Question about **recent information**\n","\n","Question about old information: 'When did Blackpink make their debut?\n","'  \n","Answer: 2016\n","\n","Question about recent information: 'What is the lowest pressure at which diamonds have been created in the world?'  \n","Answer: 1 atmospheric pressure\n","\n","<img src=\"https://i.imgur.com/ZaHVekw.png\" width=\"600\">\n","\n","<br/>\n","\n","#### 2. Question that **require only simple information** vs Question that **require complex reasoning**\n","\n","Question that require only simple information: 'What was the first animal to orbit the Earth?'  \n","Answer: Laika\n","\n","Question that require complex reasoning: 'In what country was the current World\\'s Strongest Man born?'  \n","\n","Reasoning step:  \n","1. Who is the current World\\'s Strongest Man?\n","2. Which country was the current World\\'s Strongest Man born in?\n","\n","Answer: Canada  \n","\n","<br/>\n","\n","#### 3. Question that are **too vague** vs Question that **clearly specify the desired task**\n","\n","Question that are too vague: 'I want to cook something.'  \n","\n","Question that clearly specify the desired task: 'Explain the steps to make apple pie step by step'\n","\n","Answer: Compare two responses by yourself  \n","\n","\n","```Python\n","question_old =  'When did Blackpink make their debut? '\n","question_recent = 'What is the lowest pressure at which diamonds have been created in the world?'\n","\n","question_simple = 'What was the first animal to orbit the Earth?'\n","question_complex = 'In what country was the current World\\'s Strongest Man born?'\n","\n","question_clear = 'Explain the steps to make something step by step.'\n","question_vague = 'I want to cook something.'\n","```"],"metadata":{"id":"HZNPYuw0K9KC"}},{"cell_type":"code","source":["### YOUR CODE HERE ###\n","\n","question_old =  'When did Blackpink make their debut? '\n","question_recent = 'What is the lowest pressure at which diamonds have been created in the world?'\n","\n","question_simple = 'What was the first animal to orbit the Earth?'\n","question_complex = 'In what country was the current World\\'s Strongest Man born?'\n","\n","question_clear = 'Explain the steps to make something step by step.'\n","question_vague = 'I want to cook something.'"],"metadata":{"id":"D3sX7arBYmKy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 4. Visualizaing Result\n","\n","Let's check the results by adding a question to the previously made `generate_answer` function.\n","\n","Also, we will use `pretty_print` method to automatically change the line at a certain length in case the answer is too long.\n","\n","```Python\n","import textwrap\n","\n","def pretty_print(answer):\n","    wrapped_text = textwrap.fill(answer, width=80)\n","    return wrapped_text\n","```\n","\n","```Python\n","answer_old = generate_answer(question_old)\n","print(\"Answer to question with old information: \\n\", pretty_print(answer_old))\n","print(\"------------------------------------------------------------\")\n","print(\"Real Answer: 2016\")\n","```\n","```Python\n","answer_recent = generate_answer(question_recent)\n","print(\"Answer to question with recent information: \\n\", pretty_print(answer_recent))\n","print(\"------------------------------------------------------------\")\n","print(\"Real Answer: 1 atmospheric pressure\")\n","```\n","```Python\n","answer_simple = generate_answer(question_simple)\n","print(\"Answer to a simple question: \\n\", pretty_print(answer_simple))\n","print(\"------------------------------------------------------------\")\n","print(\"Real Answer: Laika\")\n","```\n","```Python\n","answer_complex = generate_answer(question_complex)\n","print(\"Answer to a complex question: \\n\", pretty_print(answer_complex))\n","print(\"------------------------------------------------------------\")\n","print(\"Real Answer: Canada\")\n","```\n","```Python\n","answer_clear = generate_answer(question_clear)\n","print(\"Response to a clear question: \\n\", pretty_print(answer_clear))\n","print(\"------------------------------------------------------------\")\n","print(\"\\nThink about this results. Does the result fit the purpose of the question?\")\n","```\n","```Python\n","answer_vague = generate_answer(question_vague)\n","print(\"Response to a vague question: \\n\", pretty_print(answer_vague))\n","print(\"------------------------------------------------------------\")\n","print(\"\\nThink about this results. Does the result fit the purpose of the question?\")\n","```"],"metadata":{"id":"zFogBOkgfTkl"}},{"cell_type":"code","source":["### YOUR CODE HERE ###\n","\n","import textwrap\n","\n","def pretty_print(answer):\n","    wrapped_text = textwrap.fill(answer, width=80)\n","    return wrapped_text"],"metadata":{"id":"ypqI8b9gmcP5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### YOUR CODE HERE ###\n","\n","answer_old = generate_answer(question_old)\n","print(\"Answer to question with old information: \\n\", pretty_print(answer_old))\n","print(\"------------------------------------------------------------\")\n","print(\"Real Answer: 2016\")"],"metadata":{"id":"ZmMPPlJxfSqd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### YOUR CODE HERE ###\n","\n","answer_recent = generate_answer(question_recent)\n","print(\"Answer to question with recent information: \\n\", pretty_print(answer_recent))\n","print(\"------------------------------------------------------------\")\n","print(\"Real Answer: 1 atmospheric pressure\")"],"metadata":{"id":"5hIRt953kRSS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### YOUR CODE HERE ###\n","\n","answer_simple = generate_answer(question_simple)\n","print(\"Answer to a simple question: \\n\", pretty_print(answer_simple))\n","print(\"------------------------------------------------------------\")\n","print(\"Real Answer: Laika\")"],"metadata":{"id":"5uJn_sI2lZwx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### YOUR CODE HERE ###\n","\n","answer_complex = generate_answer(question_complex)\n","print(\"Answer to a complex question: \\n\", pretty_print(answer_complex))\n","print(\"------------------------------------------------------------\")\n","print(\"Real Answer: Canada\")"],"metadata":{"id":"mnwrgxxvlX3s"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### YOUR CODE HERE ###\n","\n","answer_clear = generate_answer(question_clear)\n","print(\"Response to a clear question: \\n\", pretty_print(answer_clear))\n","print(\"------------------------------------------------------------\")\n","print(\"\\nThink about this results. Does the result fit the purpose of the question?\")"],"metadata":{"id":"49k_VZRXldh0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### YOUR CODE HERE ###\n","\n","answer_vague = generate_answer(question_vague)\n","print(\"Response to a vague question: \\n\", pretty_print(answer_vague))\n","print(\"------------------------------------------------------------\")\n","print(\"\\nThink about this results. Does the result fit the purpose of the question?\")"],"metadata":{"id":"kimDezd5lbZc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["All right, we've now seen what kind of question LLM is vulnerable to and the importance of a clear prompt.\n","\n","Finally, before diving into the practice, let's ask LLM complex questions related to the city. We will look at LLM's answers and compare them with those of LLM reinforced by RAG.  \n","\n","<br/>\n","\n","Question: **Which Korean city has a sisterhood relationship with Belize City?**.   \n","Answer: **Yeosu**\n","\n","<br/>\n","\n","<img src=\"https://media.tenor.com/TWMxi0kGDTgAAAAi/hmm.gif\" width=\"150\" />  \n","\n","\n","Hmm... I think it is quite complex question. Could you answer this question?  \n","If you can, feel free to think of other questions related to the city.  \n","\n","<br/>\n","\n","However, make sure there is a Wikipedia page for that city, and if so, save the title of that Wikipedia page in the variable below.  \n","\n","```Python\n","city_question = 'Which Korean city has a sisterhood relationship with Belize City?.'\n","\n","kr_city = generate_answer(city_question)\n","print(\"Answer: \", pretty_print(kr_city))\n","\n","city_related_to_question = 'Belize City'\n","```\n"],"metadata":{"id":"FbeB2pmznh3X"}},{"cell_type":"code","source":["### YOUR CODE HERE ###\n","\n","city_question = 'Which Korean city has a sisterhood relationship with Belize City?.'\n","\n","kr_city = generate_answer(city_question)\n","print(\"Answer: \", pretty_print(kr_city))\n","\n","city_related_to_question = 'Belize City'"],"metadata":{"id":"ObtbXbQBpFIp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["If you remembered the answer above, let's start making RAG!"],"metadata":{"id":"UDbMms2ApXZm"}},{"cell_type":"markdown","source":["## I. Building a Prototype RAG.  \n","\n","In this section, we will build the Retriever and Database for use in RAG using the LlamaIndex and Milvus that we learned in the previous session.\n","\n","What makes it different from previous practice is that this time, we go through the process of directly selecting the information and the external knowledge source and inserting it into the database. In this practice, we will make the following assumptions.   \n","<br/>\n","\n","#### Input Prompt: **City-related question**  \n","#### External Knowledge Source: **Wikipedia**   \n","<br/>\n","\n","We follow the steps below:  \n","\n","#### 1. Data Load\n","#### 2. Check Query Engine\n","#### 3. Connect Retriever and Generator"],"metadata":{"id":"KVHLibfMGMy_"}},{"cell_type":"markdown","metadata":{"id":"TMLWar_B2QaX"},"source":["### 1. Data Load\n","\n","Load 100 different cities' wikipedia page data.  \n","\n","We will use LlamaIndex data connector `WikipediaReader` which that receives a tile list of the wikipedia page and returns corresponding wikipedia page by replacing it with a `Document` object.  \n","\n","```Python\n","city_name_path = '/your/path/to/city.txt' #change this path\n","\n","city_names = []\n","\n","with open(city_name_path, 'r', encoding='utf-8') as file:\n","    lines = file.readlines()\n","    for line in lines:\n","        city = line.split(':')[0][:-1]\n","        city_names.append(city)\n","\n","print(city_names) #100 different cities\n","\n","if city_related_to_question not in city_names:\n","  city_names.append(city_related_to_question)\n","```\n","\n","```Python\n","reader = WikipediaReader()\n","documents = reader.load_data(city_names, auto_suggest=False)\n","```\n","\n","Plus, in this section, we will add the city name to the list corresponding to the city-related questions we initially asked LLM."]},{"cell_type":"code","source":["! pwd"],"metadata":{"id":"b_yohfjAsBo2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1739258601939,"user_tz":-540,"elapsed":121,"user":{"displayName":"ê¹€ê°•ì„œ","userId":"03805430674786434409"}},"outputId":"dd053c8b-a363-471d-d33e-042187da140a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content\n"]}]},{"cell_type":"code","source":["### YOUR CODE HERE ###\n","\n","city_name_path = '/your/path/to/city.txt' #change this path\n","\n","city_names = []\n","\n","with open(city_name_path, 'r', encoding='utf-8') as file:\n","    lines = file.readlines()\n","    for line in lines:\n","        city = line.split(':')[0][:-1]\n","        city_names.append(city)\n","\n","print(city_names) #100 different cities\n","\n","if city_related_to_question not in city_names:\n","  city_names.append(city_related_to_question)"],"metadata":{"id":"v8L1AKLjV8cQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### YOUR CODE HERE ###\n","\n","reader = WikipediaReader()\n","documents = reader.load_data(city_names, auto_suggest=False)"],"metadata":{"id":"GxXj8_P0WBn4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<img src=\"https://i.gifer.com/B6Qs.gif\" width=\"150\">\n","\n","This process will take some time."],"metadata":{"id":"8QtTPg767shh"}},{"cell_type":"markdown","source":["If there was no problem with progressing this far, let's insert the Wikipedia page information transformed into the `Document` object.\n","\n","```Python\n","index = VectorStoreIndex.from_documents(documents)\n","```"],"metadata":{"id":"neO_Sa4hu_XV"}},{"cell_type":"code","source":["### YOUR CODE HERE ###\n","\n","index = VectorStoreIndex.from_documents(documents)"],"metadata":{"id":"mV38Qv1vYT60"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 2. Check Query Engine\n","\n","Make query engine from index, and check whether Wikipedia page information has been properly inserted using city-related queries.\n","\n","We will using query related to Berlin which is element of `city_names`, and the answer of the query can be inferred from the information on the Wikipedia page.  \n","\n","<br/>\n","\n","####**Using query: 'What's the arts and culture scene in Berlin?'**\n","\n","<img src = \"https://i.imgur.com/RQkoLL1.png\" width = 800>\n","\n","```Python\n","query_engine = index.as_query_engine()\n","response = query_engine.query(\"What's the arts and culture scene in Berlin?\")\n","print(textwrap.fill(str(response), 100))\n","```"],"metadata":{"id":"v4tSCzwyvaQX"}},{"cell_type":"code","source":["### YOUR CODE HERE ###\n","\n","query_engine = index.as_query_engine()\n","response = query_engine.query(\"What's the arts and culture scene in Berlin?\")\n","print(textwrap.fill(str(response), 100))"],"metadata":{"id":"D7tRW6RTYyfq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Let's see what happens to the response of query engine when get query asking for knowledge that does not exist in the database.\n","\n","#### **Citiese and question-answer pairs that are not in 'city.txt'**\n","\n","**Xinjiang**  \n","*Question:* What is the magnitude of the earthquake that occurred in Xinjiang, China in 2024?  \n","*Answer:* 7.1\n","\n","**Jaipur**  \n","*Question:* Where in Jaipur is the proposed location for the third largest cricket stadium in the world, which can accommodate about 75,000 people?  \n","*Answer:* Chonp Village.\n"],"metadata":{"id":"uHqhQAIDsAir"}},{"cell_type":"markdown","source":["**Evaluate for yourself whether the response is accurate of inaccurate**\n","\n","```Python\n","unknown_question_xinjiang = \"What is the magnitude of the earthquake that occurred in Xinjiang, China in 2024?\"\n","unknown_question_jaipur = \"Where in Jaipur is the proposed location for the third largest cricket stadium in the world, which can accommodate about 75,000 people?\"\n","\n","response_xinjiang = query_engine.query(unknown_question_xinjiang)\n","response_jaipur = query_engine.query(unknown_question_jaipur)\n","\n","print(pretty_print(response_xinjiang.response))\n","print(\"\\n-------------------------\\n\")\n","print(pretty_print(response_jaipur.response))\n","```"],"metadata":{"id":"MdFyQpGg9wDg"}},{"cell_type":"code","source":["### YOUR CODE HERE ###\n","\n","unknown_question_xinjiang = \"What is the magnitude of the earthquake that occurred in Xinjiang, China in 2024?\"\n","unknown_question_jaipur = \"Where in Jaipur is the proposed location for the third largest cricket stadium in the world, which can accommodate about 75,000 people?\"\n","\n","response_xinjiang = query_engine.query(unknown_question_xinjiang)\n","response_jaipur = query_engine.query(unknown_question_jaipur)\n","\n","print(pretty_print(response_xinjiang.response))\n","print(\"\\n-------------------------\\n\")\n","print(pretty_print(response_jaipur.response))"],"metadata":{"id":"771OH85gr7AF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-4dzdgJ52QaY"},"source":["### 3. Connect Retriever and Generator\n","\n","Build a RAG from scratch.\n","\n","Since RAG consists of two steps: retrieve and generation, it is a good choice to generate it as a class with two methods: `retrieve` and `generate_response`.\n","\n","In addition, when we add a method that puts the output from `retrieve` method as the input of the `generate_response` method, the RAG class can be completed."]},{"cell_type":"markdown","source":["First, import libraries\n","\n","```Python\n","from openai import OpenAI\n","from trulens.apps.custom import instrument\n","\n","oai_client = OpenAI()\n","```"],"metadata":{"id":"P0wH3mvUVT7P"}},{"cell_type":"code","source":["### YOUR CODE HERE ###\n","\n","from openai import OpenAI\n","from trulens.apps.custom import instrument\n","\n","oai_client = OpenAI()"],"metadata":{"id":"03jKMs7bojP-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Next, make a RAG from scratch. In this section, we use `instrument` decorator for later evaluation.\n","\n","```Python\n","class RAG_from_scratch:\n","    @instrument\n","    def retrieve(self, query: str) -> list:\n","        \"\"\"\n","        Retrieve relevant text from vector store.\n","        \"\"\"\n","        results = query_engine.query(query)\n","        return results\n","\n","    @instrument\n","    def generate_response(self, query: str, context_str: list) -> str:\n","        \"\"\"\n","        Generate answer from context.\n","        \"\"\"\n","        completion = oai_client.chat.completions.create(\n","        model=\"gpt-3.5-turbo\",\n","        temperature=0,\n","        messages=\n","        [\n","            {\"role\": \"user\",\n","            \"content\":\n","            f\"We have provided context information below. \\n\"\n","            f\"---------------------\\n\"\n","            f\"{context_str}\"\n","            f\"\\n---------------------\\n\"\n","            f\"Given this information, please answer the question: {query}\"\n","            }\n","        ]\n","        ).choices[0].message.content\n","        return completion\n","\n","    @instrument\n","    def query(self, query: str) -> str:\n","        context_str = self.retrieve(query)\n","        completion = self.generate_response(query, context_str)\n","        return completion\n","\n","rag = RAG_from_scratch()\n","```"],"metadata":{"id":"AiU3AgZcVlVr"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"w--nAmff2QaY"},"outputs":[],"source":["### YOUR CODE HERE ###\n","\n","class RAG_from_scratch:\n","    @instrument\n","    def retrieve(self, query: str) -> list:\n","        \"\"\"\n","        Retrieve relevant text from vector store.\n","        \"\"\"\n","        results = query_engine.query(query)\n","        return results\n","\n","    @instrument\n","    def generate_response(self, query: str, context_str: list) -> str:\n","        \"\"\"\n","        Generate answer from context.\n","        \"\"\"\n","        completion = oai_client.chat.completions.create(\n","        model=\"gpt-3.5-turbo\",\n","        temperature=0,\n","        messages=\n","        [\n","            {\"role\": \"user\",\n","            \"content\":\n","            f\"We have provided context information below. \\n\"\n","            f\"---------------------\\n\"\n","            f\"{context_str}\"\n","            f\"\\n---------------------\\n\"\n","            f\"Given this information, please answer the question: {query}\"\n","            }\n","        ]\n","        ).choices[0].message.content\n","        return completion\n","\n","    @instrument\n","    def query(self, query: str) -> str:\n","        context_str = self.retrieve(query)\n","        completion = self.generate_response(query, context_str)\n","        return completion\n","\n","rag = RAG_from_scratch()"]},{"cell_type":"markdown","source":["Now, it's time to remember the question we asked to LLM before making RAG.  \n","The default question was this.  \n","\n","<br/>\n","\n","Question: **Which Korean city has a sisterhood relationship with Belize City?**  \n","Answer: **Yeosu**\n","\n","<br/>\n","\n","Let's use the RAG class we created to get this question to LLM. What will happen?  \n","Now that we've made the RAG class and the method in it, we can also see the results of query engine.\n","\n","```Python\n","city_question = 'Which Korean city has a sisterhood relationship with Belize City?'\n","\n","answer = rag.query(city_question)\n","\n","print(pretty_print(answer))\n","```\n"],"metadata":{"id":"MB3YqVNmshvy"}},{"cell_type":"code","source":["### YOUR CODE HERE ###\n","\n","city_question = 'Which Korean city has a sisterhood relationship with Belize City?'\n","\n","answer = rag.query(city_question)\n","\n","print(pretty_print(answer))"],"metadata":{"id":"xpIK23rFcKn6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Moreover, the query engine we use is directly generated from the index, and since retriever made of the `index.as _retriever()` method used in this query engine, we can also check the retrieved passages.\n","\n","```Python\n","retriever = index.as_retriever()\n","\n","ret = retriever.retrieve(city_question)\n","for i in range(len(ret)):\n","  print(f\"Retrieved Paasge {i+1}\\n\", ret[i].text, '\\n\\n\\n')\n","```"],"metadata":{"id":"vcZIza6lIOOg"}},{"cell_type":"code","source":["### YOUR CODE HERE ###\n","\n","retriever = index.as_retriever()\n","\n","ret = retriever.retrieve(city_question)\n","for i in range(len(ret)):\n","  print(f\"Retrieved Paasge {i+1}\\n\", ret[i].text, '\\n\\n\\n')"],"metadata":{"id":"HRpyMDKlG_qF","collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### II. Finding the best RAG App Configuration.\n","\n","Let's change the configuration of the RAG and observe how its performance varies.\n","\n","The performance of RAG is measured by `Groundedness`, `Answer Relevance`, and `Context Relevance`. However, `latency` and `total cost` are also important factors. Therefore, we shounld choose the evaluation metrics carefully and evaluate RAG's performance from various perspectives.\n","\n","<br/>  \n","\n","In this section, we'll modify the following factors to examine changes in RAG's performance:\n","\n","#### 1. Chunk Size of Retriever: As the chunk size varies, retriever may or may not retrieve sufficient information. Letâ€™s adjust the chunk size to see how the results change.\n","#### 2. Query Engine: Refine the prompt used in the Query Engine's answer generation to enhance the quality of its summaries.\n","#### 3. Prompt Design for RAG: Add clear instructions to the task so that the LLM generates the answer that the user is looking for.\n","\n"],"metadata":{"id":"f-E3pWUz-dbV"}},{"cell_type":"markdown","source":["First, letâ€™s go through the process of varying the chunk size of the retriever.\n","\n","When performing a search, the retriever finds and returns the most relevant information from the existing `nodes`. These nodes are determined by the `chunk size` and `chunk overlap` settings of the `SentenceSplitter`, which can be configured when creating the `index`.\n","\n","Therefore, for questions where the information needed for the answer spans multiple chunks, the retriever may or may not return sufficient information depending on the chunk size.\n","\n","This time, we will use the following two settings:\n","\n","\n","**<h4>Retriever_long - chunk size: 1024, chunk overlap: 200</h4>**\n","\n","**<h4>Retriever_short - chunk size: 200, chunk overlap: 50</h4>**\n","\n","```Python\n","text_splitter_short = SentenceSplitter(chunk_size=200, chunk_overlap=50)\n","\n","index_short = VectorStoreIndex.from_documents(documents=documents, transformations=[text_splitter_short])\n","\n","text_splitter_long = SentenceSplitter(chunk_size=1024, chunk_overlap=200)\n","\n","index_long = VectorStoreIndex.from_documents(documents=documents, transformations=[text_splitter_long])\n","```"],"metadata":{"id":"ciwJ7HIik91G"}},{"cell_type":"code","source":["### YOUR CODE HERE ###\n","\n","text_splitter_short = SentenceSplitter(chunk_size=200, chunk_overlap=50)\n","\n","index_short = VectorStoreIndex.from_documents(documents=documents, transformations=[text_splitter_short])\n","\n","text_splitter_long = SentenceSplitter(chunk_size=1024, chunk_overlap=200)\n","\n","index_long = VectorStoreIndex.from_documents(documents=documents, transformations=[text_splitter_long])"],"metadata":{"id":"uWQRtg6ukr6T"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Additionally, to clearly compare the retrieved results, each retriever will be set to return only one node\n","\n","```Python\n","retriever_short = index_short.as_retriever(similarity_top_k=1)\n","retriever_long = index_long.as_retriever(similarity_top_k=1)\n","```"],"metadata":{"id":"gWfz5P2hnC1R"}},{"cell_type":"code","source":["### YOUR CODE HERE ###\n","\n","retriever_short = index_short.as_retriever(similarity_top_k=1)\n","retriever_long = index_long.as_retriever(similarity_top_k=1)"],"metadata":{"id":"orxfsZZmnQXF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now, letâ€™s check the impact of `chunk size` by using the following question. This question is about Antananarivo, the capital of Madagascar.\n","\n","```Python\n","question = \"In Antananarivo, what were the foundational materials of traditional architecture, and what efforts have been made to protect and restore the cityâ€™s architectural and cultural heritage?\"\n","```\n","\n","To answer this question correctly, we need information on what the foundational materials of traditional architecture in Antananarivo are, as well as the efforts made to protect and restore this architectural heritage.\n","\n","Letâ€™s see how the retriever responds to this question with the code below.\n","\n","```Python\n","ret_passages_short = retriever_short.retrieve(question)\n","for i in range(len(ret_passages_short)):\n","  print(pretty_print(ret_passages_short[i].text))\n","  print(\"\\n\\n\\n\")\n","```\n","```Python\n","ret_passages_long = retriever_long.retrieve(question)\n","for i in range(len(ret_passages_long)):\n","  print(pretty_print(ret_passages_long[i].text))\n","  print(\"\\n\\n\\n\")\n","```"],"metadata":{"id":"-CcrDLlXnXiK"}},{"cell_type":"code","source":["### YOUR CODE HERE ###\n","\n","question = \"In Antananarivo, what were the foundational materials of traditional architecture, and what efforts have been made to protect and restore the cityâ€™s architectural and cultural heritage?\""],"metadata":{"id":"wG62eCodoZPU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### YOUR CODE HERE ###\n","\n","ret_passages_short = retriever_short.retrieve(question)\n","for i in range(len(ret_passages_short)):\n","  print(pretty_print(ret_passages_short[i].text))\n","  print(\"\\n\\n\\n\")"],"metadata":{"id":"65DK03wsogak"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### YOUR CODE HERE ###\n","\n","ret_passages_long = retriever_long.retrieve(question)\n","for i in range(len(ret_passages_long)):\n","  print(pretty_print(ret_passages_long[i].text))\n","  print(\"\\n\\n\\n\")"],"metadata":{"id":"muHOYcfxo4ID"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Comparing the retrieved passages, you can see that when the `chunk size` is large, sufficient information is included, whereas with a small chunk size, the context is cut off.\n","\n","By adjusting the `chunk size` in this way, the accuracy of the query engine can change. However, increasing the chunk size may also include unnecessary information, and detailed information might be overlooked by the generator.\n","\n","Therefore, the optimal `chunk size` varies depending on the case, and this is an important configuration that can affect performance in RAG.\n","\n","Choose the retriever and corresponding index you want to use for this practice. Later, you can modify this part and redeclare the query engine to change the type of retriever. These results will also impact the creation of the Query engine in the future\n","\n","```Python\n","# retriever = index_short.as_retriever(similarity_top_k=2)\n","# retriever = index_long.as_retriever(similarity_top_k=2)\n","\n","# index = index_short #If you want to use this, remove #\n","# index = index_long #If you want to use this, remove #\n","```"],"metadata":{"id":"iCVfu9GRrChe"}},{"cell_type":"code","source":["### YOUR CODE HERE ###\n","\n","retriever = index_short.as_retriever(similarity_top_k=2)\n","# retriever = index_long.as_retriever(similarity_top_k=2)\n","\n","index = index_short #If you want to use this, remove #\n","# index = index_long #If you want to use this, remove #"],"metadata":{"id":"9HWIoq2fsp4v"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<h4>Secondly, we will redefine the query engine and response synthesizer.  \n","There are two options available:  </h4>\n","\n","First, you can use the query engine to **generate a summary** instead of an answer. The downside of this approach is that important information may be lost during the summarization process.\n","\n","The second option is to **modify the instructions of the current query engine** you have been using. Currently, the query engine is set to generate an answer directly, but you can change the prompt used for this as you see fit. However, if an error occurs in the query engine, it may propagate to the generator, causing error propagation.\n","\n","As seen in previous experiments, crafting a good prompt significantly impacts the performance of an LLM. At this point, you will decide which version of the query engine to use and what prompt to employ.  \n","\n","<br/>\n","\n","**Prompt**\n","\n","```Pytyhon\n","from llama_index.core.query_engine import CustomQueryEngine\n","from llama_index.core.retrievers import BaseRetriever\n","from llama_index.core import get_response_synthesizer\n","from llama_index.core.response_synthesizers import BaseSynthesizer\n","from llama_index.llms.openai import OpenAI\n","from llama_index.core import PromptTemplate\n","\n","simple_qa_prompt = PromptTemplate(\n","    \"Context information is below.\\n\"\n","    \"---------------------\\n\"\n","    \"{context_str}\\n\"\n","    \"---------------------\\n\"\n","    \"Given the context information and not prior knowledge, \"\n","    \"answer the query.\\n\"\n","    \"Query: {query_str}\\n\"\n","    \"Answer: \"\n",")\n","\n","short_sum_prompt = PromptTemplate(\n","\"\"\"Write a summary of the following. Try to use only the information provided.\n","Try to include as many key details as possible.\n","---------------------\\n\n","{context_str}\n","---------------------\\n\n","SUMMARY:\"\"\"\n",")\n","```\n","**Custom query engine class**\n","```Pytyhon\n","class OurCustomQueryEngine(CustomQueryEngine):\n","\n","    retriever: BaseRetriever\n","    response_synthesizer: BaseSynthesizer\n","    llm: OpenAI\n","    qa_prompt: PromptTemplate = simple_qa_prompt\n","\n","    def custom_query(self, query_str: str):\n","        nodes = self.retriever.retrieve(query_str)\n","\n","        context_str = \"\\n\\n\".join([n.node.get_content() for n in nodes])\n","        response = self.llm.complete(\n","            self.qa_prompt.format(context_str=context_str, query_str=query_str)\n","        )\n","\n","        return str(response)\n","\n","llm = OpenAI(model=\"gpt-3.5-turbo\")\n","```"],"metadata":{"id":"B1_5SdqVEkVA"}},{"cell_type":"code","source":["### YOUR CODE HERE ###\n","\n","from llama_index.core.query_engine import CustomQueryEngine\n","from llama_index.core.retrievers import BaseRetriever\n","from llama_index.core import get_response_synthesizer\n","from llama_index.core.response_synthesizers import BaseSynthesizer\n","from llama_index.llms.openai import OpenAI\n","from llama_index.core import PromptTemplate\n","\n","simple_qa_prompt = PromptTemplate(\n","    \"Context information is below.\\n\"\n","    \"---------------------\\n\"\n","    \"{context_str}\\n\"\n","    \"---------------------\\n\"\n","    \"Given the context information and not prior knowledge, \"\n","    \"answer the query.\\n\"\n","    \"Query: {query_str}\\n\"\n","    \"Answer: \"\n",")\n","\n","short_sum_prompt = PromptTemplate(\n","\"\"\"Write a summary of the following. Try to use only the information provided.\n","Try to include as many key details as possible.\n","---------------------\\n\n","{context_str}\n","---------------------\\n\n","SUMMARY:\"\"\"\n",")"],"metadata":{"id":"jYO-ZEgSwVmt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### YOUR CODE HERE ###\n","\n","class OurCustomQueryEngine(CustomQueryEngine):\n","\n","    retriever: BaseRetriever\n","    response_synthesizer: BaseSynthesizer\n","    llm: OpenAI\n","    qa_prompt: PromptTemplate = simple_qa_prompt\n","\n","    def custom_query(self, query_str: str):\n","        nodes = self.retriever.retrieve(query_str)\n","\n","        context_str = \"\\n\\n\".join([n.node.get_content() for n in nodes])\n","        response = self.llm.complete(\n","            self.qa_prompt.format(context_str=context_str, query_str=query_str)\n","        )\n","\n","        return str(response)\n","\n","llm = OpenAI(model=\"gpt-3.5-turbo\")"],"metadata":{"id":"RmIANiouxYjb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["You can directly see how the two versions of the query engine generate results by checking their outputs with the code below.\n","\n","```Python\n","retriever = index.as_retriever()\n","synthesizer = get_response_synthesizer(response_mode=\"compact\")\n","\n","query_engine_answer = OurCustomQueryEngine(\n","    retriever=retriever,\n","    response_synthesizer=synthesizer,\n","    llm=llm,\n","    qa_prompt=simple_qa_prompt,\n",")\n","\n","res_answer = query_engine_answer.query(\"What's the arts and culture scene in Berlin?\")\n","```\n","```Python\n","query_engine_sum = OurCustomQueryEngine(\n","    retriever=retriever,\n","    response_synthesizer=synthesizer,\n","    llm=llm,\n","    qa_prompt=short_sum_prompt,\n",")\n","\n","res_summary = query_engine_sum.query(\"What's the arts and culture scene in Berlin?\")\n","```\n","```Python\n","print(res_summary)\n","print(res_answer)\n","```"],"metadata":{"id":"5wYEPo7aTgsS"}},{"cell_type":"code","source":["### YOUR CODE HERE ###\n","\n","retriever = index.as_retriever()\n","synthesizer = get_response_synthesizer(response_mode=\"compact\")\n","\n","query_engine_answer = OurCustomQueryEngine(\n","    retriever=retriever,\n","    response_synthesizer=synthesizer,\n","    llm=llm,\n","    qa_prompt=simple_qa_prompt,\n",")\n","\n","res_answer = query_engine_answer.query(\"What's the arts and culture scene in Berlin?\")"],"metadata":{"id":"0b7cqNlvw--t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### YOUR CODE HERE ###\n","\n","query_engine_sum = OurCustomQueryEngine(\n","    retriever=retriever,\n","    response_synthesizer=synthesizer,\n","    llm=llm,\n","    qa_prompt=short_sum_prompt,\n",")\n","\n","res_summary = query_engine_sum.query(\"What's the arts and culture scene in Berlin?\")"],"metadata":{"id":"lHuTCcmcS1nt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### YOUR CODE HERE ###\n","\n","print(res_summary)\n","print(res_answer)"],"metadata":{"id":"e6emscsQTJSY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now, before redifining RAG, you need to first decide which query engine to use\n","\n","```Python\n","#query_engine = query_engine_sum #If you want to use this, remove #\n","#query_engine = query_engine_answer #If you want to use this, remove #\n","```"],"metadata":{"id":"UkGb2Pv-uZRQ"}},{"cell_type":"code","source":["### YOUR CODE HERE ###\n","\n","query_engine = query_engine_sum #If you want to use this, remove #\n","# query_engine = query_engine_answer #If you want to use this, remove #"],"metadata":{"id":"wuZhF3N5Vqk9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now, let's create a new RAG using the query engine and retriever we've built above.  \n","\n","In this process, we'll provide clearer instructions in the input prompt that goes to the RAG's generator.\n","\n","\n","```Python\n","class Refine_RAG:\n","    @instrument\n","    def retrieve(self, query: str) -> list:\n","        ret = retriever.retrieve(query)\n","        results = query_engine.query(query)\n","        return ret, results\n","\n","    @instrument\n","    def generate_response(self, query: str, context_str: list) -> str:\n","        \"\"\"\n","        Generate answer from context.\n","        \"\"\"\n","        messages = [\n","            {\n","                \"role\": \"system\",\n","                \"content\": f\"You are a helpful assistant. Answer as concisely as possible.\",\n","            },\n","            {\n","                \"role\": \"user\",\n","                \"content\":\n","                    f\"\"\"\n","                    ###Instruction\n","Please answer the following question based on the provided context. Your answer should be short and concise.\n","Basically, you have to answer the question based on the provided context. But you can use your parametrized knowledge when the provided context was wrong or unrelated to the quesion.\n","When you generate the answer, you should explain the reason why you deduced such a answer from the context.\n","\n","### Question\n","{query}\n","\n","### Provided Context\n","{context_str}\n","\n","### Answer\n","\n","\n","### Reason\n","                    \"\"\"\n","            }\n","        ]\n","\n","        response = oai_client.chat.completions.create(\n","            model=\"gpt-3.5-turbo\",\n","            temperature=0,\n","            messages=messages,\n","        )\n","\n","        return response.choices[0].message.content\n","\n","    @instrument\n","    def query(self, query: str) -> str:\n","        ret, context_str = self.retrieve(query)\n","        # for i in range(len(ret)):\n","        #   print(\"Retrieved Context: \\n\", ret[i].text) # use only when you want to see intermeidate result\n","        # print(\"\\n\\nIntermediate Summary: \\n\",  context_str) # use only when you want to see intermeidate result\n","        completion = self.generate_response(query, context_str)\n","        return completion\n","\n","refine_rag = Refine_RAG()\n","```"],"metadata":{"id":"hBH2d-AIF9n4"}},{"cell_type":"code","source":["### YOUR CODE HERE ###\n","\n","class Refine_RAG:\n","    @instrument\n","    def retrieve(self, query: str) -> list:\n","        ret = retriever.retrieve(query)\n","        results = query_engine.query(query)\n","        return ret, results\n","\n","    @instrument\n","    def generate_response(self, query: str, context_str: list) -> str:\n","        \"\"\"\n","        Generate answer from context.\n","        \"\"\"\n","        messages = [\n","            {\n","                \"role\": \"system\",\n","                \"content\": f\"You are a helpful assistant. Answer as concisely as possible.\",\n","            },\n","            {\n","                \"role\": \"user\",\n","                \"content\":\n","                    f\"\"\"\n","                    ###Instruction\n","Please answer the following question based on the provided context. Your answer should be short and concise.\n","Basically, you have to answer the question based on the provided context. But you can use your parametrized knowledge when the provided context was wrong or unrelated to the quesion.\n","When you generate the answer, you should explain the reason why you deduced such a answer from the context.\n","\n","### Question\n","{query}\n","\n","### Provided Context\n","{context_str}\n","\n","### Answer\n","\n","\n","### Reason\n","                    \"\"\"\n","            }\n","        ]\n","\n","        response = oai_client.chat.completions.create(\n","            model=\"gpt-3.5-turbo\",\n","            temperature=0,\n","            messages=messages,\n","        )\n","\n","        return response.choices[0].message.content\n","\n","    @instrument\n","    def query(self, query: str) -> str:\n","        ret, context_str = self.retrieve(query)\n","        # for i in range(len(ret)):\n","        #   print(\"Retrieved Context: \\n\", ret[i].text) # use only when you want to see intermeidate result\n","        # print(\"\\n\\nIntermediate Summary: \\n\",  context_str) # use only when you want to see intermeidate result\n","        completion = self.generate_response(query, context_str)\n","        return completion\n","\n","refine_rag = Refine_RAG()"],"metadata":{"id":"7sqhvd43d9Z_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Within the RAG class, you can inspect all values and content that are created and passed along.\n","\n","```Python\n","sample_question = \"City council of Suwon addressed illegal dumping of household waste in what way?\"\n","\n","answer = refine_rag.query(sample_question)\n","\n","print(f\"\\n\\n{answer}\")\n","```\n"],"metadata":{"id":"ohY7vqKgG5xf"}},{"cell_type":"code","source":["### YOUR CODE HERE ###\n","\n","sample_question = \"City council of Suwon addressed illegal dumping of household waste in what way?\"\n","\n","answer = refine_rag.query(sample_question)\n","\n","print(f\"\\n\\n{answer}\")"],"metadata":{"id":"SZERL4Ygsv78"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["I hope there are some improvement of RAG performance, even a little :)  \n","\n","Of course, we can enhance the performance of the RAG not only through the three elements mentioned above but also in many other ways.\n","\n","For instance, we can expect changes in performance by altering the following factors.\n","\n","**Temperature**: This parameter controls the randomness of the output from a language model. A higher temperature increases the randomness, resulting in more varied and unpredictable text. Conversely, a lower temperature makes the model's responses more deterministic and predictable, often sticking closer to the most likely outcomes.  \n","\n","**Top-p**: This parameter helps in controlling the diversity of the model's responses by focusing on the most probable tokens. Top-p sets a threshold to consider the top tokens up to the point where their cumulative probability exceeds a specified value p. This way, the model filters out less likely tokens and focuses only on a subset of the most probable tokens, which helps in generating coherent and contextually relevant outputs.\n","\n","```Python\n","        response = oai_client.chat.completions.create(\n","            model=\"gpt-3.5-turbo\",\n","            temperature=0, #change this value\n","            top_p=0, #change this value\n","            messages=messages,\n","        )\n","```\n","\n","**Different Embedding Function**: Using more advanced embedding models could capture the meanings of tokens more effectively, potentially allowing for more accurate retrieval.\n","\n","Increasing the dimensionality of embedding vectors can enable them to capture more information, thus improving their ability to find semantically similar passages. However, this might also lead to overfitting and make it difficult to measure similarities due to diluted data distribution across dimensions, i.e., curse of dimensionality.\n","\n","Conversely, reducing the dimensions of vectors can increase computational efficiency and prevent overfitting. Yet, overly small dimensions may not capture enough information, leading to degraded performance.\n","\n","```Python\n","from llama_index.core import Settings\n","\n","# Option 1. global default\n","Settings.embed_model = OpenAIEmbedding()\n","\n","# Option 2. local model\n","Settings.embed_model = HuggingFaceEmbedding(\n","    model_name=\"BAAI/bge-small-en-v1.5\" #change this to the local path or huggingface model name\n",")\n","\n","#per-index\n","index = VectorStoreIndex.from_documents(documents, embed_model=embed_model)\n","```\n","\n","Therefore, by using one of the above codes, you can adjust the embedding function and the dimension of the embedding vector to improve the performance of the RAG\n"],"metadata":{"id":"W2hH8ZlSIwD6"}}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.18"},"colab":{"provenance":[{"file_id":"https://github.com/truera/trulens/blob/main/trulens_eval/examples/quickstart/quickstart.ipynb","timestamp":1711609044141}]}},"nbformat":4,"nbformat_minor":0}