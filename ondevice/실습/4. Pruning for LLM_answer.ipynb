{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71XdNJRThwC3"
      },
      "source": [
        "# Assignment 3. Pruning for LLM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oivv4vbZh4HX"
      },
      "source": [
        "## Goals\n",
        "\n",
        "본 실습에서는 대규모 언어 모델(Large Language Model, LLM)의 크기를 효과적으로 줄이는 Pruning 기법을 학습합니다. 특히 Magnitude-based pruning과 최근 주목받는 Wanda 기법을 활용하여, 파라미터의 수를 효율적으로 감소시키면서 모델의 성능을 유지하는 방법을 실습합니다.\n",
        "\n",
        "\n",
        "## Contents\n",
        "\n",
        "1. **Magnitude-based Pruning 실습**:\n",
        "   - 간단한 magnitude 기반 pruning 방법을 통해 모델 크기를 감소시키고 성능 변화를 확인합니다.\n",
        "2. **Wanda를 이용한 Pruning 실습**:\n",
        "   - Activation의 중요도를 측정하여 더 정교하게 pruning을 수행하는 Wanda 방법을 구현하고 모델의 성능을 비교합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7htCbigjGPS"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IKOhoD3TjIdP"
      },
      "source": [
        "실습에 필요한 패키지를 설치합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ml1mzMEiejpI"
      },
      "outputs": [],
      "source": [
        "print('Installing packages...')\n",
        "!pip install torch transformers==4.31.0 accelerate==0.21.0 sentencepiece==0.1.99 tokenizers==0.13.3 datasets==2.15.0 tqdm zstandard"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "필요한 모듈을 불러옵니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "el7nPjZPejpK"
      },
      "outputs": [],
      "source": [
        "import tqdm\n",
        "import torch\n",
        "from torch import nn\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from datasets import load_dataset\n",
        "from functools import partial\n",
        "import gc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDCyN8n7jNwk"
      },
      "source": [
        "### 모델 평가\n",
        "\n",
        "Wikitext-2 데이터셋을 사용하여 모델의 성능을 평가하는 지표인 perplexity를 계산합니다.\n",
        "\n",
        "**Perplexity란?**\n",
        "- Perplexity는 언어 모델이 주어진 텍스트를 얼마나 잘 예측하는지를 수치로 나타낸 지표입니다.\n",
        "- 수학적으로는 모델이 예측한 확률분포의 \"불확실성\"을 측정하는 값이며, 값이 **낮을수록 모델의 성능이 좋다**고 해석합니다.\n",
        "- 단어 $\\{w_1, w_2, w_3, ..., w_N\\}$으로 구성된 문장의 Perplexity는 다음과 같은 수식으로 나타낼 수 있습니다.\n",
        "    - $Perplexity = \\sqrt[N]{\\frac{1}{\\prod_{i=1}^{N} P(w_i | w_1, w_2, ..., w_{i-1})}}$\n",
        "    - 여기서 $P(w_i | w_1, w_2, ..., w_{i-1})$은 $i$번째에 $w_i$ 단어를 생성할 확률을 의미합니다.\n",
        "    - 즉, Perplexity는 문자의 발생 확률에 대한 역수를 의미하게 됩니다.\n",
        "- 직관적으로 Perplexity가 10이라면 모델이 다음 단어 후보를 10개 정도로 생각하고 있다고 볼 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "w3yRuJ9HejpL"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, tokenizer):\n",
        "    \"\"\"\n",
        "    모델의 perplexity를 계산하는 함수입니다.\n",
        "    \n",
        "    Args:\n",
        "        model: 평가할 모델\n",
        "        tokenizer: 토크나이저\n",
        "        \n",
        "    Returns:\n",
        "        float: 계산된 perplexity 값\n",
        "    \"\"\"\n",
        "    # 테스트 데이터셋 로드 및 전처리\n",
        "    testenc = load_dataset('wikitext', 'wikitext-2-raw-v1', split='test')\n",
        "    testenc = tokenizer(\"\\n\\n\".join(testenc['text']), return_tensors='pt')\n",
        "    \n",
        "    # 입력 데이터를 모델 디바이스로 이동\n",
        "    testenc = testenc.input_ids.to(model.device)\n",
        "    nsamples = 40\n",
        "    model = model.eval()  # 평가 모드로 설정\n",
        "\n",
        "    # Negative log likelihood 계산\n",
        "    nlls = []\n",
        "    for i in tqdm.tqdm(range(nsamples), desc=\"evaluating...\"):\n",
        "        # 배치 데이터 준비\n",
        "        batch = testenc[:, (i * 2048):((i + 1) * 2048)].to(model.device)\n",
        "        \n",
        "        # 모델 추론\n",
        "        with torch.no_grad():\n",
        "            lm_logits = model(batch).logits\n",
        "            \n",
        "        # 로짓과 레이블 시프트\n",
        "        shift_logits = lm_logits[:, :-1, :].contiguous().float()\n",
        "        shift_labels = testenc[:, (i * 2048):((i + 1) * 2048)][:, 1:]\n",
        "        \n",
        "        # 손실 계산\n",
        "        loss_fct = nn.CrossEntropyLoss()\n",
        "        loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
        "        neg_log_likelihood = loss.float() * 2048\n",
        "        nlls.append(neg_log_likelihood)\n",
        "\n",
        "    # Perplexity 계산 및 반환\n",
        "    return torch.exp(torch.stack(nlls).sum() / (nsamples * 2048))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EAVp3bh5jmft"
      },
      "source": [
        "### OPT 1.3B 모델 로딩\n",
        "\n",
        "OPT-1.3B 모델을 로딩하고 평가합니다.\n",
        "여기서 tokenizer는 텍스트(문장)를 모델이 이해할 수 있는 작은 단위(token)로 나누는 역할을 수행합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UjP_N0UiejpM"
      },
      "outputs": [],
      "source": [
        "model_path = \"facebook/opt-1.3b\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\")\n",
        "\n",
        "# Evaluate the model\n",
        "model_perplexity = evaluate(model, tokenizer)\n",
        "print(f\"\\nmodel perplexity: {model_perplexity:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8wwbgHjejpN"
      },
      "source": [
        "## [실습 1] Magnitude-based Pruning 구현\n",
        "\n",
        "Magnitude-based pruning 함수를 구현하세요. 구현은 CNN에서 magnitude-based pruning을 구현하는 방식과 유사합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "UOV59TCmejpO"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def prune_magnitude_opt(model, sparsity):\n",
        "    for n, m in model.named_modules():\n",
        "        if isinstance(m, nn.Linear) and \"lm_head\" not in n:\n",
        "            W = m.weight.data\n",
        "            ##################### YOUR CODE STARTS HERE #####################\n",
        "            num_elements = W.numel()\n",
        "            num_zeros = round(num_elements * sparsity)\n",
        "            importance = torch.abs(W)\n",
        "            threshold = torch.kthvalue(importance.flatten(), num_zeros)[0]\n",
        "            mask = importance > threshold\n",
        "            ##################### YOUR CODE ENDS HERE #######################\n",
        "            W.mul_(mask)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Pruning된 모델을 평가합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hG2Sysb5ejpP"
      },
      "outputs": [],
      "source": [
        "del model\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "model = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\")\n",
        "prune_magnitude_opt(model, 0.5)\n",
        "\n",
        "# Evaluate the model\n",
        "model_perplexity = evaluate(model, tokenizer)\n",
        "print(f\"\\nmodel perplexity (magnitude 50%): {model_perplexity:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKwCkcKoyJuP"
      },
      "source": [
        "## [실습 2] Calibration 데이터셋 준비\n",
        " \n",
        "[Wanda](https://arxiv.org/pdf/2306.11695)를 사용하여 importance를 계산하기 위해서는 calibration 데이터셋을 통해 activation을 추출해야 합니다.\n",
        "\n",
        "아래 빈칸을 작성하여 `activation_norm`을 계산하세요.\n",
        "Wanda 기법에서는 `activation_norm`을 L2 norm으로 계산하지만, 구현에서는 calibration 데이터셋에 대해 여러 번 반복하여 누적하므로 \n",
        "`activation_norm` 계산 시 제곱을 적용한 형태로 누적합니다. (이후 값을 사용할 때는 제곱근을 적용해야 합니다.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "juFujaD5ejpP"
      },
      "outputs": [],
      "source": [
        "def get_calib_dataset(tokenizer=None, n_samples=256, block_size=512):\n",
        "    dataset = load_dataset(\"mit-han-lab/pile-val-backup\", split=\"validation\")\n",
        "    dataset = dataset.shuffle(seed=42)\n",
        "    samples = []\n",
        "    n_run = 0\n",
        "    for data in dataset:\n",
        "        line = data[\"text\"]\n",
        "        line = line.strip()\n",
        "        line_encoded = tokenizer.encode(line)\n",
        "        if len(line_encoded) > block_size:\n",
        "            continue\n",
        "        sample = torch.tensor([line_encoded])\n",
        "        if sample.numel() == 0:\n",
        "            continue\n",
        "        samples.append(sample)\n",
        "        n_run += 1\n",
        "        if n_run == n_samples:\n",
        "            break\n",
        "\n",
        "    # now concatenate all samples and split according to block size\n",
        "    cat_samples = torch.cat(samples, dim=1)\n",
        "    n_split = cat_samples.shape[1] // block_size\n",
        "    print(f\" * Split into {n_split} blocks\")\n",
        "    return [cat_samples[:, i*block_size:(i+1)*block_size] for i in range(n_split)]\n",
        "\n",
        "@torch.no_grad()\n",
        "def get_calib_feat(model, tokenizer):\n",
        "    input_dict = dict()\n",
        "    nsamples_dict = dict()\n",
        "    def add_batch(m, x, y, name):\n",
        "        if name not in input_dict:\n",
        "            input_dict[name] = torch.zeros((m.weight.data.shape[1]), device=m.weight.data.device)\n",
        "            nsamples_dict[name] = 0\n",
        "\n",
        "        if isinstance(x, tuple):\n",
        "            x = x[0]\n",
        "\n",
        "        if len(x.shape) == 2:\n",
        "            x = x.unsqueeze(0)\n",
        "        tmp = x.shape[0]\n",
        "        if len(x.shape) == 3:\n",
        "            x = x.reshape((-1, x.shape[-1]))\n",
        "        x = x.t()\n",
        "\n",
        "        input_dict[name] *= nsamples_dict[name] / (nsamples_dict[name] + tmp)\n",
        "        nsamples_dict[name] += tmp\n",
        "\n",
        "        x = x.type(torch.float32)\n",
        "        ##################### YOUR CODE STARTS HERE #####################\n",
        "        # activation_norm을 계산하세요.\n",
        "        # x.shape => (hidden_size, batch_size)\n",
        "        activation_norm = torch.norm(x, p=2, dim=1) ** 2\n",
        "        # activation_norm.shape => (hidden_size)\n",
        "        ##################### YOUR CODE ENDS HERE #######################\n",
        "        input_dict[name] += activation_norm / nsamples_dict[name]\n",
        "\n",
        "    hooks = []\n",
        "    for name, m in model.named_modules():\n",
        "        if isinstance(m, nn.Linear) and \"lm_head\" not in name:\n",
        "            hooks.append(\n",
        "                m.register_forward_hook(\n",
        "                    partial(add_batch, name=name)))\n",
        "\n",
        "    print(\"Collecting norm of input activations...\")\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    samples = get_calib_dataset(tokenizer)\n",
        "    pbar = tqdm.tqdm(samples)\n",
        "    for input_ids in pbar:\n",
        "        input_ids = input_ids.to(device)\n",
        "        model(input_ids)\n",
        "\n",
        "    for hook in hooks:\n",
        "        hook.remove()\n",
        "    return input_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xNGhY9o6ejpQ"
      },
      "outputs": [],
      "source": [
        "del model\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "model = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\")\n",
        "input_feat = get_calib_feat(model, tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## [실습 3] Wanda Pruning 구현\n",
        " \n",
        "Wanda 논문을 참고하여 아래 빈 칸을 채워 Wanda Pruning을 구현하고 실행해보세요.\n",
        "\n",
        "단, 원래 Wanda에서는 weight parameter의 행(row)별로 동일한 희소성(sparsity)으로 pruning하지만, \n",
        "본 실습에서는 이를 고려하지 않고 전체 weight에 대해 동일한 희소성을 적용하겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "zFw9VSo-ejpR"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def prune_wanda_opt(model, sparsity, input_feat):\n",
        "    for n, m in model.named_modules():\n",
        "        if isinstance(m, nn.Linear) and \"lm_head\" not in n:\n",
        "            W = m.weight.data\n",
        "            ##################### YOUR CODE STARTS HERE #####################\n",
        "            num_elements = W.numel()\n",
        "            num_zeros = round(num_elements * sparsity)\n",
        "            importance = torch.abs(W) * torch.sqrt(input_feat[n])\n",
        "            threshold = torch.kthvalue(importance.flatten(), num_zeros)[0]\n",
        "            mask = importance > threshold\n",
        "            ##################### YOUR CODE ENDS HERE #######################\n",
        "            W.mul_(mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jZZGZZ4rejpR"
      },
      "outputs": [],
      "source": [
        "del model\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "model = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\")\n",
        "prune_wanda_opt(model, sparsity=0.5, input_feat=input_feat)\n",
        "\n",
        "# Evaluate the model\n",
        "model_perplexity = evaluate(model, tokenizer)\n",
        "print(f\"\\nmodel perplexity (wanda 50%): {model_perplexity:.2f}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
