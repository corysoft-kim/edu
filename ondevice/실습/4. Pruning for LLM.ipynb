{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "71XdNJRThwC3"
   },
   "source": [
    "# Assignment 4. Pruning for LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oivv4vbZh4HX"
   },
   "source": [
    "## Goals\n",
    "\n",
    "본 실습에서는 대규모 언어 모델(Large Language Model, LLM)의 크기를 효과적으로 줄이는 Pruning 기법을 학습합니다. 특히 Magnitude-based pruning과 최근 주목받는 Wanda 기법을 활용하여, 파라미터의 수를 효율적으로 감소시키면서 모델의 성능을 유지하는 방법을 실습합니다.\n",
    "\n",
    "\n",
    "## Contents\n",
    "\n",
    "1. **Magnitude-based Pruning 실습**:\n",
    "   - 간단한 magnitude 기반 pruning 방법을 통해 모델 크기를 감소시키고 성능 변화를 확인합니다.\n",
    "2. **Wanda를 이용한 Pruning 실습**:\n",
    "   - Activation의 중요도를 측정하여 더 정교하게 pruning을 수행하는 Wanda 방법을 구현하고 모델의 성능을 비교합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E7htCbigjGPS"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "필요한 모듈을 불러옵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "el7nPjZPejpK"
   },
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from functools import partial\n",
    "import gc\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UDCyN8n7jNwk"
   },
   "source": [
    "### 모델 평가\n",
    "\n",
    "Wikitext-2 데이터셋을 사용하여 모델의 성능을 평가하는 지표인 perplexity를 계산합니다.\n",
    "\n",
    "**Perplexity란?**\n",
    "- Perplexity는 언어 모델이 주어진 텍스트를 얼마나 잘 예측하는지를 수치로 나타낸 지표입니다.\n",
    "- 수학적으로는 모델이 예측한 확률분포의 \"불확실성\"을 측정하는 값이며, 값이 **낮을수록 모델의 성능이 좋다**고 해석합니다.\n",
    "- 단어 $\\{w_1, w_2, w_3, ..., w_N\\}$으로 구성된 문장의 Perplexity는 다음과 같은 수식으로 나타낼 수 있습니다.\n",
    "    - $Perplexity = \\sqrt[N]{\\frac{1}{\\prod_{i=1}^{N} P(w_i | w_1, w_2, ..., w_{i-1})}}$\n",
    "    - 여기서 $P(w_i | w_1, w_2, ..., w_{i-1})$은 $i$번째에 $w_i$ 단어를 생성할 확률을 의미합니다.\n",
    "    - 즉, Perplexity는 문자의 발생 확률에 대한 역수를 의미하게 됩니다.\n",
    "- 직관적으로 Perplexity가 10이라면 모델이 다음 단어 후보를 10개 정도로 생각하고 있다고 볼 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "w3yRuJ9HejpL"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, tokenizer):\n",
    "    \"\"\"\n",
    "    모델의 perplexity를 계산하는 함수입니다.\n",
    "    \n",
    "    Args:\n",
    "        model: 평가할 모델\n",
    "        tokenizer: 토크나이저\n",
    "        \n",
    "    Returns:\n",
    "        float: 계산된 perplexity 값\n",
    "    \"\"\"\n",
    "    # 테스트 데이터셋 로드 및 전처리\n",
    "    testenc = load_dataset('wikitext', 'wikitext-2-raw-v1', split='test')\n",
    "    testenc = tokenizer(\"\\n\\n\".join(testenc['text']), return_tensors='pt')\n",
    "    \n",
    "    # 입력 데이터를 모델 디바이스로 이동\n",
    "    testenc = testenc.input_ids.to(model.device)\n",
    "    nsamples = 40\n",
    "    model = model.eval()  # 평가 모드로 설정\n",
    "\n",
    "    # Negative log likelihood 계산\n",
    "    nlls = []\n",
    "    for i in tqdm.tqdm(range(nsamples), desc=\"evaluating...\"):\n",
    "        # 배치 데이터 준비\n",
    "        batch = testenc[:, (i * 2048):((i + 1) * 2048)].to(model.device)\n",
    "        \n",
    "        # 모델 추론\n",
    "        with torch.no_grad():\n",
    "            lm_logits = model(batch).logits\n",
    "            \n",
    "        # 로짓과 레이블 시프트\n",
    "        shift_logits = lm_logits[:, :-1, :].contiguous().float()\n",
    "        shift_labels = testenc[:, (i * 2048):((i + 1) * 2048)][:, 1:]\n",
    "        \n",
    "        # 손실 계산\n",
    "        loss_fct = nn.CrossEntropyLoss()\n",
    "        loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "        neg_log_likelihood = loss.float() * 2048\n",
    "        nlls.append(neg_log_likelihood)\n",
    "\n",
    "    # Perplexity 계산 및 반환\n",
    "    return torch.exp(torch.stack(nlls).sum() / (nsamples * 2048))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EAVp3bh5jmft"
   },
   "source": [
    "### OPT 125M 모델 로딩\n",
    "\n",
    "OPT-125M 모델을 로딩하고 평가합니다.\n",
    "여기서 tokenizer는 텍스트(문장)를 모델이 이해할 수 있는 작은 단위(token)로 나누는 역할을 수행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "UjP_N0UiejpM"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since wikitext couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'wikitext-2-raw-v1' at C:\\Users\\swsuser-k06\\.cache\\huggingface\\datasets\\wikitext\\wikitext-2-raw-v1\\0.0.0\\b08601e04326c79dfdd32d625aee71d232d685c3 (last modified on Wed Aug 27 14:52:51 2025).\n",
      "evaluating...: 100%|██████████| 40/40 [00:02<00:00, 17.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "model perplexity: 27.57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model_path = \"facebook/opt-125m\"\n",
    "os.environ[\"HF_HOME\"] = \"D:\\\\data\\\\hf_cache\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\", torch_dtype=torch.float16, use_safetensors=True)\n",
    "\n",
    "# Evaluate the model\n",
    "model_perplexity = evaluate(model, tokenizer)\n",
    "print(f\"\\nmodel perplexity: {model_perplexity:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g8wwbgHjejpN"
   },
   "source": [
    "## [실습 1] Magnitude-based Pruning 구현\n",
    "\n",
    "Magnitude-based pruning 함수를 구현하세요. 구현은 CNN에서 magnitude-based pruning을 구현하는 방식과 유사합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "UOV59TCmejpO"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def prune_magnitude_opt(model, sparsity):\n",
    "    for n, m in model.named_modules():\n",
    "        if isinstance(m, nn.Linear) and \"lm_head\" not in n:\n",
    "            W = m.weight.data\n",
    "            ##################### YOUR CODE STARTS HERE #####################\n",
    "            num_elements = W.numel()\n",
    "            num_zeros = round(num_elements * sparsity)\n",
    "            importance = torch.abs(W)\n",
    "            threshold = torch.kthvalue(importance.flatten(), num_zeros)[0]\n",
    "            mask = importance > threshold\n",
    "            ##################### YOUR CODE ENDS HERE #######################\n",
    "            W.mul_(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pruning된 모델을 평가합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "hG2Sysb5ejpP"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating...: 100%|██████████| 40/40 [00:02<00:00, 19.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "model perplexity (magnitude 50%): 191.96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\", torch_dtype=torch.float16, use_safetensors=True)\n",
    "prune_magnitude_opt(model, 0.5)\n",
    "\n",
    "# Evaluate the model\n",
    "model_perplexity = evaluate(model, tokenizer)\n",
    "print(f\"\\nmodel perplexity (magnitude 50%): {model_perplexity:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GKwCkcKoyJuP"
   },
   "source": [
    "## [실습 2] Calibration 데이터셋 준비\n",
    " \n",
    "[Wanda](https://arxiv.org/pdf/2306.11695)를 사용하여 importance를 계산하기 위해서는 calibration 데이터셋을 통해 activation을 추출해야 합니다.\n",
    "\n",
    "이를 위해 ```get_calib_dataset``` 함수는 calibration dataset을 준비합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6844001eb67549459ca72753a087cac0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/167 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\miniconda3\\envs\\env_aias_test\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\swsuser-k06\\.cache\\huggingface\\hub\\datasets--mit-han-lab--pile-val-backup. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68982dd09ae24283874a41975d32b283",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "val.jsonl.zst:   0%|          | 0.00/471M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "732cbd540cc445149db91a2cc1ac1556",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/214670 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Split into 127 blocks\n"
     ]
    }
   ],
   "source": [
    "def get_calib_dataset(tokenizer=None, n_samples=256, block_size=512):\n",
    "    dataset = load_dataset(\"mit-han-lab/pile-val-backup\", split=\"validation\")\n",
    "    dataset = dataset.shuffle(seed=42)\n",
    "    samples = []\n",
    "    n_run = 0\n",
    "    for data in dataset:\n",
    "        line = data[\"text\"]\n",
    "        line = line.strip()\n",
    "        line_encoded = tokenizer.encode(line)\n",
    "        if len(line_encoded) > block_size:\n",
    "            continue\n",
    "        sample = torch.tensor([line_encoded])\n",
    "        if sample.numel() == 0:\n",
    "            continue\n",
    "        samples.append(sample)\n",
    "        n_run += 1\n",
    "        if n_run == n_samples:\n",
    "            break\n",
    "\n",
    "    # now concatenate all samples and split according to block size\n",
    "    cat_samples = torch.cat(samples, dim=1)\n",
    "    n_split = cat_samples.shape[1] // block_size\n",
    "    print(f\" * Split into {n_split} blocks\")\n",
    "    return [cat_samples[:, i*block_size:(i+1)*block_size] for i in range(n_split)]\n",
    "    \n",
    "samples = get_calib_dataset(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 빈칸을 작성하여 `activation_norm`을 계산하세요.\n",
    "Wanda 기법에서는 `activation_norm`을 L2 norm으로 계산하지만, 구현에서는 calibration 데이터셋에 대해 여러 번 반복하여 누적하므로 \n",
    "`activation_norm` 계산 시 제곱을 적용한 형태로 누적합니다. (이후 값을 사용할 때는 제곱근을 적용해야 합니다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "juFujaD5ejpP"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def get_calib_feat(model, tokenizer, samples):\n",
    "    input_dict = dict()\n",
    "    nsamples_dict = dict()\n",
    "    def add_batch(m, x, y, name):\n",
    "        if name not in input_dict:\n",
    "            input_dict[name] = torch.zeros((m.weight.data.shape[1]), device=m.weight.data.device)\n",
    "            nsamples_dict[name] = 0\n",
    "        \n",
    "        if isinstance(x, tuple):\n",
    "            x = x[0]\n",
    "\n",
    "        if len(x.shape) == 2:\n",
    "            x = x.unsqueeze(0)\n",
    "        tmp = x.shape[0]\n",
    "        if len(x.shape) == 3:\n",
    "            x = x.reshape((-1, x.shape[-1]))\n",
    "        x = x.t()\n",
    "\n",
    "        input_dict[name] *= nsamples_dict[name] / (nsamples_dict[name] + tmp)\n",
    "        nsamples_dict[name] += tmp\n",
    "\n",
    "        x = x.type(torch.float32)\n",
    "        ##################### YOUR CODE STARTS HERE #####################\n",
    "        # activation_norm을 계산하세요.\n",
    "        # x.shape => (hidden_size, batch_size)\n",
    "        activation_norm = torch.norm(x, p=2, dim=1) ** 2\n",
    "        # activation_norm.shape => (hidden_size)\n",
    "        ##################### YOUR CODE ENDS HERE #######################\n",
    "        input_dict[name] += activation_norm / nsamples_dict[name]\n",
    "\n",
    "    hooks = []\n",
    "    for name, m in model.named_modules():\n",
    "        if isinstance(m, nn.Linear) and \"lm_head\" not in name:\n",
    "            hooks.append(\n",
    "                m.register_forward_hook(\n",
    "                    partial(add_batch, name=name)))\n",
    "\n",
    "    print(\"Collecting norm of input activations...\")\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    pbar = tqdm.tqdm(samples)\n",
    "    for input_ids in pbar:\n",
    "        input_ids = input_ids.to(device)\n",
    "        model(input_ids)\n",
    "\n",
    "    for key in input_dict.keys():\n",
    "        input_dict[key].sqrt_()\n",
    "\n",
    "    for hook in hooks:\n",
    "        hook.remove()\n",
    "    return input_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "xNGhY9o6ejpQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting norm of input activations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 127/127 [00:04<00:00, 26.70it/s]\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\", torch_dtype=torch.float16, use_safetensors=True)\n",
    "input_feat = get_calib_feat(model, tokenizer, samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [실습 3] Wanda Pruning 구현\n",
    " \n",
    "Wanda 논문을 참고하여 아래 빈 칸을 채워 Wanda Pruning을 구현하고 실행해보세요.\n",
    "\n",
    "Wanda에서는 일반적인 magnitude-based pruning 방법과 다르게게 weight parameter의 행(row)별로 동일한 희소성(sparsity)으로 pruning을 진행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "zFw9VSo-ejpR"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def prune_wanda_opt(model, sparsity, input_feat):\n",
    "    for n, m in model.named_modules():\n",
    "        if isinstance(m, nn.Linear) and \"lm_head\" not in n:\n",
    "            W = m.weight.data\n",
    "            ##################### YOUR CODE STARTS HERE #####################\n",
    "            row, col = W.shape\n",
    "            num_zeros_per_row = round(col * sparsity)\n",
    "            importance = torch.abs(W) * input_feat[n]\n",
    "            threshold = torch.kthvalue(importance, num_zeros_per_row, dim=1)[0]\n",
    "            mask = importance > threshold.reshape(row, 1)\n",
    "            ##################### YOUR CODE ENDS HERE #######################\n",
    "            W.mul_(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "jZZGZZ4rejpR"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating...: 100%|██████████| 40/40 [00:02<00:00, 19.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "model perplexity (wanda 50%): 39.76\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\", torch_dtype=torch.float16, use_safetensors=True)\n",
    "prune_wanda_opt(model, sparsity=0.5, input_feat=input_feat)\n",
    "\n",
    "# Evaluate the model\n",
    "model_perplexity = evaluate(model, tokenizer)\n",
    "print(f\"\\nmodel perplexity (wanda 50%): {model_perplexity:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "env_aias_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
