{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07b998ce",
   "metadata": {
    "id": "07b998ce"
   },
   "source": [
    "# Assignment 4. Quantization for LLM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9df27e7",
   "metadata": {
    "id": "e9df27e7"
   },
   "source": [
    "## Goals\n",
    "\n",
    "ë³¸ ì‹¤ìŠµì—ì„œëŠ” ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(Large Language Model, LLM)ì— ëŒ€í•´ ì–‘ìí™”(Quantization)ì„ ìˆ˜í–‰í•˜ì—¬ ëª¨ë¸ì„ ì••ì¶•í•˜ëŠ” ë°©ë²•ì„ ì‹¤ìŠµí•©ë‹ˆë‹¤.\n",
    "\n",
    "LLMì€ íŒŒë¼ë¯¸í„°ì˜ ê°œìˆ˜ê°€ ë§¤ìš° ë§ê¸° ë•Œë¬¸ì— ì¼ë°˜ì ìœ¼ë¡œ FP16ìœ¼ë¡œ ê´€ë¦¬í•©ë‹ˆë‹¤. ê·¸ëŸ¼ì—ë„ íŒŒë¼ë¯¸í„°ì˜ í¬ê¸°ê°€ ë§ì´ í¬ë©°, LLaMA-2 7Bì™€ ê°™ì´ ì‘ì€ ëª¨ë¸ì— ëŒ€í•´ì„œë„ ëª¨ë°”ì¼ í™˜ê²½ì—ì„œ ìˆ˜í–‰í•˜ê³ ì í•˜ëŠ” ê²½ìš° FP16ì—ì„œë„ ìµœì†Œ 14GB ì´ìƒì˜ ë©”ëª¨ë¦¬ë¥¼ ìš”êµ¬í•˜ë©° ì´ëŠ” ì‹¤ì œë¡œ ëŒë¦¬ê¸°ì— ë¬´ë¦¬ê°€ ìˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ, ì–‘ìí™”ë¥¼ í†µí•´ ëª¨ë¸ì˜ weightë¥¼ ë” ë‚®ì€ precisionìœ¼ë¡œ ì••ì¶•í•˜ëŠ” ê²ƒì´ ê°€ëŠ¥í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aeb698d",
   "metadata": {
    "id": "8aeb698d"
   },
   "source": [
    "## Contents\n",
    "1. Weight-only Quantization\n",
    "  - Weightë§Œ quantizationì„ ì ìš©í•©ë‹ˆë‹¤.\n",
    "  - ì¥ì : ë§¤ìš° ë‚®ì€ bit-widthë¡œ weightë¥¼ ì–‘ìí™”í•  ìˆ˜ ìˆìœ¼ë©°, ë‹¨ì¼ ë°°ì¹˜ ì¶”ë¡  í™˜ê²½ì—ì„œ ìœ ë¦¬í•©ë‹ˆë‹¤.\n",
    "  - ë‹¨ì : Dequantization í›„ FP16 ì—°ì‚°ì„ ìˆ˜í–‰í•´ì•¼ í•©ë‹ˆë‹¤.\n",
    "  - ì˜ˆì‹œ: AWQ (W3A16, W4A16)\n",
    "2. Weight and Activation Quantization\n",
    "  - Weightì™€ activation ëª¨ë‘ quantizationì„ ì ìš©í•©ë‹ˆë‹¤.\n",
    "  - ì¥ì : ë‚®ì€ precisionì˜ ì—°ì‚°ì„ í†µí•´ ê°€ì† ê°€ëŠ¥í•˜ë©°, ëŒ€ê·œëª¨ ë°°ì¹˜ ì¶”ë¡  í™˜ê²½ì—ì„œ ìœ ë¦¬í•©ë‹ˆë‹¤.\n",
    "  - ë‹¨ì : weightì˜ bit-widthë¥¼ ë‚®ì¶”ê¸°ì— í•œê³„ê°€ ì¡´ì¬í•©ë‹ˆë‹¤.\n",
    "  - ì˜ˆì‹œ: SmoothQuant (W8A8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174d2d1e",
   "metadata": {
    "id": "174d2d1e"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70247c0",
   "metadata": {
    "id": "e70247c0"
   },
   "source": [
    "ì‹¤ìŠµì— í•„ìš”í•œ íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2797112a",
   "metadata": {
    "collapsed": true,
    "id": "2797112a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing packages...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100  1311  100  1311    0     0   1900      0 --:--:-- --:--:-- --:--:--  1905\n",
      "100  1311  100  1311    0     0   1900      0 --:--:-- --:--:-- --:--:--  1902\n",
      "\n",
      " 10  449M   10 45.6M    0     0  31.6M      0  0:00:14  0:00:01  0:00:13 31.6M\n",
      " 41  449M   41  187M    0     0  77.2M      0  0:00:05  0:00:02  0:00:03  144M\n",
      " 73  449M   73  328M    0     0  94.7M      0  0:00:04  0:00:03  0:00:01  139M\n",
      "100  449M  100  449M    0     0   106M      0  0:00:04  0:00:04 --:--:--  146M\n"
     ]
    }
   ],
   "source": [
    "print('Installing packages...')\n",
    "# !pip install torch transformers==4.31.0 accelerate==0.21.0 sentencepiece==0.1.99 tokenizers==0.13.3 datasets==2.15.0 tqdm zstandard huggingface-hub==0.27.0\n",
    "!curl -L https://huggingface.co/datasets/mit-han-lab/pile-val-backup/resolve/main/val.jsonl.zst -o \"D:\\\\data\\\\val.jsonl.zst\"\n",
    "datapath = \"D:\\\\data\\\\val.jsonl.zst\"\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ[\"HF_HOME\"] = \"D:\\\\data\\\\hf_cache\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56475ee1",
   "metadata": {
    "id": "56475ee1"
   },
   "source": [
    "ì‹¤ìŠµì— í•„ìš”í•œ ëª¨ë“ˆì„ ë¡œë“œí•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cfe5330c",
   "metadata": {
    "id": "cfe5330c"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from functools import partial\n",
    "import gc\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import Normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Kq-32KkcyKcM",
   "metadata": {
    "id": "Kq-32KkcyKcM"
   },
   "source": [
    "ë‹¤ìŒ ì½”ë“œëŠ” ëª¨ë¸ í¬ê¸°ë¥¼ ê³„ì‚°í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32204623",
   "metadata": {
    "id": "32204623"
   },
   "outputs": [],
   "source": [
    "class LLMModel:\n",
    "    def __init__(self, model_name):\n",
    "        self.model_name = model_name\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", torch_dtype=torch.float16, use_safetensors=True)\n",
    "        self.model.eval()\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
    "        testenc = load_dataset('wikitext', 'wikitext-2-raw-v1', split='test')\n",
    "        testenc = self.tokenizer(\"\\n\\n\".join(testenc['text']), return_tensors='pt')\n",
    "        self.testenc = testenc.input_ids.to(self.model.device)\n",
    "\n",
    "        self.model_changed = False\n",
    "\n",
    "    def _evaluate(self):\n",
    "        nsamples = 10\n",
    "        nlls = []\n",
    "        for i in tqdm(range(nsamples), desc=\"evaluating...\"):\n",
    "            batch = self.testenc[:, (i * 2048):((i + 1) * 2048)].to(self.model.device)\n",
    "            with torch.no_grad():\n",
    "                lm_logits = self.model(batch).logits\n",
    "            shift_logits = lm_logits[:, :-1, :].contiguous().float()\n",
    "            shift_labels = self.testenc[:, (i * 2048):((i + 1) * 2048)][:, 1:]\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "            neg_log_likelihood = loss.float() * 2048\n",
    "            nlls.append(neg_log_likelihood)\n",
    "\n",
    "        return torch.exp(torch.stack(nlls).sum() / (nsamples * 2048))\n",
    "\n",
    "    def get_model_size(self, data_width=16, group_size=-1):\n",
    "        if group_size != -1:\n",
    "            data_width += (16 + 4) / group_size\n",
    "\n",
    "        num_elements = 0\n",
    "        for param in self.model.parameters():\n",
    "            num_elements += param.numel()\n",
    "        return num_elements * data_width\n",
    "\n",
    "    def model_delete(self):\n",
    "        del self.model\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    def model_evaluate(self, data_width, group_size):\n",
    "        model_perplexity = self._evaluate()\n",
    "        model_size = self.get_model_size(data_width=data_width, group_size=group_size)\n",
    "        print(f\"\\nmodel perplexity: {model_perplexity:.2f}\")\n",
    "        print(f\"model size: {model_size/1024/1024/8:.2f} MiB\")\n",
    "        return model_perplexity\n",
    "\n",
    "    def model_reset(self):\n",
    "        if self.model_changed:\n",
    "            self.model_delete()\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(self.model_name, device_map=\"auto\", torch_dtype=torch.float16, use_safetensors=True)\n",
    "            self.model.eval()\n",
    "            self.model_changed = False\n",
    "\n",
    "    def model_change(self, model: nn.Module):\n",
    "        self.model_delete()\n",
    "        self.model = model\n",
    "        self.model.eval()\n",
    "        self.model_changed = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926ee638",
   "metadata": {
    "id": "926ee638"
   },
   "source": [
    "ë¨¼ì € FP32 ëª¨ë¸ì˜ í˜¼ë€ë„(perflexity)ì™€ ëª¨ë¸ í¬ê¸°ë¥¼ í‰ê°€í•´ë´…ì‹œë‹¤.\n",
    "\n",
    "LLaMA-65B ëª¨ë¸ì˜ ë””ì½”ë”© ë‹¨ê³„ì—ì„œ ë‹¨ì¼ ë°°ì¹˜ ì¶”ë¡ ì„ ìˆ˜í–‰í•  ë•Œ, ìš°ë¦¬ëŠ” $[1, 8192] \\times [8192, 8192]$ í˜•íƒœì˜ GEMV(General Matrix-Vector Multiplication)ì—°ì‚°ì„ ìˆ˜í–‰í•´ì•¼ í•©ë‹ˆë‹¤.\n",
    "\n",
    "NVIDIA A100 80Gì˜ ê²½ìš°, **half-precision(FP16)** ì—ì„œì˜ ì„±ëŠ¥ì€ 312TFLOPSì´ë©°, memory bandwidthëŠ” ì•½ 2000GB/s ì…ë‹ˆë‹¤. ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ, **ê³„ì‚° ì§‘ì•½ë„(computation intensity)** ë¥¼ ê³„ì‚°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:\n",
    "\n",
    "$$\n",
    "\\frac{\\text{FLOP}}{\\text{Byte}} = \\frac{2\\times 8192^2}{8192^2} << \\frac{3.12\\times 10^{11}}{2\\times 10^9}\n",
    "$$\n",
    "\n",
    "ì´ëŠ” ë§¤ìš° ë©”ëª¨ë¦¬ ì œì•½ì (Memory-bounded)(~$10^2$ gap)ìœ¼ë¡œ, ì €ë¹„íŠ¸ ê°€ì¤‘ì¹˜ ì–‘ìí™”ê°€ í•„ìš”í•œ ì´ìœ ì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b723b52b",
   "metadata": {
    "id": "b723b52b"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dced98c042f3441f9b0363db546abd8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/651 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\miniconda3\\envs\\env_aias_test\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in D:\\data\\hf_cache\\hub\\models--facebook--opt-125m. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efac670bc04347b19cf0263d0ff122ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/251M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0f27aca7ba0467bb1aaae6d470132d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90284d54ecc1402eb8c2c9e0c4536984",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/685 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d17ee9358f14d61bf35c73279e3e507",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a55d1594f6d74f6c8a58bd823bfb4d15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f7b9a7debdc420c95fdaaef0442b3cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/441 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7205270097a4a72a2171bf309779892",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\miniconda3\\envs\\env_aias_test\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in D:\\data\\hf_cache\\hub\\datasets--wikitext. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b14e4b1058914bb6af96af92e5aaa2a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/733k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa17edd88ac648e28f5ff6e4613634d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/6.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98a02cf47cbb41e89208ced3158c3030",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation-00000-of-00001.parquet:   0%|          | 0.00/657k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57fd6988bcb1467dac5a2328d34ae328",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/4358 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d383b2dc87da4e6d9ad5654ca7146498",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/36718 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5784c98815d94bab9a3fddae32945c18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/3760 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:01<00:00,  7.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "model perplexity: 28.67\n",
      "model size: 480.08 MiB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(28.6723, device='cuda:0')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = \"facebook/opt-125m\"\n",
    "llm_model = LLMModel(model_path)\n",
    "\n",
    "# Evaluate the model\n",
    "llm_model.model_evaluate(data_width=32, group_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7e1c7d",
   "metadata": {
    "id": "2e7e1c7d"
   },
   "source": [
    "# 4.1. Weight-only Quantization (AWQ)\n",
    "\n",
    "AWQ (activation aware weight only quantization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61fe61f6",
   "metadata": {
    "id": "61fe61f6"
   },
   "source": [
    "ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì€ ë‹¤ì–‘í•œ ì‘ì—…ì—ì„œ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ê³  ìˆì§€ë§Œ, ì—„ì²­ë‚œ ëª¨ë¸ í¬ê¸°ë¡œ ì¸í•´ í•˜ë“œì›¨ì–´ì  ì¥ë²½(ë©”ëª¨ë¦¬ í¬ê¸°)ì´ ë†’ì•„ì§€ê³ , í† í° ìƒì„± ì†ë„ê°€ ëŠë ¤ì§‘ë‹ˆë‹¤(ë©”ëª¨ë¦¬ ëŒ€ì—­í­). LLMì˜ í¬ê¸°ì™€ ê³„ì‚°ëŸ‰ì€ ê¸°í•˜ê¸‰ìˆ˜ì ìœ¼ë¡œ ì¦ê°€í•˜ê³  ìˆëŠ” ë°˜ë©´, ë©”ëª¨ë¦¬ ëŒ€ì—­í­ì€ ëŠë¦¬ê²Œ ì¦ê°€í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì´ ê²©ì°¨ëŠ” LLM ì„±ëŠ¥ì—ì„œ ì¤‘ìš”í•œ ë³‘ëª© í˜„ìƒì…ë‹ˆë‹¤. ì´ë²ˆ ì‹¤ìŠµì—ì„œëŠ” **ìƒˆë¡œìš´ ì–‘ìí™” ì•Œê³ ë¦¬ì¦˜(AWQ)**ì„ ì‚¬ìš©í•˜ì—¬ LLMì˜ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ì¤„ì´ê³  ì¶”ë¡  ì†ë„ë¥¼ ê°€ì†í™”í•˜ëŠ” ë°©ë²•ì„ íƒêµ¬í•  ê²ƒì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9be846",
   "metadata": {
    "id": "2a9be846"
   },
   "source": [
    "## AWQ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "VB2bNIrKySLZ",
   "metadata": {
    "id": "VB2bNIrKySLZ"
   },
   "source": [
    "Uniform quantization ì€ ì‹¤ìˆ˜ ê°’ì„ range $[\\beta, \\alpha]$ì—ì„œ $[0, 2^{b} - 1]$ë¡œ ë§¤í•‘í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.\n",
    "\n",
    "Notation:\n",
    "\n",
    "- Quantized Weight: $w_q$\n",
    "\n",
    "- Scale factor: $s_q$\n",
    "\n",
    "- Zero Point: $z$\n",
    "\\begin{equation}\n",
    "s_q = \\frac{\\alpha - \\beta}{2^{b} - 1} \\tag{1},\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "z = -\\text{Round}(\\beta * scale) \\tag{2}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "w_q = \\text{Clamp}(\\text{Round}(\\frac{w}{s_q}) + z) \\tag{3},\n",
    "\\end{equation}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "au0OJ3lV_irH",
   "metadata": {
    "id": "au0OJ3lV_irH"
   },
   "source": [
    "## Pseudo Quantization\n",
    "ì•„ë˜ ì½”ë“œëŠ” ì˜ì‚¬ ì–‘ìí™”(pseudo quantization)ì„ ìœ„í•œ ê²ƒì…ë‹ˆë‹¤.\n",
    "\n",
    "\n",
    "Pseudo QuantizationëŠ” ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ë¥¼ ì‹¤ì œë¡œ ì–‘ìí™”í•˜ì§€ ì•Šê³ , ì–‘ìí™”ì˜ ì˜í–¥ì„ ì‹œë®¬ë ˆì´ì…˜í•˜ê¸° ìœ„í•´ ì‚¬ìš©ë©ë‹ˆë‹¤. (ì¦‰, ê°€ì¥ ê°€ê¹Œìš´ ì–‘ìí™”ëœ ê°’ìœ¼ë¡œ ë°˜ì˜¬ë¦¼í•œ ë‹¤ìŒ, **ë‹¤ì‹œ ë¶€ë™ ì†Œìˆ˜ì ìœ¼ë¡œ ë³µì›(dequantizing)í•˜ëŠ”** ê²ƒì…ë‹ˆë‹¤.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d63486a4",
   "metadata": {
    "id": "d63486a4"
   },
   "outputs": [],
   "source": [
    "# core quantization method (simulated quantization)\n",
    "def pseudo_quantize_tensor(w, n_bit=4, q_group_size=-1):\n",
    "    org_w_shape = w.shape\n",
    "    if q_group_size > 0:\n",
    "        assert org_w_shape[-1] % q_group_size == 0\n",
    "        w = w.reshape(-1, q_group_size)\n",
    "\n",
    "    assert w.dim() == 2\n",
    "\n",
    "    # Calculate the maximum (\\alpha) and minimum values (\\beta) in the tensor.\n",
    "    max_val = w.amax(dim=1, keepdim=True)\n",
    "    assert max_val.dim() == 2 and max_val.size(0) == w.size(0) and max_val.size(1) == 1\n",
    "    min_val = w.amin(dim=1, keepdim=True)\n",
    "    assert min_val.dim() == 2 and min_val.size(0) == w.size(0) and min_val.size(1) == 1\n",
    "\n",
    "    # Calculate the scale factor and zero point.  (Formula 1 & 2)\n",
    "    max_int = 2 ** n_bit - 1\n",
    "    scales = (max_val - min_val).clamp(min=1e-5) / max_int\n",
    "    assert scales.shape == max_val.shape\n",
    "    zeros = (-torch.round(min_val / scales)).clamp_(0, max_int)\n",
    "    assert scales.shape == min_val.shape\n",
    "\n",
    "    assert torch.isnan(scales).sum() == 0\n",
    "    assert torch.isnan(w).sum() == 0\n",
    "\n",
    "    # Quantize W: Map values in the range [\\beta, \\alpha] to lie within [0, 2^b - 1] (Formula 3)\n",
    "    w = torch.clamp(torch.round(w / scales) + zeros, 0, max_int)\n",
    "    assert w.dim() == 2 and w.size(0) == scales.size(0) and w.size(1) == q_group_size\n",
    "\n",
    "    # Dequantize W (pseudo quantization, the inverse transformation of Formula 3)\n",
    "    w = (w - zeros) * scales\n",
    "    assert w.dim() == 2 and w.size(0) == scales.size(0) and w.size(1) == q_group_size\n",
    "\n",
    "    assert torch.isnan(w).sum() == 0\n",
    "\n",
    "    w = w.reshape(org_w_shape)\n",
    "    return w\n",
    "\n",
    "@torch.no_grad()\n",
    "def pseudo_quantize_model_weight(\n",
    "    model, w_bit, q_group_size,\n",
    "):\n",
    "    for n, m in model.named_modules():\n",
    "        if isinstance(m, nn.Linear):\n",
    "            m.weight.data = pseudo_quantize_tensor(m.weight.data, n_bit=w_bit, q_group_size=q_group_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hU8nK-JY0iKA",
   "metadata": {
    "id": "hU8nK-JY0iKA"
   },
   "source": [
    "ì´ì œ quantized 3-bit ëª¨ë¸ì˜ í˜¼ë€ë„(perplexity)ì™€ í¬ê¸°ë¥¼ í‰ê°€í•´ ë´…ì‹œë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc985610",
   "metadata": {
    "id": "fc985610"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 24.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "model perplexity: 64.88\n",
      "model size: 47.12 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(64.8840, device='cuda:0')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_model.model_reset()\n",
    "pseudo_quantize_model_weight(llm_model.model, w_bit=3, q_group_size=128)\n",
    "llm_model.model_changed = True\n",
    "\n",
    "# Evaluate the model\n",
    "llm_model.model_evaluate(data_width=3, group_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "KrDoPrp7Ncma",
   "metadata": {
    "id": "KrDoPrp7Ncma"
   },
   "source": [
    "ëª¨ë¸ í¬ê¸°ê°€ ì¤„ì–´ë“  ê²ƒì€ í™•ì¸í•  ìˆ˜ ìˆì§€ë§Œ, í˜¼ë€ë„(perplexity)ëŠ” ìƒë‹¹íˆ ì¦ê°€í–ˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-BAC3SPY0swu",
   "metadata": {
    "id": "-BAC3SPY0swu"
   },
   "source": [
    "ë…¼ë¬¸ì—ì„œì˜ ê´€ì°°ì— ë”°ë¥´ë©´ LLMì˜ í™œì„±í™”(activations)ì—ì„œ ì¼ë¶€ ì±„ë„ì— **ì•„ì›ƒë¼ì´ì–´(outliers)**ê°€ ì†ŒëŸ‰ ë°œìƒí•˜ê³  ìˆìŠµë‹ˆë‹¤. íŠ¹ì • ì±„ë„ì— ì•„ì›ƒë¼ì´ì–´ê°€ ìˆëŠ” ê²½ìš°, ì´ëŠ” **ëª¨ë“  í† í°ì—ì„œ ì§€ì†ì ìœ¼ë¡œ ë‚˜íƒ€ë‚©ë‹ˆë‹¤.**\n",
    "\n",
    "ì£¼ì–´ì§„ í† í°ì— ëŒ€í•œ ì±„ë„ ê°„ì˜ ë¶„ì‚°(variance)ì€ í¬ì§€ë§Œ(ì¼ë¶€ ì±„ë„ì˜ í™œì„±í™”ëŠ” ë§¤ìš° í¬ê³ , ëŒ€ë¶€ë¶„ì€ ì‘ìŠµë‹ˆë‹¤), íŠ¹ì • ì±„ë„ì˜ í¬ê¸°(magnitude)ê°€ í† í° ê°„ì— ê°€ì§€ëŠ” ë¶„ì‚°ì€ ì‘ìŠµë‹ˆë‹¤(ì•„ì›ƒë¼ì´ì–´ ì±„ë„ì€ ì§€ì†ì ìœ¼ë¡œ í½ë‹ˆë‹¤).\n",
    "\n",
    "\n",
    "AWQ(Activation Aware Weight Quantization)ì˜ ê¸°ë²•ì— ë”°ë¥´ë©´, í™œì„±í™”(activation) ì•„ì›ƒë¼ì´ì–´ì— í•´ë‹¹í•˜ëŠ” ê°€ì¤‘ì¹˜ ì±„ë„ì€ ë” ë‘ë“œëŸ¬ì§€ë©°, ì´ëŸ¬í•œ ë‘ë“œëŸ¬ì§„ ê°€ì¤‘ì¹˜ë¥¼ ë³´ì¡´í•˜ëŠ” ê²ƒì´ ì„±ëŠ¥ í–¥ìƒìœ¼ë¡œ ì´ì–´ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë‹¤ìŒìœ¼ë¡œ, ë‘ë“œëŸ¬ì§„ ê°€ì¤‘ì¹˜ë¥¼ ì°¾ê³  ì›ë˜ ê°’ìœ¼ë¡œ ìœ ì§€í•˜ì—¬ í˜¼ë€ë„(perplexity)ì˜ ë³€í™”ë¥¼ ê´€ì°°í•´ ë³´ê² ìŠµë‹ˆë‹¤.\n",
    "\n",
    "ì•„ë˜ ì½”ë“œëŠ” calibration ë°ì´í„°ì…‹ì„ ë¡œë“œí•˜ì—¬ í™œì„±í™” ì•„ì›ƒë¼ì´ì–´ë¥¼ ì–»ê³  ë‘ë“œëŸ¬ì§„ ê°€ì¤‘ì¹˜ë¥¼ ì‹ë³„í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb52620d",
   "metadata": {
    "id": "fb52620d"
   },
   "outputs": [],
   "source": [
    "def get_calib_dataset(tokenizer=None, n_samples=256, block_size=512):\n",
    "    dataset = load_dataset(\"mit-han-lab/pile-val-backup\", split=\"validation\")\n",
    "    dataset = dataset.shuffle(seed=42)\n",
    "    samples = []\n",
    "    n_run = 0\n",
    "    for data in dataset:\n",
    "        line = data[\"text\"]\n",
    "        line = line.strip()\n",
    "        line_encoded = tokenizer.encode(line)\n",
    "        if len(line_encoded) > block_size:\n",
    "            continue\n",
    "        sample = torch.tensor([line_encoded])\n",
    "        if sample.numel() == 0:\n",
    "            continue\n",
    "        samples.append(sample)\n",
    "        n_run += 1\n",
    "        if n_run == n_samples:\n",
    "            break\n",
    "\n",
    "    # now concatenate all samples and split according to block size\n",
    "    cat_samples = torch.cat(samples, dim=1)\n",
    "    n_split = cat_samples.shape[1] // block_size\n",
    "    print(f\" * Split into {n_split} blocks\")\n",
    "    return [cat_samples[:, i*block_size:(i+1)*block_size] for i in range(n_split)]\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_calib_feat(model, tokenizer):\n",
    "    input_dict = dict()\n",
    "    def stat_input_max_hook(m, x, y, name):\n",
    "        if isinstance(x, tuple):\n",
    "            x = x[0]\n",
    "        x_max = x.view(-1, x.shape[-1]).abs().mean(dim=0).cpu().detach()\n",
    "        if name not in input_dict:\n",
    "            input_dict[name] = [x_max]\n",
    "        else:\n",
    "            input_dict[name] += [x_max]\n",
    "\n",
    "    hooks = []\n",
    "    for name, m in model.named_modules():\n",
    "        if isinstance(m, nn.Linear):\n",
    "            hooks.append(\n",
    "                m.register_forward_hook(\n",
    "                    partial(stat_input_max_hook, name=name)))\n",
    "\n",
    "    print(\"Collecting activation scales...\")\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    samples = get_calib_dataset(tokenizer)\n",
    "    pbar = tqdm(samples)\n",
    "    for input_ids in pbar:\n",
    "        input_ids = input_ids.to(device)\n",
    "        model(input_ids)\n",
    "\n",
    "    for hook in hooks:\n",
    "        hook.remove()\n",
    "    return input_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bfd04606",
   "metadata": {
    "id": "bfd04606"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HTTP Error 429 thrown while requesting HEAD https://huggingface.co/facebook/opt-125m/resolve/main/config.json\n",
      "Retrying in 1s [Retry 1/5].\n",
      "HTTP Error 429 thrown while requesting HEAD https://huggingface.co/facebook/opt-125m/resolve/main/config.json\n",
      "Retrying in 2s [Retry 2/5].\n",
      "HTTP Error 429 thrown while requesting HEAD https://huggingface.co/facebook/opt-125m/resolve/main/config.json\n",
      "Retrying in 4s [Retry 3/5].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting activation scales...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "416fd1fc46ea43caa2ee25de8afc4025",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/167 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\miniconda3\\envs\\env_aias_test\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in D:\\data\\hf_cache\\hub\\datasets--mit-han-lab--pile-val-backup. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee67b0fc4d3447a1a88aef1b94e8a677",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "val.jsonl.zst:   0%|          | 0.00/471M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e6a4730f3fb4f48805678568a5e6e07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/214670 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Split into 127 blocks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 127/127 [00:04<00:00, 26.78it/s]\n"
     ]
    }
   ],
   "source": [
    "llm_model.model_reset()\n",
    "input_feat = get_calib_feat(llm_model.model, llm_model.tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "X_pPpL0FGful",
   "metadata": {
    "id": "X_pPpL0FGful"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "127\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n"
     ]
    }
   ],
   "source": [
    "print(type(input_feat['model.decoder.layers.0.self_attn.q_proj']))\n",
    "print(len(input_feat['model.decoder.layers.0.self_attn.q_proj']))\n",
    "print(input_feat['model.decoder.layers.0.self_attn.q_proj'][0].shape)\n",
    "print(sum(input_feat['model.decoder.layers.0.self_attn.q_proj']).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "h5GGLwczKghz",
   "metadata": {
    "id": "h5GGLwczKghz"
   },
   "source": [
    "# [ì‹¤ìŠµ 1] Scale 1% salient channels\n",
    "\n",
    "1%ì˜ ê°€ì¤‘ì¹˜ë¥¼ FP16ìœ¼ë¡œ ìœ ì§€í•˜ë©´ ëª¨ë¸ í¬ê¸°(ì´ ë¹„íŠ¸ ìˆ˜ë¡œ ì¸¡ì •)ë¥¼ í¬ê²Œ ëŠ˜ë¦¬ì§€ ì•Šê³ ë„ ì–‘ìí™” ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆì§€ë§Œ, ì´ëŸ¬í•œ í˜¼í•© ì •ë°€ë„ ë°ì´í„° ìœ í˜•ì€ ì‹œìŠ¤í…œ êµ¬í˜„ì„ ì–´ë µê²Œ ë§Œë“­ë‹ˆë‹¤.\n",
    "\n",
    "ë”°ë¼ì„œ ì¤‘ìš”í•œ ê°€ì¤‘ì¹˜ë¥¼ ì‹¤ì œë¡œ FP16ìœ¼ë¡œ ìœ ì§€í•˜ì§€ ì•Šê³  ì¤‘ìš”í•œ ê°€ì¤‘ì¹˜ë¥¼ ë³´í˜¸í•  ìˆ˜ ìˆëŠ” ë°©ë²•ì„ ì°¾ì•„ì•¼ í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "srM0CaQw3xyc",
   "metadata": {
    "id": "srM0CaQw3xyc"
   },
   "source": [
    "AWQì˜ ë°©ë²•ë¡ ì— ë”°ë¥´ë©´, ì¤‘ìš”í•œ ê°€ì¤‘ì¹˜ ì±„ë„ì„ ë‹¨ìˆœíˆ ìŠ¤ì¼€ì¼ë§í•˜ì—¬(íŠ¹ì •í•œ ê°’ì„ ê³±í•´ ì£¼ì–´) ë³´í˜¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì›ë¦¬ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n",
    "\n",
    "- Linear layer channel $\\mathbf{y} = \\mathbf{w}x$ (from $\\mathbf{W}x$)ì¼ ë•Œ, ìš°ë¦¬ê°€ ì£¼ëª©í•´ì•¼ í•  ê²ƒì€ ì–‘ìí™” í•¨ìˆ˜ $Q(\\mathbf{w})x$ìœ¼ë¡œ ë°œìƒí•˜ëŠ” quantization errorì…ë‹ˆë‹¤.\n",
    "\n",
    "- Quantization function $Q(\\mathbf{w})$ = $Î”\\cdot Round(\\frac{\\mathbf{w}}{Î”})$, $Î” = \\frac{\\max(|w|)}{2^{N - 1}}$.\n",
    "\n",
    "- Quantization error $Err(Q(\\mathbf{w}) x) = Î”\\cdot RoundErr(\\frac{\\mathbf{w}}{Î”})\\cdot x$\n",
    "- ìŠ¤ì¼€ì¼ë§ ëœ  Quantization error $Err(Q(\\mathbf{w} \\cdot s)(\\frac{x}{s})) = Î”\\cdot RoundErr(\\frac{\\mathbf{w}}{Î”})\\cdot x\\cdot \\mathbf{\\frac{1}{s}}$.\n",
    "- $RoundErr$ ëŠ” ì–¸ì œë‚˜ ~0.25 ì…ë‹ˆë‹¤ (0-0.5 ì‚¬ì´ì˜ í‰ê· ì´ë¯€ë¡œ).\n",
    "- ê·¸ë£¹ì˜ í¬ê¸°ê°€ ì¶©ë¶„íˆ í´ ë•Œ(e.g., 128), í•˜ë‚˜ì˜ ì±„ë„ì„ ìŠ¤ì¼€ì¼ë§í•˜ëŠ” ê²ƒì€ ì¼ë°˜ì ìœ¼ë¡œ ê·¸ë£¹ ë‚´ ìµœëŒ€ ê°’ì„ ì¦ê°€ì‹œí‚¤ì§€ ì•ŠìŠµë‹ˆë‹¤ (ì¦‰, $Î”$ ëŠ” ë³€í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤).\n",
    "- ê·¸ëŸ¬ë¯€ë¡œ, $Err(Q(\\mathbf{w} \\cdot s)(\\frac{x}{s})) = Î”\\cdot RoundErr(\\frac{\\mathbf{w}}{Î”})\\cdot x\\cdot \\mathbf{\\frac{1}{s}}$ < $Î”\\cdot RoundErr(\\frac{\\mathbf{w}}{Î”})\\cdot x = Err(Q(\\mathbf{w}) x)$.\n",
    "\n",
    "ì•„ë˜ ì½”ë“œë¥¼ ì™„ì„±í•˜ì—¬ ì¤‘ìš”í•œ ê°€ì¤‘ì¹˜ ì±„ë„ì„ ìŠ¤ì¼€ì¼ë§í•˜ê³ , ì–‘ìí™” í•œ ë‹¤ìŒ, ë‹¤ì‹œ ìŠ¤ì¼€ì¼ì„ ì¤„ì¸ í›„ í˜¼ë€ë„(perplexity)ì˜ ë³€í™”ë¥¼ ê´€ì°°í•´ë³´ì„¸ìš”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "TdO4KtnsNKf5",
   "metadata": {
    "id": "TdO4KtnsNKf5"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def pseudo_quantize_model_weight_scaleup(\n",
    "    model, w_bit, q_group_size, input_feat, scale_factor\n",
    "):\n",
    "    for n, m in model.named_modules():\n",
    "        if isinstance(m, nn.Linear):\n",
    "            importance = sum(input_feat[n]).float()\n",
    "            # 1í¼ì„¼íŠ¸ ì±„ë„ì˜ ê°œìˆ˜\n",
    "            num_samples = int(len(importance) * 0.01)\n",
    "\n",
    "            ############### YOUR CODE STARTS HERE ###############\n",
    "\n",
    "            # Step 1: importanceë¥¼ ê¸°ì¤€ìœ¼ë¡œ 1%ì˜ ì¤‘ìš”í•œ ì±„ë„ì„ ì°¾ìœ¼ì„¸ìš”  (hint: use torch.topk())\n",
    "            # hint : torch.topk() í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”. torch.topk() í•¨ìˆ˜ëŠ” PyTorchì—ì„œ í…ì„œì˜ ê°’ ì¤‘ ìƒìœ„ kê°œì˜ ê°’ê³¼ ê·¸ë“¤ì˜ ì¸ë±ìŠ¤ë¥¼ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤. torch.topk()[0]ëŠ” ê°’ì„, torch.topk()[1]ì€ ì¸ë±ìŠ¤ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
    "            outlier_mask = torch.topk(importance, k=num_samples)[1]\n",
    "\n",
    "            ############### YOUR CODE ENDS HERE #################\n",
    "            assert outlier_mask.dim() == 1\n",
    "\n",
    "            # ìŠ¤ì¼€ì¼ íŒ©í„°ë¥¼ ì ìš©í•˜ëŠ” ê²ƒì„ ì‹œë®¬ë ˆì´ì…˜í•˜ê¸° ìœ„í•´, ì–‘ìí™” ì „ì— ìŠ¤ì¼€ì¼ íŒ©í„°ë¥¼ ê³±í•˜ê³ , ì–‘ìí™” í›„ì— ìŠ¤ì¼€ì¼ íŒ©í„°ë¡œ ë‚˜ëˆ•ë‹ˆë‹¤.\n",
    "            # scale_factorë¥¼ ì´ìš©í•´ ì¤‘ìš”í•œ ê°€ì¤‘ì¹˜ ì±„ë„ì˜ ê°’ì„ í™•ëŒ€í•©ë‹ˆë‹¤.\n",
    "            m.weight.data[:, outlier_mask] *= scale_factor\n",
    "\n",
    "            m.weight.data = pseudo_quantize_tensor(m.weight.data, n_bit=w_bit, q_group_size=q_group_size)\n",
    "\n",
    "            ############### YOUR CODE STARTS HERE ###############\n",
    "\n",
    "            # Step 2: scale_factorë¥¼ ì´ìš©í•´ ì¤‘ìš”í•œ ê°€ì¤‘ì¹˜ ì±„ë„ì˜ ê°’ì„ ë‹¤ì‹œ ì¶•ì†Œí•˜ì„¸ìš”.\n",
    "            m.weight.data[:, outlier_mask] /= scale_factor\n",
    "\n",
    "            ############### YOUR CODE ENDS HERE #################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "GoTh5CzuPhtV",
   "metadata": {
    "id": "GoTh5CzuPhtV"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 25.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "model perplexity: 53.28\n",
      "model size: 47.12 MiB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(53.2786, device='cuda:0')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_model.model_reset()\n",
    "pseudo_quantize_model_weight_scaleup(llm_model.model, w_bit=3, q_group_size=128, input_feat=input_feat, scale_factor=2)\n",
    "llm_model.model_changed = True\n",
    "\n",
    "# Evaluate the model\n",
    "llm_model.model_evaluate(data_width=3, group_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc63e22",
   "metadata": {
    "id": "ffc63e22"
   },
   "source": [
    "ìŠ¤ì¼€ì¼ë§ì„ í†µí•´ì„œ ì¤‘ìš”í•œ ê°€ì¤‘ì¹˜ë¥¼ ë³´í˜¸í•¨ê³¼ ë™ì‹œì—, ëª¨ë“  ê°€ì¤‘ì¹˜ë¥¼ 3bitë¡œ ìœ ì§€í•  ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b245f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_path = \"facebook/opt-125m\"\n",
    "# llm_model = LLMModel(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4SBUNQc_MDcP",
   "metadata": {
    "id": "4SBUNQc_MDcP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scale_factor=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 23.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "model perplexity: 64.88\n",
      "model size: 47.12 MiB\n",
      "scale_factor=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 21.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "model perplexity: 53.28\n",
      "model size: 47.12 MiB\n",
      "scale_factor=3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 20.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "model perplexity: 69.95\n",
      "model size: 47.12 MiB\n",
      "scale_factor=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 21.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "model perplexity: 154.36\n",
      "model size: 47.12 MiB\n"
     ]
    }
   ],
   "source": [
    "for scale_factor in [1,2,3,4]:\n",
    "    llm_model.model_reset()\n",
    "    pseudo_quantize_model_weight_scaleup(llm_model.model, w_bit=3, q_group_size=128, input_feat=input_feat, scale_factor=scale_factor)\n",
    "    llm_model.model_changed = True\n",
    "\n",
    "    # Evaluate the model\n",
    "    print(f\"scale_factor={scale_factor}\")\n",
    "    llm_model.model_evaluate(data_width=3, group_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kDIYt2xJEt84",
   "metadata": {
    "id": "kDIYt2xJEt84"
   },
   "source": [
    "ì½”ë“œì—ì„œ ì„œë¡œ ë‹¤ë¥¸ ìŠ¤ì¼€ì¼ë§ íŒ©í„° $s$(ì˜ˆ: 1, 2, 3, 4)ë¥¼ ì‹œë„í•˜ê³  í˜¼ë€ë„(perplexity)ì˜ ë³€í™”ë¥¼ ê´€ì°°í•´ë³´ì„¸ìš”.\n",
    "\n",
    "í˜¼ë€ë„(perplexity)ê°€ ë¨¼ì € ê°ì†Œí•˜ë‹¤ê°€ ë‹¤ì‹œ ì¦ê°€í•˜ëŠ” ê²ƒì„ ê´€ì°°í–ˆë‚˜ìš”?\n",
    "\n",
    "ë„ˆë¬´ í° íŒ©í„°ë¡œ ìŠ¤ì¼€ì¼ë§í•˜ë©´ ê·¸ë£¹ ë‚´ ìµœëŒ€ ê°’ì´ ì¦ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤(ì¦‰,$Î”$ê°€ ì¦ê°€í•¨).\n",
    "\n",
    "ì´ëŠ” ë‹¤ë¥¸ ì±„ë„ì˜ ì–‘ìí™”ì— ì˜í–¥ì„ ë¯¸ì¹  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ueFx8RCHF2zu",
   "metadata": {
    "id": "ueFx8RCHF2zu"
   },
   "source": [
    "# [ì‹¤ìŠµ 2] Scale factor search\n",
    "\n",
    "ì§€ê¸ˆê¹Œì§€ ìš°ë¦¬ëŠ” ìŠ¤ì¼€ì¼ë§ íŒ©í„°$s$ë¥¼ ì§ì ‘ ì •ì˜í•´ ì£¼ì—ˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "ê·¸ëŸ¬ë‚˜ Fine-tuningì˜ ë¶ˆì•ˆì •ì„± ë•Œë¬¸ì—, ë¯¸ë¦¬ ì •ì˜ëœ ê²€ìƒ‰ ê³µê°„ ë‚´ì—ì„œ ìµœì ì˜\n",
    "$s$ë¥¼ ì°¾ëŠ” ê²ƒì´ ë” ë‚˜ì€ ì„ íƒì´ ë  ê²ƒì…ë‹ˆë‹¤. ìš°ë¦¬ëŠ” ì¤‘ìš”í•œ ê°€ì¤‘ì¹˜ë¥¼ ë³´í˜¸í•˜ë©´ì„œ ë‹¤ë¥¸ ê°’ì„ ê³ ë ¤í•˜ê¸° ìœ„í•´ ê²€ìƒ‰ ê³µê°„ ë‚´ì—ì„œ ìµœì ì˜ ìŠ¤ì¼€ì¼ì„ ì°¾ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "ì‹¤ì œë¡œ, ë…¼ë¬¸ì—ì„œëŠ” í™œì„±í™”ë§Œ ê³ ë ¤í•˜ëŠ” ê²ƒìœ¼ë¡œë„ ì¢‹ì€ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆìŒì„ ê´€ì°°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "ìš°ë¦¬ëŠ” ìŠ¤ì¼€ì¼ë§ íŒ©í„° $s$ë¥¼ í™œì„±í™”ì˜ L1-norm (ì¦‰, acviation matrixì˜ ì ˆëŒ“ê°’ë“¤ì˜ í‰ê· )ì˜ $\\alpha$ì œê³±ìœ¼ë¡œ ì„¤ì •í•  ê²ƒì…ë‹ˆë‹¤.\n",
    "\n",
    "$\\alpha$ì˜ ê°’ì€ grid searchë¥¼ í†µí•´ ì ì ˆí•œ ê°’ìœ¼ë¡œ ê²€ìƒ‰í•©ë‹ˆë‹¤.\n",
    "\n",
    "ê²€ìƒ‰ì„ ìœ„í•œ ì½”ë“œë¥¼ ì¶”ê°€í•˜ê³  ì‹¤í–‰í•˜ì—¬ í˜¼ë€ë„(perplexity)ë¥¼ ê´€ì°°í•˜ì„¸ìš”."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bA4b4L2OL05N",
   "metadata": {
    "id": "bA4b4L2OL05N"
   },
   "source": [
    "$$\n",
    "ğ‹(\\mathbf{s})=\\lVert Q(\\mathbf{W}\\cdot \\mathbf{s})  (\\mathbf{s^{-1}} \\cdot \\mathbf{X}) - \\mathbf{W}\\mathbf{X}  \\rVert,  \\quad\\mathbf{s}= \\mathbf{s_X}^{\\alpha},  \\mathbf{s_X} = \\|X\\|_1\n",
    "$$\n",
    "$$\n",
    "\\mathbf{s}^* = \\text{argmin}_{\\mathbf{s}} ğ‹(\\mathbf{s}),\\quad \\alpha^*=\\text{argmin}_{\\alpha} ğ‹(\\mathbf{s_X}^{\\alpha})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ff_sv6k0R2Eb",
   "metadata": {
    "id": "ff_sv6k0R2Eb"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def scale_ln_fcs(ln, fcs, scales):\n",
    "    if not isinstance(fcs, list):\n",
    "        fcs = [fcs]\n",
    "\n",
    "    scales = scales.to(ln.weight.device)\n",
    "\n",
    "    ln.weight.div_(scales)\n",
    "    if hasattr(ln, 'bias') and ln.bias is not None:\n",
    "        ln.bias.div_(scales)\n",
    "\n",
    "    for fc in fcs:\n",
    "        fc.weight.mul_(scales.view(1, -1))\n",
    "\n",
    "    for p in ln.parameters():\n",
    "        assert torch.isnan(p).sum() == 0\n",
    "    for fc in fcs:\n",
    "        for p in fc.parameters():\n",
    "            assert torch.isnan(p).sum() == 0\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def scale_fc_fc(fc1, fc2, scales):\n",
    "    assert isinstance(fc1, nn.Linear)\n",
    "    assert isinstance(fc2, nn.Linear)\n",
    "\n",
    "    scales = scales.to(fc1.weight.device)\n",
    "\n",
    "    # fc1.weight.div_(scales.view(-1, 1))\n",
    "    fc1.weight[-scales.size(0):].div_(scales.view(-1, 1))\n",
    "    if fc1.bias is not None:\n",
    "        fc1.bias.div_(scales.view(-1))\n",
    "\n",
    "    fc2.weight.mul_(scales.view(1, -1))\n",
    "\n",
    "    for p in fc1.parameters():\n",
    "        assert torch.isnan(p).sum() == 0\n",
    "    for p in fc2.parameters():\n",
    "        assert torch.isnan(p).sum() == 0\n",
    "\n",
    "@torch.no_grad()\n",
    "def auto_scale_block(module, name, w_bit,\n",
    "                     q_group_size,\n",
    "                     input_feat):\n",
    "\n",
    "    # find the best scale ratio\n",
    "    def _search_module_scale(block, linears2scale: list, x, kwargs={}):\n",
    "\n",
    "        x = x.to(next(block.parameters()).device)\n",
    "        with torch.no_grad():\n",
    "            org_out = block(x, **kwargs)\n",
    "            if isinstance(org_out, tuple):\n",
    "                org_out = org_out[0]\n",
    "\n",
    "        s_x = x.view(-1, x.shape[-1]).abs().mean(0)\n",
    "        s_x = torch.clamp(s_x, 1e-5)\n",
    "\n",
    "\n",
    "        # Step 1: best_error, best_ratio, ë° best_scalesë¥¼ ì´ˆê¸°í™”\n",
    "        best_error = torch.inf\n",
    "        best_ratio = -1\n",
    "        best_scales = 0\n",
    "\n",
    "\n",
    "        n_grid = 20\n",
    "        history = []\n",
    "\n",
    "        org_sd = {k: v.cpu() for k, v in block.state_dict().items()}\n",
    "        for ratio in range(n_grid):\n",
    "            # ratio is the \\alpha in the formula\n",
    "            ratio = ratio * 1 / n_grid\n",
    "\n",
    "            ############### YOUR CODE STARTS HERE ###############\n",
    "\n",
    "            # Step 2: ê³µì‹ì— ë”°ë¼ ìŠ¤ì¼€ì¼ ê³„ì‚°\n",
    "            scales = s_x ** ratio\n",
    "            # scales = --- IGNORE ---\n",
    "\n",
    "            ############### YOUR CODE ENDS HERE #################\n",
    "            assert scales.shape == s_x.shape\n",
    "\n",
    "            scales = scales / (scales.max() * scales.min()).sqrt().view(1, -1)\n",
    "\n",
    "            for fc in linears2scale:\n",
    "\n",
    "                scales = scales.to(fc.weight.device)\n",
    "\n",
    "                # scale_factorë¥¼ ì´ìš©í•´ ì¤‘ìš”í•œ ê°€ì¤‘ì¹˜ ì±„ë„ì˜ ê°’ì„ í™•ëŒ€í•©ë‹ˆë‹¤.\n",
    "                fc.weight.mul_(scales)\n",
    "\n",
    "                fc.weight.data = pseudo_quantize_tensor(fc.weight.data, w_bit, q_group_size)\n",
    "\n",
    "                ############### YOUR CODE STARTS HERE ###############\n",
    "\n",
    "                # Step 3: scale_factorë¥¼ ì´ìš©í•´ ì¤‘ìš”í•œ ê°€ì¤‘ì¹˜ ì±„ë„ì˜ ê°’ì„ ë‹¤ì‹œ ì¶•ì†Œí•˜ì„¸ìš”.\n",
    "                fc.weight.data\n",
    "\n",
    "                ############### YOUR CODE ENDS HERE #################\n",
    "\n",
    "            out = block(x, **kwargs)\n",
    "            if isinstance(out, tuple):\n",
    "                out = out[0]\n",
    "\n",
    "            loss = (org_out - out).float().pow(2).mean().item()  # float prevents overflow\n",
    "            history.append(loss)\n",
    "            is_best = loss < best_error\n",
    "            if is_best:\n",
    "                best_error = loss\n",
    "                best_ratio = ratio\n",
    "                best_scales = scales\n",
    "            block.load_state_dict(org_sd)\n",
    "\n",
    "        if best_ratio == -1:\n",
    "            print(history)\n",
    "            raise Exception\n",
    "\n",
    "        best_scales = best_scales.view(-1)\n",
    "\n",
    "        assert torch.isnan(best_scales).sum() == 0, best_scales\n",
    "        return best_scales.detach()\n",
    "\n",
    "    # attention input\n",
    "    inp = input_feat[name + '.self_attn.out_proj']\n",
    "    inp = torch.cat([x.unsqueeze(0) for x in inp], dim=0).unsqueeze(0)\n",
    "    qkv = [module.self_attn.q_proj, module.self_attn.k_proj, module.self_attn.v_proj]\n",
    "    final_scales = _search_module_scale(module.self_attn, qkv, inp)\n",
    "    scale_ln_fcs(module.self_attn_layer_norm, qkv, final_scales)\n",
    "\n",
    "    # attn out\n",
    "    inp = input_feat[name + '.self_attn.out_proj']\n",
    "    inp = torch.cat([x.unsqueeze(0) for x in inp], dim=0)\n",
    "    final_scales = _search_module_scale(module.self_attn.out_proj, [module.self_attn.out_proj], inp)\n",
    "    scale_fc_fc(module.self_attn.v_proj, module.self_attn.out_proj, final_scales)\n",
    "\n",
    "    # fc1\n",
    "    inp = input_feat[name + '.fc1']\n",
    "    inp = torch.cat([x.unsqueeze(0) for x in inp], dim=0)\n",
    "    final_scales = _search_module_scale(module.fc1, [module.fc1], inp)\n",
    "    scale_ln_fcs(module.final_layer_norm, module.fc1, final_scales)\n",
    "\n",
    "    # fc2\n",
    "    inp = input_feat[name + '.fc2']\n",
    "    inp = torch.cat([x.unsqueeze(0) for x in inp], dim=0)\n",
    "    final_scales = _search_module_scale(module.fc2, [module.fc2], inp)\n",
    "    scale_fc_fc(module.fc1, module.fc2, final_scales)\n",
    "\n",
    "@torch.no_grad()\n",
    "def pseudo_quantize_model_weight_auto_scale(\n",
    "    model, w_bit, q_group_size, input_feat\n",
    "):\n",
    "    from transformers.models.opt.modeling_opt import OPTDecoderLayer\n",
    "\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, OPTDecoderLayer):\n",
    "            auto_scale_block(module, name, w_bit, q_group_size, input_feat)\n",
    "\n",
    "    for n, m in model.named_modules():\n",
    "        if isinstance(m, nn.Linear):\n",
    "            m.weight.data = pseudo_quantize_tensor(m.weight.data, n_bit=w_bit, q_group_size=q_group_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cXuQUykZMdKa",
   "metadata": {
    "id": "cXuQUykZMdKa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 25.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "model perplexity: 61.74\n",
      "model size: 47.12 MiB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(61.7358, device='cuda:0')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_model.model_reset()\n",
    "pseudo_quantize_model_weight_auto_scale(llm_model.model, w_bit=3, q_group_size=128, input_feat=input_feat)\n",
    "llm_model.model_changed = True\n",
    "\n",
    "# Evaluate and delete the model\n",
    "llm_model.model_evaluate(data_width=3, group_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8edb0c1",
   "metadata": {
    "id": "c8edb0c1"
   },
   "source": [
    "# 4.2. Weight and Activation Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ae31c3",
   "metadata": {
    "id": "60ae31c3"
   },
   "source": [
    "ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì€ ë‹¤ì–‘í•œ ì‘ì—…ì—ì„œ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ê³  ìˆì§€ë§Œ, ì—„ì²­ë‚œ ëª¨ë¸ í¬ê¸°ë¡œ ì¸í•´ í•˜ë“œì›¨ì–´ì  ì¥ë²½(ë©”ëª¨ë¦¬ í¬ê¸°)ì´ ë†’ì•„ì§€ê³ , í† í° ìƒì„± ì†ë„ê°€ ëŠë ¤ì§‘ë‹ˆë‹¤(ë©”ëª¨ë¦¬ ëŒ€ì—­í­). LLMì˜ í¬ê¸°ì™€ ê³„ì‚°ëŸ‰ì€ ê¸°í•˜ê¸‰ìˆ˜ì ìœ¼ë¡œ ì¦ê°€í•˜ê³  ìˆëŠ” ë°˜ë©´, ë©”ëª¨ë¦¬ ëŒ€ì—­í­ì€ ëŠë¦¬ê²Œ ì¦ê°€í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì´ ê²©ì°¨ëŠ” LLM ì„±ëŠ¥ì—ì„œ ì¤‘ìš”í•œ ë³‘ëª© í˜„ìƒì…ë‹ˆë‹¤. ì´ë²ˆ ì‹¤ìŠµì—ì„œëŠ” **ìƒˆë¡œìš´ ì–‘ìí™” ì•Œê³ ë¦¬ì¦˜(AWQ)**ì„ ì‚¬ìš©í•˜ì—¬ LLMì˜ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ì¤„ì´ê³  ì¶”ë¡  ì†ë„ë¥¼ ê°€ì†í™”í•˜ëŠ” ë°©ë²•ì„ íƒêµ¬í•  ê²ƒì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235839a0",
   "metadata": {
    "id": "235839a0"
   },
   "source": [
    "ì´ì „ ìˆ˜ì—…ì—ì„œëŠ” ì–‘ìí™”(Quantization)ì˜ ê¸°ë³¸ ë°©ë²•ë“¤ì„ ë°°ì› ìŠµë‹ˆë‹¤.\n",
    "\n",
    "ì–‘ìí™”ì—ëŠ” ë‘ ê°€ì§€ ìœ í˜•ì´ ìˆìŠµë‹ˆë‹¤:\n",
    "\n",
    "- ê°€ì¤‘ì¹˜(weight)ì™€ í™œì„±í™”(activation) ëª¨ë‘ ì–‘ìí™”\n",
    "    - ê³„ì‚° í•œê³„ê°€ ìˆëŠ” ì‹œë‚˜ë¦¬ì˜¤ì—ì„œ ë” ìœ ë¦¬í•©ë‹ˆë‹¤: ì˜ˆë¥¼ ë“¤ì–´ ì»¨í…ìŠ¤íŠ¸ ë‹¨ê³„ë‚˜ ëŒ€ê·œëª¨ ë°°ì¹˜ ì¶”ë¡ \n",
    "    - ì˜ˆì‹œ: SmoothQuant(W8A8 quantization)\n",
    "- ê°€ì¤‘ì¹˜(weight)ë§Œ ì–‘ìí™”\n",
    "    - ë©”ëª¨ë¦¬ í•œê³„ê°€ ìˆëŠ” ì‹œë‚˜ë¦¬ì˜¤ì—ì„œ ë” ìœ ë¦¬í•©ë‹ˆë‹¤: ì˜ˆë¥¼ ë“¤ì–´ ë””ì½”ë”© ë‹¨ê³„ë‚˜ ë‹¨ì¼ ë°°ì¹˜ ì¶”ë¡ .\n",
    "    - ì˜ˆì‹œ: AWQ(W4A16 quantization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be754200",
   "metadata": {
    "id": "be754200"
   },
   "source": [
    "ì´ ë…¸íŠ¸ë¶ì—ì„œëŠ” OPT-125m ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ SmoothQuantê°€ ê°€ì¤‘ì¹˜ì™€ í™œì„±í™” ëª¨ë‘ì— 8ë¹„íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ FP16 ëª¨ë¸ê³¼ ë™ì¼í•œ ì •í™•ë„ë¥¼ ë‹¬ì„±í•  ìˆ˜ ìˆìŒì„ ë³´ì—¬ì¤ë‹ˆë‹¤. SmoothQuantëŠ” Linear layerì—ì„œ ì™„ì „í•œ INT8 GEMMì„ ê°€ëŠ¥í•˜ê²Œ í•˜ê³ , ì´ìƒê°’ì„ ë‚˜íƒ€ë‚´ê¸° ìœ„í•´ ê³ ì •ë°€ë„ ìˆ«ìë¥¼ ìš”êµ¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cce38d88",
   "metadata": {
    "id": "cce38d88"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from transformers.models.opt.modeling_opt import OPTDecoderLayer\n",
    "from transformers.models.bloom.modeling_bloom import BloomBlock\n",
    "from transformers.models.llama.modeling_llama import LlamaDecoderLayer, LlamaRMSNorm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f061090a",
   "metadata": {
    "id": "f061090a"
   },
   "source": [
    "Uniform quantization ì€ ì‹¤ìˆ˜ ê°’ì„ range$[\\beta, \\alpha]$ì—ì„œ $[0, 2^{b} - 1]$ë¡œ ë§¤í•‘í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.\n",
    "\n",
    "Notation:\n",
    "\n",
    "- Quantized Weight: $w_q$\n",
    "\n",
    "- Scale factor: $s_q$\n",
    "\n",
    "- Zero Point: $z$\n",
    "\\begin{equation}\n",
    "s_q = \\frac{\\alpha - \\beta}{2^{b} - 1} \\tag{1},\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "z = -\\text{Round}(\\beta * scale) \\tag{2}\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "w_q = \\text{Clamp}(\\text{Round}(\\frac{w}{s_q}) + z) \\tag{3},\n",
    "\\end{equation}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8dafc5",
   "metadata": {
    "id": "fd8dafc5"
   },
   "source": [
    "## Pseudo Quantization\n",
    "ì•„ë˜ ì½”ë“œëŠ” ì˜ì‚¬ ì–‘ìí™”(pseudo quantization)ì„ ìœ„í•œ í´ë˜ìŠ¤ì…ë‹ˆë‹¤.\n",
    "\n",
    "Pseudo QuantizationëŠ” ëª¨ë¸ì˜ weightì™€ activationì„ ì‹¤ì œë¡œ ì–‘ìí™”í•˜ì§€ ì•Šê³ , ì–‘ìí™”ì˜ ì˜í–¥ì„ ì‹œë®¬ë ˆì´ì…˜í•˜ê¸° ìœ„í•´ ì‚¬ìš©ë©ë‹ˆë‹¤. (ì¦‰, ê°€ì¥ ê°€ê¹Œìš´ ì–‘ìí™”ëœ ê°’ìœ¼ë¡œ ë°˜ì˜¬ë¦¼í•œ ë‹¤ìŒ, **ë‹¤ì‹œ ë¶€ë™ ì†Œìˆ˜ì ìœ¼ë¡œ ë³µì›(dequantizing)**í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.)\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì—ì„œëŠ” ì‹¤ì œ ì—°ì‚°ì—ì„œëŠ” FP16ì„ ì‚¬ìš©í•˜ì—¬ 8ë¹„íŠ¸ dynamic weight and activation qaunitzationì„ ì‹œë®¬ë ˆì´ì…˜ í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "af56c647",
   "metadata": {
    "id": "af56c647"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from functools import partial\n",
    "\n",
    "class W8A8Linear(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features,\n",
    "        out_features,\n",
    "        bias=True,\n",
    "        act_quant=\"per_token\",\n",
    "        quantize_output=False,\n",
    "        quantize_bits=8\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "\n",
    "        self.register_buffer(\n",
    "            \"weight\",\n",
    "            torch.randn(\n",
    "                self.out_features,\n",
    "                self.in_features,\n",
    "                dtype=torch.float16,\n",
    "                requires_grad=False,\n",
    "            ),\n",
    "        )\n",
    "        if bias:\n",
    "            self.register_buffer(\n",
    "                \"bias\",\n",
    "                torch.zeros(\n",
    "                    (1, self.out_features), dtype=torch.float16, requires_grad=False\n",
    "                ),\n",
    "            )\n",
    "        else:\n",
    "            self.register_buffer(\"bias\", None)\n",
    "\n",
    "        if act_quant == \"per_token\":\n",
    "            self.act_quant_name = \"per_token\"\n",
    "            self.act_quant = partial(quantize_activation_per_token_absmax, n_bits=8)\n",
    "        elif act_quant == \"per_tensor\":\n",
    "            self.act_quant_name = \"per_tensor\"\n",
    "            self.act_quant = partial(quantize_activation_per_tensor_absmax, n_bits=8)\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid act_quant: {act_quant}\")\n",
    "\n",
    "        if quantize_output:\n",
    "            self.output_quant_name = self.act_quant_name\n",
    "            self.output_quant = self.act_quant\n",
    "        else:\n",
    "            self.output_quant_name = \"None\"\n",
    "            self.output_quant = lambda x: x\n",
    "\n",
    "        self.quantize_bits = quantize_bits\n",
    "\n",
    "    def to(self, *args, **kwargs):\n",
    "        super(W8A8Linear, self).to(*args, **kwargs)\n",
    "        self.weight = self.weight.to(*args, **kwargs)\n",
    "        if self.bias is not None:\n",
    "            self.bias = self.bias.to(*args, **kwargs)\n",
    "        return self\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, x):\n",
    "        q_x = self.act_quant(x)\n",
    "        y = torch.functional.F.linear(q_x, self.weight, self.bias)\n",
    "        q_y = self.output_quant(y)\n",
    "        return q_y\n",
    "\n",
    "    @staticmethod\n",
    "    def from_float(\n",
    "        module, weight_quant=\"per_channel\", act_quant=\"per_token\", quantize_output=False, quantize_bits=8\n",
    "    ):\n",
    "        assert isinstance(module, torch.nn.Linear)\n",
    "        new_module = W8A8Linear(\n",
    "            module.in_features,\n",
    "            module.out_features,\n",
    "            module.bias is not None,\n",
    "            act_quant=act_quant,\n",
    "            quantize_output=quantize_output,\n",
    "        )\n",
    "        if weight_quant == \"per_channel\":\n",
    "            new_module.weight = quantize_weight_per_channel_absmax(\n",
    "                module.weight, n_bits=new_module.quantize_bits\n",
    "            )  # use 8-bit integer for weight\n",
    "        elif weight_quant == \"per_tensor\":\n",
    "            new_module.weight = quantize_weight_per_tensor_absmax(\n",
    "                module.weight, n_bits=new_module.quantize_bits\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid weight_quant: {weight_quant}\")\n",
    "        new_module.weight_quant_name = weight_quant\n",
    "        if module.bias is not None:\n",
    "            new_module.bias = module.bias\n",
    "        return new_module\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"W8A8Linear({self.in_features}, {self.out_features}, bias={self.bias is not None}, weight_quant={self.weight_quant_name}, act_quant={self.act_quant_name}, output_quant={self.output_quant_name})\"\n",
    "\n",
    "@torch.no_grad()\n",
    "def quantize_weight_per_channel_absmax(w, n_bits=8):\n",
    "    # w: (out_features, in_features)\n",
    "    scales = w.abs().max(dim=-1, keepdim=True)[0]\n",
    "    q_max = 2 ** (n_bits - 1) - 1\n",
    "    scales.clamp_(min=1e-5).div_(q_max)\n",
    "    w.div_(scales).round_().mul_(scales)\n",
    "    return w\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def quantize_weight_per_tensor_absmax(w, n_bits=8):\n",
    "    # w: (out_features, in_features)\n",
    "    scales = w.abs().max()\n",
    "    q_max = 2 ** (n_bits - 1) - 1\n",
    "    scales.clamp_(min=1e-5).div_(q_max)\n",
    "    w.div_(scales).round_().mul_(scales)\n",
    "    return w\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def quantize_activation_per_token_absmax(t, n_bits=8):\n",
    "    t_shape = t.shape\n",
    "    t.view(-1, t_shape[-1])\n",
    "    scales = t.abs().max(dim=-1, keepdim=True)[0]\n",
    "    q_max = 2 ** (n_bits - 1) - 1\n",
    "    scales.clamp_(min=1e-5).div_(q_max)\n",
    "    t.div_(scales).round_().mul_(scales)\n",
    "    return t\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def quantize_activation_per_tensor_absmax(t, n_bits=8):\n",
    "    t_shape = t.shape\n",
    "    t.view(-1, t_shape[-1])\n",
    "    scales = t.abs().max()\n",
    "    q_max = 2 ** (n_bits - 1) - 1\n",
    "    scales.clamp_(min=1e-5).div_(q_max)\n",
    "    t.div_(scales).round_().mul_(scales)\n",
    "    return t\n",
    "\n",
    "def quantize_opt(\n",
    "    model, weight_quant=\"per_tensor\", act_quant=\"per_tensor\", quantize_bmm_input=True, quantize_bits=8\n",
    "):\n",
    "    from transformers.models.opt.modeling_opt import (\n",
    "        OPTAttention,\n",
    "        OPTDecoderLayer,\n",
    "    )\n",
    "\n",
    "    for name, m in model.model.named_modules():\n",
    "        if isinstance(m, OPTDecoderLayer):\n",
    "            m.fc1 = W8A8Linear.from_float(\n",
    "                m.fc1, weight_quant=weight_quant, act_quant=act_quant, quantize_bits=8\n",
    "            )\n",
    "            m.fc2 = W8A8Linear.from_float(\n",
    "                m.fc2, weight_quant=weight_quant, act_quant=act_quant, quantize_bits=8\n",
    "            )\n",
    "        elif isinstance(m, OPTAttention):\n",
    "            # Her we simulate quantizing BMM inputs by quantizing the output of q_proj, k_proj, v_proj\n",
    "            m.q_proj = W8A8Linear.from_float(\n",
    "                m.q_proj,\n",
    "                weight_quant=weight_quant,\n",
    "                act_quant=act_quant,\n",
    "                quantize_output=quantize_bmm_input, quantize_bits=8\n",
    "            )\n",
    "            m.k_proj = W8A8Linear.from_float(\n",
    "                m.k_proj,\n",
    "                weight_quant=weight_quant,\n",
    "                act_quant=act_quant,\n",
    "                quantize_output=quantize_bmm_input, quantize_bits=8\n",
    "            )\n",
    "            m.v_proj = W8A8Linear.from_float(\n",
    "                m.v_proj,\n",
    "                weight_quant=weight_quant,\n",
    "                act_quant=act_quant,\n",
    "                quantize_output=quantize_bmm_input, quantize_bits=8\n",
    "            )\n",
    "            m.out_proj = W8A8Linear.from_float(\n",
    "                m.out_proj, weight_quant=weight_quant, act_quant=act_quant, quantize_bits=8\n",
    "            )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87c4b54",
   "metadata": {
    "id": "a87c4b54"
   },
   "source": [
    "ì´ì œ quantized 8-bit ëª¨ë¸ì˜ í˜¼ë€ë„(perplexity)ì™€ í¬ê¸°ë¥¼ í‰ê°€í•´ ë´…ì‹œë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "37ca07f9",
   "metadata": {
    "id": "37ca07f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPTForCausalLM(\n",
      "  (model): OPTModel(\n",
      "    (decoder): OPTDecoder(\n",
      "      (embed_tokens): Embedding(50272, 768, padding_idx=1)\n",
      "      (embed_positions): OPTLearnedPositionalEmbedding(2050, 768)\n",
      "      (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (layers): ModuleList(\n",
      "        (0-11): 12 x OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): W8A8Linear(768, 768, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (v_proj): W8A8Linear(768, 768, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (q_proj): W8A8Linear(768, 768, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (out_proj): W8A8Linear(768, 768, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): W8A8Linear(768, 3072, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (fc2): W8A8Linear(3072, 768, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50272, bias=False)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:01<00:00,  9.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "model perplexity: 31.64\n",
      "model size: 121.77 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(31.6428, device='cuda:0')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_model.model_reset()\n",
    "model_w8a8 = quantize_opt(llm_model.model, quantize_bits=8)\n",
    "print(model_w8a8)\n",
    "llm_model.model_change(model_w8a8)\n",
    "\n",
    "# Evaluate and delete the model\n",
    "llm_model.model_evaluate(data_width=8, group_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790d536c",
   "metadata": {
    "id": "790d536c"
   },
   "source": [
    "ëª¨ë¸ í¬ê¸°ê°€ ì¤„ì–´ë“  ê²ƒì€ í™•ì¸í•  ìˆ˜ ìˆì§€ë§Œ, í˜¼ë€ë„(perplexity)ëŠ” ì•½ê°„ ì¦ê°€í–ˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc12e9b",
   "metadata": {
    "id": "bcc12e9b"
   },
   "source": [
    "AWQì˜ ê´€ì°°ì—ì„œì™€ ë§ˆì°¬ê°€ì§€ë¡œ, LLMì˜ í™œì„±í™”(activations)ì—ì„œ ì¼ë¶€ ì±„ë„ì— **ì•„ì›ƒë¼ì´ì–´(outliers)**ê°€ ì†ŒëŸ‰ ë°œìƒí•˜ê³  ìˆìŠµë‹ˆë‹¤. íŠ¹ì • ì±„ë„ì— ì•„ì›ƒë¼ì´ì–´ê°€ ìˆëŠ” ê²½ìš°, ì´ëŠ” **ëª¨ë“  í† í°ì—ì„œ ì§€ì†ì ìœ¼ë¡œ ë‚˜íƒ€ë‚©ë‹ˆë‹¤.**\n",
    "\n",
    "ì£¼ì–´ì§„ í† í°ì— ëŒ€í•œ ì±„ë„ ê°„ì˜ ë¶„ì‚°(variance)ì€ í¬ì§€ë§Œ(ì¼ë¶€ ì±„ë„ì˜ í™œì„±í™”ëŠ” ë§¤ìš° í¬ê³ , ëŒ€ë¶€ë¶„ì€ ì‘ìŠµë‹ˆë‹¤), íŠ¹ì • ì±„ë„ì˜ í¬ê¸°(magnitude)ê°€ í† í° ê°„ì— ê°€ì§€ëŠ” ë¶„ì‚°ì€ ì‘ìŠµë‹ˆë‹¤(ì•„ì›ƒë¼ì´ì–´ ì±„ë„ì€ ì§€ì†ì ìœ¼ë¡œ í½ë‹ˆë‹¤).\n",
    "\n",
    "Smoothquant ë…¼ë¬¸ì˜ ê´€ì°°ì— ë”°ë¥´ë©´, ì´ëŸ¬í•œ í˜„ìƒì€ activationì—ì„œë§Œ ë°œê²¬ë˜ëŠ” í˜„ìƒì´ë©°, weightì—ì„œëŠ” ë°œê²¬ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\n",
    "\n",
    "ê·¸ë ‡ê¸° ë–„ë¬¸ì—, AWQ ê¸°ë²•ê³¼ ê°™ì´ weightì— ëŒ€í•´ì„œëŠ” 4bit ì •ë„ì˜ ë‚®ì€ ì •ë°€ë„ë¡œ quantizationì´ ê°€ëŠ¥í•˜ì§€ë§Œ, activationì—ì„œëŠ” ë§¤ìš° í° ì •í™•ë„ í•˜ë½ì´ ë°œìƒí•©ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50a1ebc",
   "metadata": {
    "id": "c50a1ebc"
   },
   "source": [
    "## Migrate the quantization difficulty from activations to weights\n",
    "\n",
    "ì–‘ìí™” ì˜¤ë¥˜(quantization error)ë¥¼ ì¤„ì´ê¸° ìœ„í•´ì„œëŠ” ëª¨ë“  ì±„ë„ì— ëŒ€í•´ ìœ íš¨ ì–‘ìí™” ë¹„íŠ¸ìˆ˜ë¥¼ ì¦ê°€ì‹œì¼œì•¼ í•©ë‹ˆë‹¤.\n",
    "\n",
    "ê·¸ëŸ¬ë‚˜ ì—°ì‚° ê³¼ì •ì—ì„œ activationì€ ì±„ë„ ì°¨ì›ì´ ì•„ë‹Œ í† í° ì°¨ì›ì—ì„œ í–‰ë ¬ ê³±ì…ˆì´ ì´ë£¨ì–´ì§€ê¸° ë•Œë¬¸ì—, per-channel quantizationì„ ë„ì…í•˜ëŠ” ê²ƒìœ¼ë¡œëŠ” ì†ë„ì˜ í–¥ìƒì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n",
    "\n",
    "ëŒ€ì‹  Smoothquantì—ì„œëŠ” activationì„ per-channel smoothing fator $\\mathbf{s}$ë¡œ ë‚˜ëˆ„ì–´ \"smooth\"í•˜ëŠ” ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤.\n",
    "\n",
    "ì´ ë°©ë²•ì€ ê° activation ì±„ë„ì— ë…ë¦½ì ì¸ ìŠ¤ì¼€ì¼ë§ ì¸ìë¥¼ ì ìš©í•˜ì—¬ ì…ë ¥ í™œì„±í™”ì˜ ì´ìƒì¹˜(outliers)ë¥¼ í‰í™œí™”í•˜ê³ , ì´ë¡œ ì¸í•´ ì–‘ìí™” ê³¼ì •ì—ì„œ ë°œìƒí•  ìˆ˜ ìˆëŠ” ì˜¤ë¥˜ì™€ ì •í™•ë„ ì†ì‹¤ì„ ìµœì†Œí™”í•©ë‹ˆë‹¤.\n",
    "\n",
    "ì¦‰, ê° í™œì„±í™” ì±„ë„ì˜ ìŠ¤ì¼€ì¼ì„ ì¡°ì •í•¨ìœ¼ë¡œì¨ ì „ì²´ í–‰ë ¬ì˜ ì–‘ìí™”ê°€ ë”ìš± íš¨ê³¼ì ì´ê³  ì•ˆì •ì ìœ¼ë¡œ ì´ë£¨ì–´ì§ˆ ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤.\n",
    "\n",
    "$$\n",
    "Y = (X \\text{diag}(s)^{-1}) \\cdot (\\text{diag}(s)W) = \\hat{X}\\hat{W}\n",
    "$$\n",
    "\n",
    "\n",
    "ì—¬ê¸°ì„œ ì…ë ¥ ğ‘‹ëŠ” ì¼ë°˜ì ìœ¼ë¡œ ì´ì „ì˜ ì„ í˜• ì—°ì‚°(ì˜ˆ: Linear layer, Layer Normalization ë“±)ì—ì„œ ìƒì„±ë˜ë¯€ë¡œ, ìš°ë¦¬ëŠ” ìŠ¤ì¼€ì¼ë§ íŒ©í„° $s$ë¥¼ ì´ì „ ë ˆì´ì–´ì˜ íŒŒë¼ë¯¸í„°ì— ì˜¤í”„ë¼ì¸ìœ¼ë¡œ ë¯¸ë¦¬ ê²°í•©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ë ‡ê²Œ í•˜ë©´ ì¶”ê°€ì ì¸ ìŠ¤ì¼€ì¼ë§ìœ¼ë¡œ ì¸í•œ ì»¤ë„ í˜¸ì¶œ ì˜¤ë²„í—¤ë“œê°€ ë°œìƒí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561dac5e",
   "metadata": {
    "id": "561dac5e"
   },
   "source": [
    "# [ì‹¤ìŠµ 3] Quantization difficulty migration\n",
    "\n",
    "ì‹¤ìŠµì„ í†µí•´ weightì—ëŠ” së¥¼ ê³±í•˜ê³ , activationì—ëŠ” së¥¼ ë‚˜ëˆ„ì–´ quantization ë‚œì´ë„ë¥¼ ë¶„ë°°í•´ ì¤€ ë‹¤ìŒ, quantizationì„ ì§„í–‰í•´ í˜¼ë€ë„(perplexity)ì˜ ë³€í™”ë¥¼ ê´€ì°°í•´ë³´ì„¸ìš”.\n",
    "\n",
    "ì¼ë°˜ì ì¸ Transformerì˜ ë ˆì´ì–´ êµ¬ì¡°ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n",
    "\n",
    "1. Self-Attention Block\n",
    "\n",
    "    Input â†’ LayerNorm â†’ Query/Key/Value ìƒì„± (FC) â†’ Attention ì—°ì‚° â†’ Softmax â†’ Attention Output\n",
    "\n",
    "2. Feed-Forward Network (FFN)\n",
    "\n",
    "    Attention Output â†’ LayerNorm â†’ FC1 â†’ í™œì„±í™” í•¨ìˆ˜ (ReLU, GELU ë“±) â†’ FC2 â†’ Output\n",
    "\n",
    "ê·¸ëŸ¬ë¯€ë¡œ Transformerì˜ ë ˆì´ì–´ì—ì„œ LayerNormì˜ ê°€ì¤‘ì¹˜ì— ìŠ¤ì¼€ì¼ë§ íŒ©í„° $s$ë¥¼ ë‚˜ëˆ„ê³ , FCì˜ ê°€ì¤‘ì¹˜ì— ìŠ¤ì¼€ì¼ë§ íŒ©í„° $s$ë¥¼ ê³±í•˜ë©´ weightì—ëŠ” së¥¼ ê³±í•˜ê³ , activationì—ëŠ” së¥¼ ë‚˜ëˆ„ëŠ” íš¨ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "ì•„ë˜ ì½”ë“œë¥¼ ìˆ˜ì •í•˜ì—¬ LayerNormì˜ ê°€ì¤‘ì¹˜ì— ìŠ¤ì¼€ì¼ë§ íŒ©í„° $s$ë¥¼ ë‚˜ëˆ„ê³ , FCì˜ ê°€ì¤‘ì¹˜ì— ìŠ¤ì¼€ì¼ë§ íŒ©í„° $s$ë¥¼ ê³±í•´ì£¼ì„¸ìš”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8d58d548",
   "metadata": {
    "id": "8d58d548"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def smooth_lm_by_scale(model, scale):\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, OPTDecoderLayer):\n",
    "            attn_ln = module.self_attn_layer_norm\n",
    "            qkv = [\n",
    "                module.self_attn.q_proj,\n",
    "                module.self_attn.k_proj,\n",
    "                module.self_attn.v_proj,\n",
    "            ]\n",
    "            smooth_ln_fcs_by_scale(attn_ln, qkv, scale)\n",
    "\n",
    "            ffn_ln = module.final_layer_norm\n",
    "            fc1 = module.fc1\n",
    "            smooth_ln_fcs_by_scale(ffn_ln, fc1, scale)\n",
    "\n",
    "@torch.no_grad()\n",
    "def smooth_ln_fcs_by_scale(ln, fcs, scale):\n",
    "    if not isinstance(fcs, list):\n",
    "        fcs = [fcs]\n",
    "    assert isinstance(ln, nn.LayerNorm)\n",
    "    for fc in fcs:\n",
    "        assert isinstance(fc, nn.Linear)\n",
    "    ############### YOUR CODE STARTS HERE ###############\n",
    "    # Step 1: layernormì˜ weightì™€ biasë¥¼ scaleë¡œ ë‚˜ëˆ„ì–´ì£¼ì„¸ìš”. (hint: div_()í•¨ìˆ˜ë¥¼ í†µí•´ tensor ì „ì²´ë¥¼ íŠ¹ì •í•œ ê°’ìœ¼ë¡œ ë‚˜ëˆ„ì–´ ì¤„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.)\n",
    "    ln.weight.div_(scale)\n",
    "    ln.bias.div_(scale)\n",
    "    ############### YOUR CODE ENDS HERE #################\n",
    "\n",
    "    for fc in fcs:\n",
    "        ############### YOUR CODE STARTS HERE ###############\n",
    "        # Step 2: fcì˜ weightì— scaleì„ ê³±í•´ì£¼ì„¸ìš”. (hint: mul_()í•¨ìˆ˜ë¥¼ í†µí•´ tensor ì „ì²´ì— íŠ¹ì •í•œ ê°’ì„ ê³±í•´ ì¤„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.)\n",
    "        fc.weight.mul_(scale)\n",
    "        ############### YOUR CODE ENDS HERE #################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "edc223d5",
   "metadata": {
    "id": "edc223d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPTForCausalLM(\n",
      "  (model): OPTModel(\n",
      "    (decoder): OPTDecoder(\n",
      "      (embed_tokens): Embedding(50272, 768, padding_idx=1)\n",
      "      (embed_positions): OPTLearnedPositionalEmbedding(2050, 768)\n",
      "      (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (layers): ModuleList(\n",
      "        (0-11): 12 x OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): W8A8Linear(768, 768, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (v_proj): W8A8Linear(768, 768, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (q_proj): W8A8Linear(768, 768, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (out_proj): W8A8Linear(768, 768, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): W8A8Linear(768, 3072, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (fc2): W8A8Linear(3072, 768, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50272, bias=False)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 10.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "model perplexity: 31.65\n",
      "model size: 121.77 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(31.6455, device='cuda:0')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_model.model_reset()\n",
    "smooth_lm_by_scale(llm_model.model, 5)\n",
    "model_smoothquant_scale = quantize_opt(llm_model.model)\n",
    "print(model_smoothquant_scale)\n",
    "llm_model.model_change(model_smoothquant_scale)\n",
    "\n",
    "# Evaluate the model\n",
    "llm_model.model_evaluate(data_width=8, group_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fef5e4c",
   "metadata": {
    "id": "4fef5e4c"
   },
   "source": [
    "ìŠ¤ì¼€ì¼ë§ì„ í†µí•´ì„œ ê°€ì¤‘ì¹˜ì™€ í™œì„±í™”ì˜ ì–‘ìí™” ë‚œì´ë„ë¥¼ ì ì ˆíˆ ë¶„ë°°í•˜ê³ , ê°€ì¤‘ì¹˜ì™€ í™œì„±í™”ë¥¼ ëª¨ë‘ 8bitë¡œ ìœ ì§€í•  ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b015ccf",
   "metadata": {
    "id": "1b015ccf"
   },
   "source": [
    "ì´ë²ˆì—ëŠ” ì½”ë“œì—ì„œ ì„œë¡œ ë‹¤ë¥¸ ë‹¤ì–‘í•œ ìŠ¤ì¼€ì¼ë§ íŒ©í„° $s$(ì˜ˆ: 0.001, 0.01, 1)ì„ ì‹œë„í•˜ê³  í˜¼ë€ë„(perplexity)ì˜ ë³€í™”ë¥¼ ê´€ì°°í•´ë³´ê² ìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2131db92",
   "metadata": {
    "id": "2131db92"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scale_factor=0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:01<00:00,  9.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "model perplexity: 31.71\n",
      "model size: 121.77 MiB\n",
      "scale_factor=0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:01<00:00,  9.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "model perplexity: 31.62\n",
      "model size: 121.77 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HTTP Error 429 thrown while requesting HEAD https://huggingface.co/facebook/opt-125m/resolve/main/config.json\n",
      "Retrying in 1s [Retry 1/5].\n",
      "HTTP Error 429 thrown while requesting HEAD https://huggingface.co/facebook/opt-125m/resolve/main/config.json\n",
      "Retrying in 2s [Retry 2/5].\n",
      "HTTP Error 429 thrown while requesting HEAD https://huggingface.co/facebook/opt-125m/resolve/main/config.json\n",
      "Retrying in 4s [Retry 3/5].\n",
      "HTTP Error 429 thrown while requesting HEAD https://huggingface.co/facebook/opt-125m/resolve/main/config.json\n",
      "Retrying in 8s [Retry 4/5].\n",
      "HTTP Error 429 thrown while requesting HEAD https://huggingface.co/facebook/opt-125m/resolve/main/config.json\n",
      "Retrying in 8s [Retry 5/5].\n",
      "HTTP Error 429 thrown while requesting HEAD https://huggingface.co/facebook/opt-125m/resolve/main/config.json\n"
     ]
    },
    {
     "ename": "HfHubHTTPError",
     "evalue": "429 Client Error: Too Many Requests for url: https://huggingface.co/api/models/facebook/opt-125m",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\envs\\env_aias_test\\Lib\\site-packages\\huggingface_hub\\utils\\_http.py:409\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    408\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 409\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    410\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\envs\\env_aias_test\\Lib\\site-packages\\requests\\models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[1;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mHTTPError\u001b[0m: 429 Client Error: Too Many Requests for url: https://huggingface.co/api/models/facebook/opt-125m",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mHfHubHTTPError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m scale_factor \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m0.001\u001b[39m,\u001b[38;5;241m0.01\u001b[39m,\u001b[38;5;241m0.1\u001b[39m]:\n\u001b[1;32m----> 2\u001b[0m     \u001b[43mllm_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_reset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m     smooth_lm_by_scale(llm_model\u001b[38;5;241m.\u001b[39mmodel,scale_factor)\n\u001b[0;32m      4\u001b[0m     model \u001b[38;5;241m=\u001b[39m quantize_opt(llm_model\u001b[38;5;241m.\u001b[39mmodel)\n",
      "Cell \u001b[1;32mIn[3], line 54\u001b[0m, in \u001b[0;36mLLMModel.model_reset\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_changed:\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_delete()\n\u001b[1;32m---> 54\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat16\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_safetensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_changed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\envs\\env_aias_test\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:600\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    598\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mconfig_class \u001b[38;5;241m==\u001b[39m config\u001b[38;5;241m.\u001b[39msub_configs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext_config\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    599\u001b[0m         config \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget_text_config()\n\u001b[1;32m--> 600\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    601\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    602\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    603\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    604\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    605\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    606\u001b[0m )\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\envs\\env_aias_test\\Lib\\site-packages\\transformers\\modeling_utils.py:316\u001b[0m, in \u001b[0;36mrestore_default_torch_dtype.<locals>._wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    314\u001b[0m old_dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mget_default_dtype()\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    318\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_default_dtype(old_dtype)\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\envs\\env_aias_test\\Lib\\site-packages\\transformers\\modeling_utils.py:4910\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   4900\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   4901\u001b[0m     gguf_file\n\u001b[0;32m   4902\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   4903\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m ((\u001b[38;5;28misinstance\u001b[39m(device_map, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisk\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisk\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map)\n\u001b[0;32m   4904\u001b[0m ):\n\u001b[0;32m   4905\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   4906\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOne or more modules is configured to be mapped to disk. Disk offload is not supported for models \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   4907\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloaded from GGUF files.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   4908\u001b[0m     )\n\u001b[1;32m-> 4910\u001b[0m checkpoint_files, sharded_metadata \u001b[38;5;241m=\u001b[39m \u001b[43m_get_resolved_checkpoint_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4911\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4912\u001b[0m \u001b[43m    \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4913\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvariant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvariant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4914\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgguf_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgguf_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4915\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_tf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4916\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_flax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4917\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_safetensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_safetensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4918\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4919\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4920\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4921\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4922\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4923\u001b[0m \u001b[43m    \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4924\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4925\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4926\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_auto_class\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   4927\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransformers_explicit_filename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransformers_explicit_filename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4928\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4930\u001b[0m is_sharded \u001b[38;5;241m=\u001b[39m sharded_metadata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   4931\u001b[0m is_quantized \u001b[38;5;241m=\u001b[39m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\envs\\env_aias_test\\Lib\\site-packages\\transformers\\modeling_utils.py:1193\u001b[0m, in \u001b[0;36m_get_resolved_checkpoint_files\u001b[1;34m(pretrained_model_name_or_path, subfolder, variant, gguf_file, from_tf, from_flax, use_safetensors, cache_dir, force_download, proxies, local_files_only, token, user_agent, revision, commit_hash, is_remote_code, transformers_explicit_filename)\u001b[0m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m use_safetensors:\n\u001b[0;32m   1192\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m revision \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmain\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 1193\u001b[0m         resolved_archive_file, revision, is_sharded \u001b[38;5;241m=\u001b[39m \u001b[43mauto_conversion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1194\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcached_file_kwargs\u001b[49m\n\u001b[0;32m   1195\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1196\u001b[0m     cached_file_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrevision\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m revision\n\u001b[0;32m   1197\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m resolved_archive_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\envs\\env_aias_test\\Lib\\site-packages\\transformers\\safetensors_conversion.py:105\u001b[0m, in \u001b[0;36mauto_conversion\u001b[1;34m(pretrained_model_name_or_path, ignore_errors_during_conversion, **cached_file_kwargs)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    104\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ignore_errors_during_conversion:\n\u001b[1;32m--> 105\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\envs\\env_aias_test\\Lib\\site-packages\\transformers\\safetensors_conversion.py:84\u001b[0m, in \u001b[0;36mauto_conversion\u001b[1;34m(pretrained_model_name_or_path, ignore_errors_during_conversion, **cached_file_kwargs)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     83\u001b[0m     api \u001b[38;5;241m=\u001b[39m HfApi(token\u001b[38;5;241m=\u001b[39mcached_file_kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m\"\u001b[39m), headers\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser-agent\u001b[39m\u001b[38;5;124m\"\u001b[39m: http_user_agent()})\n\u001b[1;32m---> 84\u001b[0m     sha \u001b[38;5;241m=\u001b[39m \u001b[43mget_conversion_pr_reference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mapi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcached_file_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     86\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sha \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     87\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\envs\\env_aias_test\\Lib\\site-packages\\transformers\\safetensors_conversion.py:59\u001b[0m, in \u001b[0;36mget_conversion_pr_reference\u001b[1;34m(api, model_id, **kwargs)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_conversion_pr_reference\u001b[39m(api: HfApi, model_id: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m---> 59\u001b[0m     private \u001b[38;5;241m=\u001b[39m \u001b[43mapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mprivate\n\u001b[0;32m     61\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempting to create safetensors variant\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     62\u001b[0m     pr_title \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAdding `safetensors` variant of this model\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\envs\\env_aias_test\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[0;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\envs\\env_aias_test\\Lib\\site-packages\\huggingface_hub\\hf_api.py:2638\u001b[0m, in \u001b[0;36mHfApi.model_info\u001b[1;34m(self, repo_id, revision, timeout, securityStatus, files_metadata, expand, token)\u001b[0m\n\u001b[0;32m   2636\u001b[0m     params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpand\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m expand\n\u001b[0;32m   2637\u001b[0m r \u001b[38;5;241m=\u001b[39m get_session()\u001b[38;5;241m.\u001b[39mget(path, headers\u001b[38;5;241m=\u001b[39mheaders, timeout\u001b[38;5;241m=\u001b[39mtimeout, params\u001b[38;5;241m=\u001b[39mparams)\n\u001b[1;32m-> 2638\u001b[0m \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2639\u001b[0m data \u001b[38;5;241m=\u001b[39m r\u001b[38;5;241m.\u001b[39mjson()\n\u001b[0;32m   2640\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ModelInfo(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdata)\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\envs\\env_aias_test\\Lib\\site-packages\\huggingface_hub\\utils\\_http.py:482\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _format(HfHubHTTPError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    480\u001b[0m \u001b[38;5;66;03m# Convert `HTTPError` into a `HfHubHTTPError` to display request information\u001b[39;00m\n\u001b[0;32m    481\u001b[0m \u001b[38;5;66;03m# as well (request id and/or server error message)\u001b[39;00m\n\u001b[1;32m--> 482\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m _format(HfHubHTTPError, \u001b[38;5;28mstr\u001b[39m(e), response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mHfHubHTTPError\u001b[0m: 429 Client Error: Too Many Requests for url: https://huggingface.co/api/models/facebook/opt-125m"
     ]
    }
   ],
   "source": [
    "for scale_factor in [0.001,0.01,0.1]:\n",
    "    llm_model.model_reset()\n",
    "    smooth_lm_by_scale(llm_model.model,scale_factor)\n",
    "    model = quantize_opt(llm_model.model)\n",
    "    llm_model.model_change(model)\n",
    "\n",
    "    # Evaluate the model\n",
    "    print(f\"scale_factor={scale_factor}\")\n",
    "    llm_model.model_evaluate(data_width=8, group_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10278aff",
   "metadata": {
    "id": "10278aff"
   },
   "source": [
    "## [ì‹¤ìŠµ 4] Scale factor sampling\n",
    "\n",
    "ìŠ¤ì¼€ì¼ë§ íŒ©í„° $s$ê°’ì˜ ì„¤ì •ì— ë”°ë¼ í˜¼ë€ë„(perplexity)ê°€ ë¨¼ì € ë³€í™”í•˜ëŠ” ê²ƒì„ ê´€ì°°í–ˆë‚˜ìš”?\n",
    "\n",
    "\n",
    "ìš°ë¦¬ì˜ ëª©í‘œëŠ” ê° ì±„ë„ë³„ ìŠ¤ì¼€ì¼ë§ íŒ©í„° së¥¼ ì„ íƒí•˜ì—¬  XÌ‚ = Xdiag(s)â»Â¹ê°€ ì–‘ìí™”í•˜ê¸° ì‰½ë„ë¡ ë§Œë“œëŠ” ê²ƒì…ë‹ˆë‹¤.\n",
    "\n",
    "ì–‘ìí™” ì˜¤ë¥˜ë¥¼ ì¤„ì´ê¸° ìœ„í•´ ëª¨ë“  ì±„ë„ì˜ ìœ íš¨ ì–‘ìí™” ë¹„íŠ¸ë¥¼ ëŠ˜ë ¤ì•¼ í•©ë‹ˆë‹¤.\n",
    "\n",
    "ê°€ì¥ ê°„ë‹¨í•œ ì„ íƒì€ ì±„ë„ë³„ë¡œ ì„œë¡œ ë‹¤ë¥¸ ìŠ¤ì¼€ì¼ë§ íŒ©í„°ë¥¼ ì„¤ì •í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.\n",
    "\n",
    "ìŠ¤ì¼€ì¼ë§ íŒ©í„°ë¥¼ weightì˜ ìµœëŒ€ê°’ìœ¼ë¡œ ì„¤ì •í•˜ë©´ weightì˜ ì–‘ìí™” ë‚œì´ë„ê°€ ì‰¬ì›Œì§‘ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ activationì˜ ì–‘ìí™”ëŠ” ì–´ë ¤ì›Œì§‘ë‹ˆë‹¤.\n",
    "\n",
    "ë°˜ëŒ€ë¡œ, ìŠ¤ì¼€ì¼ë§ íŒ©í„°ë¥¼ activationì˜ ìµœëŒ€ê°’ìœ¼ë¡œ ì„¤ì •í•˜ë©´ weightì˜ ì–‘ìí™”ê°€ ì–´ë ¤ì›Œì§‘ë‹ˆë‹¤.\n",
    "\n",
    "ìš°ë¦¬ëŠ” weightì™€ activationì˜ ì–‘ìí™” ë‚œì´ë„ ì‚¬ì´ì—ì„œ ê· í˜•ì„ ë§ì¶”ê¸° ìœ„í•´ ìŠ¤ì¼€ì¼ë§ íŒ©í„° së¥¼ ë‹¤ìŒê³¼ ê°™ì´ ì„¤ì •í•©ë‹ˆë‹¤.\n",
    "\n",
    "$s = \\max(|X|)^{\\alpha} / \\max(|W|)^{1-\\alpha}$\n",
    "\n",
    "ì•„ë˜ ì½”ë“œë¥¼ ìˆ˜ì •í•˜ì—¬ ìŠ¤ì¼€ì¼ë§ íŒ©í„° së¥¼ ìœ„ì˜ ì‹ê³¼ ê°™ì´ ì„¤ì •í•´ì£¼ì„¸ìš”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "AQtTutXMuIYD",
   "metadata": {
    "id": "AQtTutXMuIYD"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def smooth_ln_fcs(ln, fcs, act_scales, alpha=0.5):\n",
    "    if not isinstance(fcs, list):\n",
    "        fcs = [fcs]\n",
    "    assert isinstance(ln, nn.LayerNorm)\n",
    "    for fc in fcs:\n",
    "        assert isinstance(fc, nn.Linear)\n",
    "        assert ln.weight.numel() == fc.in_features == act_scales.numel()\n",
    "\n",
    "    device, dtype = fcs[0].weight.device, fcs[0].weight.dtype\n",
    "    act_scales = act_scales.to(device=device, dtype=dtype)\n",
    "    weight_scales = torch.cat(\n",
    "        [fc.weight.abs().max(dim=0, keepdim=True)[0] for fc in fcs], dim=0\n",
    "    )\n",
    "    weight_scales = weight_scales.max(dim=0)[0].clamp(min=1e-5)\n",
    "\n",
    "    scales = (\n",
    "        ############### YOUR CODE STARTS HERE ###############\n",
    "        #Activation Scales ê°’ê³¼ Weight Scales ê°’ì— alphaë¥¼ ì ì ˆíˆ ê±°ë“­ì œê³±í•´ì£¼ì–´ì•¼ í•©ë‹ˆë‹¤.\n",
    "        #Hint: pow()í•¨ìˆ˜ë¥¼ í†µí•´ì„œ ê±°ë“­ì œê³±ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "        (act_scales ** alpha) / (weight_scales ** (1 - alpha))\n",
    "\n",
    "        ############### YOUR CODE ENDS HERE #################\n",
    "    )\n",
    "\n",
    "    scales.clamp(min=1e-5).to(device).to(dtype)\n",
    "\n",
    "    ln.weight.div_(scales)\n",
    "    ln.bias.div_(scales)\n",
    "\n",
    "    for fc in fcs:\n",
    "        fc.weight.mul_(scales.view(1, -1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sZKrDAfywHK4",
   "metadata": {
    "id": "sZKrDAfywHK4"
   },
   "source": [
    "ì—¬ê¸°ì„œ í™œì„±í™” ë²”ìœ„ëŠ” ë™ì ì´ë©° ì…ë ¥ ìƒ˜í”Œì— ë”°ë¼ ë‹¬ë¼ì§‘ë‹ˆë‹¤.\n",
    "\n",
    "ì‚¬ì „ í›ˆë ¨ ë°ì´í„° ì„¸íŠ¸ì˜ ë³´ì • ìƒ˜í”Œì„ ì‚¬ìš©í•˜ì—¬ í™œì„±í™” ì±„ë„ì˜ í¬ê¸°ë¥¼ ì¶”ì •í•´ë³´ê² ìŠµë‹ˆë‹¤.\n",
    "\n",
    "ì•„ë˜ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ë©´ 512ê°œì˜ ì‚¬ì „ í›ˆë ¨ ìƒ˜í”Œ ë°ì´í„° ì„¸íŠ¸ë¥¼ í†µí•´ ìë™ìœ¼ë¡œ ì ì ˆí•œ ìŠ¤ì¼€ì¼ë§ íŒ©í„° $s$ê°’ì„ ì°¾ì•„ ì–‘ìí™”ë¥¼ ì§„í–‰í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "178b1225",
   "metadata": {
    "id": "178b1225"
   },
   "outputs": [],
   "source": [
    "import functools\n",
    "def get_act_scales(model, tokenizer, dataset_path, num_samples=512, seq_len=512):\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    act_scales = {}\n",
    "\n",
    "    def stat_tensor(name, tensor):\n",
    "        hidden_dim = tensor.shape[-1]\n",
    "        tensor = tensor.view(-1, hidden_dim).abs().detach()\n",
    "        comming_max = torch.max(tensor, dim=0)[0].float().cpu()\n",
    "        if name in act_scales:\n",
    "            act_scales[name] = torch.max(act_scales[name], comming_max)\n",
    "        else:\n",
    "            act_scales[name] = comming_max\n",
    "\n",
    "    def stat_input_hook(m, x, y, name):\n",
    "        if isinstance(x, tuple):\n",
    "            x = x[0]\n",
    "        stat_tensor(name, x)\n",
    "\n",
    "    hooks = []\n",
    "    for name, m in model.named_modules():\n",
    "        if isinstance(m, nn.Linear):\n",
    "            hooks.append(\n",
    "                m.register_forward_hook(functools.partial(stat_input_hook, name=name))\n",
    "            )\n",
    "\n",
    "    dataset = load_dataset(\"json\", data_files=dataset_path, split=\"train\")\n",
    "    dataset = dataset.shuffle(seed=42)\n",
    "\n",
    "    for i in tqdm(range(num_samples)):\n",
    "        input_ids = tokenizer(\n",
    "            dataset[i][\"text\"], return_tensors=\"pt\", max_length=seq_len, truncation=True\n",
    "        ).input_ids.to(device)\n",
    "        model(input_ids)\n",
    "\n",
    "    for h in hooks:\n",
    "        h.remove()\n",
    "\n",
    "    return act_scales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c1dee862",
   "metadata": {
    "id": "c1dee862"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def smooth_lm(model, scales, alpha=0.5):\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, OPTDecoderLayer):\n",
    "            attn_ln = module.self_attn_layer_norm\n",
    "            qkv = [\n",
    "                module.self_attn.q_proj,\n",
    "                module.self_attn.k_proj,\n",
    "                module.self_attn.v_proj,\n",
    "            ]\n",
    "            qkv_input_scales = scales[name + \".self_attn.q_proj\"]\n",
    "            smooth_ln_fcs(attn_ln, qkv, qkv_input_scales, alpha)\n",
    "\n",
    "            ffn_ln = module.final_layer_norm\n",
    "            fc1 = module.fc1\n",
    "            fc1_input_scales = scales[name + \".fc1\"]\n",
    "            smooth_ln_fcs(ffn_ln, fc1, fc1_input_scales, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be97158c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HTTP Error 429 thrown while requesting HEAD https://huggingface.co/facebook/opt-125m/resolve/main/config.json\n",
      "Retrying in 1s [Retry 1/5].\n",
      "HTTP Error 429 thrown while requesting HEAD https://huggingface.co/facebook/opt-125m/resolve/main/config.json\n",
      "Retrying in 2s [Retry 2/5].\n",
      "HTTP Error 429 thrown while requesting HEAD https://huggingface.co/facebook/opt-125m/resolve/main/config.json\n",
      "Retrying in 4s [Retry 3/5].\n",
      "HTTP Error 429 thrown while requesting HEAD https://huggingface.co/facebook/opt-125m/resolve/main/config.json\n",
      "Retrying in 8s [Retry 4/5].\n"
     ]
    }
   ],
   "source": [
    "# model_path = \"facebook/opt-125m\"\n",
    "# llm_model = LLMModel(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deaf4309",
   "metadata": {
    "id": "deaf4309"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84c9273664eb43beb6fdfac249c50eca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "llm_model.model_reset()\n",
    "\n",
    "act_scales = get_act_scales(\n",
    "        llm_model.model, llm_model.tokenizer, datapath, 512, 512)\n",
    "smooth_lm(llm_model.model, act_scales)\n",
    "model_sampled = quantize_opt(llm_model.model)\n",
    "llm_model.model_change(model_sampled)\n",
    "\n",
    "# Evaluate the model\n",
    "llm_model.model_evaluate(data_width=8, group_size=128)\n",
    "\n",
    "del llm_model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eDeUfFGQyRNv",
   "metadata": {
    "id": "eDeUfFGQyRNv"
   },
   "source": [
    "# Rotation Based Quantization\n",
    "ìµœê·¼ì—ëŠ” LLM Quantizationì˜ Outlier ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ Rotation Matrixë¥¼ ê³±í•´ì„œ Outlierë¥¼ ì œê±°í•˜ëŠ” ë°©ë²•ë“¤ì´ ì œì‹œë˜ê³  ìˆìŠµë‹ˆë‹¤. (QuaRot, SpinQuant ë“±)\n",
    "\n",
    "![SpinQuant](https://raw.githubusercontent.com/facebookresearch/SpinQuant/refs/heads/main/SpinQuant.png)\n",
    "\n",
    "í•´ë‹¹ ê¸°ë²•ë“¤ì€ ì§êµ í–‰ë ¬ì˜ ì•„ë˜ì™€ ê°™ì€ íŠ¹ì„±ì„ í™œìš©í•©ë‹ˆë‹¤.\n",
    "\n",
    "1. $$ R\\,R^{T} = I $$\n",
    "\n",
    "2. ë²¡í„° Vì— ì§êµ í–‰ë ¬ Rì„ ê³±í•˜ë©´, ê¸¸ì´ëŠ” ê°™ìœ¼ë‚˜ ë°©í–¥ì´ ë°”ë€ ë²¡í„° V'ë¥¼ êµ¬í•  ìˆ˜ ìˆìŒ\n",
    "\n",
    "\n",
    "\n",
    "## ìˆ˜ì‹ ìœ ë„\n",
    "\n",
    "1. ì›ë˜ ì—°ì‚°  \n",
    "   $$\n",
    "     y = x\\,W\n",
    "   $$\n",
    "\n",
    "2. ì§êµ í–‰ë ¬ $R$ ë„ì…  \n",
    "   $$\n",
    "     y = x\\,I\\,W = x\\,(RR^{T})\\,W = (x\\,R)\\,(R^{T}\\,W)\n",
    "   $$\n",
    "\n",
    "3. ìƒˆë¡œìš´ ë³€ìˆ˜ë¡œ ì •ì˜\n",
    "   $$\n",
    "     x' = x\\,R^{T}\n",
    "     \\quad\n",
    "     W' = R\\,W\n",
    "   $$\n",
    "   ì´ë•Œ  \n",
    "   $$\n",
    "     y = x'\\,W'\n",
    "     = x\\,W\n",
    "   $$  \n",
    "   ê°€ ìˆ˜ì‹ì ìœ¼ë¡œ ë³´ì¡´ë¨\n",
    "  \n",
    "## ì¥ì \n",
    "  - íšŒì „ì„ í†µí•´ ë¶„í¬ë¥¼ ë¶„ì‚°ì‹œì¼œ Outlier í˜„ìƒì„ ì™„í™”  \n",
    "  - ì‚¬ì „ì— Weightì— ê³±í•´ë‘ëŠ” ê²ƒì„ í†µí•´ ì ìš© ê°€ëŠ¥ ë° ì˜¨ë¼ì¸ ì—°ì‚° ì œê±° ê°€ëŠ¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kclGHvQOFbmk",
   "metadata": {
    "id": "kclGHvQOFbmk"
   },
   "outputs": [],
   "source": [
    "llm_model = LLMModel(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RB8t1atrqqaT",
   "metadata": {
    "id": "RB8t1atrqqaT"
   },
   "outputs": [],
   "source": [
    "def get_orthogonal_matrix(size, mode=\"random\", dtype=torch.float32, device=\"cpu\"):\n",
    "    if mode == \"random\":\n",
    "        A = torch.randn(size, size, dtype=torch.float32, device=device)\n",
    "        Q, R = torch.linalg.qr(A)\n",
    "        Q *= torch.sign(torch.diag(R)).unsqueeze(0)\n",
    "        return Q.to(dtype)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown mode: {mode}\")\n",
    "\n",
    "dummy_vector = torch.randn(100)\n",
    "dummy_vector[50] = 30\n",
    "\n",
    "rotation_matrix = get_orthogonal_matrix(100)\n",
    "rotated_vector = rotation_matrix @ dummy_vector\n",
    "\n",
    "def plot_vector(vec, title):\n",
    "    plt.figure(figsize=(5, 2))\n",
    "    plt.plot(vec.abs().numpy(), linewidth=2)\n",
    "    plt.title(title, fontsize=14)\n",
    "    plt.xlabel('Index', fontsize=12)\n",
    "    plt.ylabel('Absolute Value', fontsize=12)\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(rotation_matrix)\n",
    "print(torch.round(rotation_matrix @ rotation_matrix.T))\n",
    "plot_vector(dummy_vector, 'Original Vector (Absolute Values)')\n",
    "plot_vector(rotated_vector, 'Rotated Vector (Absolute Values)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9oSOzVPXd5lo",
   "metadata": {
    "id": "9oSOzVPXd5lo"
   },
   "outputs": [],
   "source": [
    "def quantize_tinyllama(\n",
    "    model, weight_quant=\"per_tensor\", act_quant=\"per_tensor\", quantize_bmm_input=True, quantize_bits=4\n",
    "):\n",
    "    \"\"\"\n",
    "    Quantize TinyLlama model using W8A8Linear layers.\n",
    "    \"\"\"\n",
    "    for name, m in model.model.named_modules():\n",
    "        if isinstance(m, torch.nn.Linear):\n",
    "            # Skip lm_head as it's handled separately\n",
    "            if \"lm_head\" in name:\n",
    "                continue\n",
    "\n",
    "            # Create quantized linear layer\n",
    "            quantized_layer = W8A8Linear.from_float(\n",
    "                m, weight_quant=weight_quant, act_quant=act_quant, quantize_bits=4\n",
    "            )\n",
    "\n",
    "            # Replace the original layer with quantized one\n",
    "            # We need to find the parent module and replace the child\n",
    "            parent_name = \".\".join(name.split(\".\")[:-1])\n",
    "            child_name = name.split(\".\")[-1]\n",
    "\n",
    "            if parent_name:\n",
    "                parent = model.model.get_submodule(parent_name)\n",
    "                setattr(parent, child_name, quantized_layer)\n",
    "            else:\n",
    "                # Root level module\n",
    "                setattr(model.model, child_name, quantized_layer)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "TrHadElhixtg",
   "metadata": {
    "id": "TrHadElhixtg"
   },
   "source": [
    "## Layernorm â†” Linear Fusion\n",
    "\n",
    "ì‹¤ì œ ëª¨ë¸ì—ì„œëŠ” Rotation Matrix ì‚¬ì´ì— Layernormì´ ì¡´ì¬í•˜ëŠ” ê²½ìš°ê°€ ë§ìŠµë‹ˆë‹¤.\n",
    "\n",
    "![LayerNorm](https://miro.medium.com/v2/resize:fit:1252/1*kC-cWBWDEZpkSCtYIUsj4w.png)\n",
    "\n",
    "Normalization Layerì˜ ì˜í–¥ìœ¼ë¡œ Rotation Matrixê°€ ê³±í•´ì§€ì§€ ëª»í•´ ì •ìƒì ìœ¼ë¡œ ì œê±°ë˜ì§€ ëª»í•˜ê³  ëª¨ë¸ì˜ ì—°ì‚°ì´ ë¶€ì •í™•í•´ì§€ëŠ” ë¬¸ì œê°€ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "ë”°ë¼ì„œ, Rotation ì ìš© ì´ì „ì— Normalization Layerë¥¼ Linear Layerì™€ Fusioní•´ì•¼ í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jFe23ot0ePl4",
   "metadata": {
    "id": "jFe23ot0ePl4"
   },
   "outputs": [],
   "source": [
    "# Layer Norm Fusion Functions for TinyLlama\n",
    "def fuse_ln_linear(layernorm, linear_layers):\n",
    "    \"\"\"\n",
    "    Fuse the linear operations in Layernorm into the adjacent linear blocks.\n",
    "    \"\"\"\n",
    "    for linear in linear_layers:\n",
    "        linear_dtype = linear.weight.dtype\n",
    "\n",
    "        # Calculating new weight and bias\n",
    "        W_ = linear.weight.data.double()\n",
    "        linear.weight.data = (W_ * layernorm.weight.double()).to(linear_dtype)\n",
    "\n",
    "        if hasattr(layernorm, \"bias\") and layernorm.bias is not None:\n",
    "            if linear.bias is None:\n",
    "                linear.bias = torch.nn.Parameter(\n",
    "                    torch.zeros(linear.out_features, dtype=torch.float64)\n",
    "                )\n",
    "            linear.bias.data = linear.bias.data.double() + torch.matmul(\n",
    "                W_, layernorm.bias.double()\n",
    "            )\n",
    "            linear.bias.data = linear.bias.data.to(linear_dtype)\n",
    "\n",
    "def fuse_layer_norms_tinyllama(model):\n",
    "    \"\"\"\n",
    "    Fuse layer norms for TinyLlama model structure.\n",
    "    \"\"\"\n",
    "    # Embedding fusion\n",
    "    for W in [model.model.embed_tokens]:\n",
    "        W_ = W.weight.data.double()\n",
    "        W.weight.data = (W_ - W_.mean(dim=-1, keepdim=True)).to(W.weight.data.dtype)\n",
    "\n",
    "    layers = [layer for layer in model.model.layers]\n",
    "\n",
    "    # Fuse the linear operations in Layernorm into the adjacent linear blocks.\n",
    "    for layer in layers:\n",
    "        # fuse the input layernorms into the linear layers\n",
    "        fuse_ln_linear(\n",
    "            layer.input_layernorm,\n",
    "            [layer.self_attn.q_proj, layer.self_attn.k_proj, layer.self_attn.v_proj]\n",
    "        )\n",
    "        fuse_ln_linear(\n",
    "            layer.post_attention_layernorm,\n",
    "            [layer.mlp.gate_proj, layer.mlp.up_proj]\n",
    "        )\n",
    "\n",
    "        # Set layernorm weights to ones\n",
    "        W_norm = layer.input_layernorm.weight.data\n",
    "        layer.input_layernorm.weight.data = torch.ones_like(W_norm)\n",
    "        W_norm = layer.post_attention_layernorm.weight.data\n",
    "        layer.post_attention_layernorm.weight.data = torch.ones_like(W_norm)\n",
    "\n",
    "    # Fuse final norm into lm_head\n",
    "    fuse_ln_linear(\n",
    "        model.model.norm,\n",
    "        [model.lm_head],\n",
    "    )\n",
    "    W_norm = model.model.norm.weight.data\n",
    "    model.model.norm.weight.data = torch.ones_like(W_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hvTcVLnimz2D",
   "metadata": {
    "id": "hvTcVLnimz2D"
   },
   "source": [
    "## [ì‹¤ìŠµ 5] Rotate Matrix ì ìš©\n",
    "\n",
    "QuaRotëŠ” R1ë§Œ ì‚¬ìš©í•˜ì—¬ Roationì„ ì ìš©í•©ë‹ˆë‹¤.\n",
    "\n",
    "ê·¸ë¥¼ ë³´ì™„í•œ SpinQuantì—ì„œëŠ” R2, R3, R4 ë“± ë‹¤ì–‘í•œ Rotation Matrixë¥¼ ì ìš©í•˜ì—¬ Quantization ì •í™•ë„ë¥¼ ë†’ì…ë‹ˆë‹¤.\n",
    "\n",
    "ê·¸ëŸ¬ë‚˜, R2ëŠ” R1ê³¼ ìœ ì‚¬í•˜ê²Œ ì ìš© ê°€ëŠ¥í•˜ê³ , R3ì™€ R4ëŠ” On-lineì—ì„œ êµ¬í•´ì§€ëŠ” Matrixì´ê¸° ë•Œë¬¸ì— êµ¬í˜„ ë‚œì´ë„ë¥¼ ë‚®ì¶”ê¸° ìœ„í•´ QuaRotì„ êµ¬í˜„í•˜ëŠ” ê²ƒìœ¼ë¡œ í•˜ê² ìŠµë‹ˆë‹¤.\n",
    "\n",
    "QuaRot í˜¹ì€ SpinQuant ê·¸ë¦¼ì„ ì°¸ê³ í•˜ì…”ì„œ ê°ê°ì˜ ì—°ì‚°ì— R1ì´ ì–´ë–»ê²Œ ì ìš©ë  ê²ƒì¸ì§€ êµ¬í˜„í•´ë³´ì‹œê¸° ë°”ëë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "huCR1D1d3kf6",
   "metadata": {
    "id": "huCR1D1d3kf6"
   },
   "outputs": [],
   "source": [
    "def rotate_model_weight(\n",
    "    model, R1\n",
    "):\n",
    "    for n, m in model.named_modules():\n",
    "      ############### YOUR CODE STARTS HERE ###############\n",
    "      # Pytorchì—ì„œ @ ì—°ì‚°ì´ Dot Product ì„ì„ ì‚¬ìš©í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.\n",
    "      # nn.Linear ì—°ì‚°ì˜ ParameterëŠ” W^T í˜•íƒœë¡œ ì €ì¥ë˜ì–´ ìˆë‹¤ëŠ” ê²ƒì„ ìœ ì˜í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.\n",
    "      # Embedding Parameter Shape : (Num_Tokens, Hidden_dim)\n",
    "      # Linear Parameter Shape : (Output_Channel, Input_Channel)\n",
    "      # Roation Matrix Shape : (Hidden_dim, Hidden_dim)\n",
    "\n",
    "      if isinstance(m, nn.Embedding):\n",
    "        W_ = m.weight.data\n",
    "        m.weight.data =\n",
    "\n",
    "      if isinstance(m, nn.Linear):\n",
    "        if \"o_proj\" in n or \"down_proj\" in n:\n",
    "          # Att Out Proj, FFN Down Proj\n",
    "          W_ = m.weight.data\n",
    "          m.weight.data =\n",
    "\n",
    "        else:\n",
    "          # QKV Proj, FFN Up Proj, FFN Gate Proj\n",
    "          W_ = m.weight.data\n",
    "          m.weight.data =\n",
    "\n",
    "      ############### YOUR CODE ENDS HERE #################\n",
    "\n",
    "      torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "JiIhAnRdda4P",
   "metadata": {
    "id": "JiIhAnRdda4P"
   },
   "outputs": [],
   "source": [
    "print(\"\\nOriginal ëª¨ë¸ ì„±ëŠ¥ ì¸¡ì • ì¤‘...\")\n",
    "llm_model.model_reset()\n",
    "original_perplexity = llm_model.model_evaluate(data_width=16, group_size=128)\n",
    "output_orig = llm_model.model(llm_model.testenc[:,:500].to(llm_model.model.device), output_hidden_states=True)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Mbdzg5mVIL3j",
   "metadata": {
    "id": "Mbdzg5mVIL3j"
   },
   "outputs": [],
   "source": [
    "Q_BITS = 8\n",
    "\n",
    "print(\"\\nQuantizationë§Œ ì ìš©í•œ ëª¨ë¸ ì„±ëŠ¥ ì¸¡ì • ì¤‘...\")\n",
    "llm_model.model_reset()\n",
    "model = quantize_tinyllama(llm_model.model, quantize_bits=Q_BITS)\n",
    "llm_model.model_change(model)\n",
    "quantized_only_perplexity = llm_model.model_evaluate(data_width=Q_BITS, group_size=128)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zzcHRRb1ILlA",
   "metadata": {
    "id": "zzcHRRb1ILlA"
   },
   "outputs": [],
   "source": [
    "print(\"\\nRotation + Quantization ì ìš©í•œ ëª¨ë¸ ì„±ëŠ¥ ì¸¡ì • ì¤‘...\")\n",
    "llm_model.model_reset()\n",
    "fuse_layer_norms_tinyllama(llm_model.model)\n",
    "hidden_size = llm_model.model.config.hidden_size\n",
    "R1_random = get_orthogonal_matrix(hidden_size, mode=\"random\", dtype=llm_model.model.dtype, device=llm_model.model.device)\n",
    "rotate_model_weight(llm_model.model, R1_random)\n",
    "model = quantize_tinyllama(llm_model.model, quantize_bits=Q_BITS)\n",
    "llm_model.model_change(model)\n",
    "rotation_quantized_perplexity = llm_model.model_evaluate(data_width=Q_BITS, group_size=128)\n",
    "output_rotated = llm_model.model(llm_model.testenc[:,:500].to(llm_model.model.device), output_hidden_states=True)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rSiV8ayaew7s",
   "metadata": {
    "id": "rSiV8ayaew7s"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_2d(data, title):\n",
    "    # Get dimensions of hidden states - handle different shapes\n",
    "    if len(data.shape) == 3:  # (batch, seq_len, hidden_dim)\n",
    "        data = data[0]  # Take first batch\n",
    "    elif len(data.shape) == 2:  # (seq_len, hidden_dim)\n",
    "        pass\n",
    "    else:\n",
    "        print(f\"Unexpected data shape: {data.shape}\")\n",
    "        return\n",
    "\n",
    "    seq_len, hidden_dim = data.shape\n",
    "\n",
    "    mean_activations = np.mean(np.abs(data), axis=0)\n",
    "\n",
    "    # 2D Plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(np.arange(hidden_dim), mean_activations, color='blue', linewidth=1.5)\n",
    "\n",
    "    plt.xlabel(\"Hidden Dimension\", fontsize=12)\n",
    "    plt.ylabel(\"Absolute Value\", fontsize=12)\n",
    "    plt.title(f\"Hidden States Mean Activation - {title}\", fontsize=14)\n",
    "\n",
    "    plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# ì‚¬ìš© ì˜ˆì‹œ\n",
    "plot_2d(output_orig.hidden_states[16][:, 100:].detach().cpu().numpy(), \"Original\")\n",
    "plot_2d(output_rotated.hidden_states[16][:, 100:].detach().cpu().numpy(), \"Rotated\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "env_aias_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
