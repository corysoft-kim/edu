{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07b998ce",
   "metadata": {
    "id": "07b998ce"
   },
   "source": [
    "# Assignment 4. Quantization for LLM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9df27e7",
   "metadata": {
    "id": "e9df27e7"
   },
   "source": [
    "## Goals\n",
    "\n",
    "본 실습에서는 대형 언어 모델(Large Language Model, LLM)에 대해 양자화(Quantization)을 수행하여 모델을 압축하는 방법을 실습합니다.\n",
    "\n",
    "LLM은 파라미터의 개수가 매우 많기 때문에 일반적으로 FP16으로 관리합니다. 그럼에도 파라미터의 크기가 많이 크며, LLaMA-2 7B와 같이 작은 모델에 대해서도 모바일 환경에서 수행하고자 하는 경우 FP16에서도 최소 14GB 이상의 메모리를 요구하며 이는 실제로 돌리기에 무리가 있습니다. 따라서, 양자화를 통해 모델의 weight를 더 낮은 precision으로 압축하는 것이 가능합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aeb698d",
   "metadata": {
    "id": "8aeb698d"
   },
   "source": [
    "## Contents\n",
    "1. Weight-only Quantization\n",
    "  - Weight만 quantization을 적용합니다.\n",
    "  - 장점: 매우 낮은 bit-width로 weight를 양자화할 수 있으며, 단일 배치 추론 환경에서 유리합니다.\n",
    "  - 단점: Dequantization 후 FP16 연산을 수행해야 합니다.\n",
    "  - 예시: AWQ (W3A16, W4A16)\n",
    "2. Weight and Activation Quantization\n",
    "  - Weight와 activation 모두 quantization을 적용합니다.\n",
    "  - 장점: 낮은 precision의 연산을 통해 가속 가능하며, 대규모 배치 추론 환경에서 유리합니다.\n",
    "  - 단점: weight의 bit-width를 낮추기에 한계가 존재합니다.\n",
    "  - 예시: SmoothQuant (W8A8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174d2d1e",
   "metadata": {
    "id": "174d2d1e"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70247c0",
   "metadata": {
    "id": "e70247c0"
   },
   "source": [
    "실습에 필요한 패키지를 설치합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2797112a",
   "metadata": {
    "collapsed": true,
    "id": "2797112a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing packages...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100  1311  100  1311    0     0   1900      0 --:--:-- --:--:-- --:--:--  1905\n",
      "100  1311  100  1311    0     0   1900      0 --:--:-- --:--:-- --:--:--  1902\n",
      "\n",
      " 10  449M   10 45.6M    0     0  31.6M      0  0:00:14  0:00:01  0:00:13 31.6M\n",
      " 41  449M   41  187M    0     0  77.2M      0  0:00:05  0:00:02  0:00:03  144M\n",
      " 73  449M   73  328M    0     0  94.7M      0  0:00:04  0:00:03  0:00:01  139M\n",
      "100  449M  100  449M    0     0   106M      0  0:00:04  0:00:04 --:--:--  146M\n"
     ]
    }
   ],
   "source": [
    "print('Installing packages...')\n",
    "# !pip install torch transformers==4.31.0 accelerate==0.21.0 sentencepiece==0.1.99 tokenizers==0.13.3 datasets==2.15.0 tqdm zstandard huggingface-hub==0.27.0\n",
    "!curl -L https://huggingface.co/datasets/mit-han-lab/pile-val-backup/resolve/main/val.jsonl.zst -o \"D:\\\\data\\\\val.jsonl.zst\"\n",
    "datapath = \"D:\\\\data\\\\val.jsonl.zst\"\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ[\"HF_HOME\"] = \"D:\\\\data\\\\hf_cache\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56475ee1",
   "metadata": {
    "id": "56475ee1"
   },
   "source": [
    "실습에 필요한 모듈을 로드합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cfe5330c",
   "metadata": {
    "id": "cfe5330c"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from functools import partial\n",
    "import gc\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import Normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Kq-32KkcyKcM",
   "metadata": {
    "id": "Kq-32KkcyKcM"
   },
   "source": [
    "다음 코드는 모델 크기를 계산하는 데 사용됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32204623",
   "metadata": {
    "id": "32204623"
   },
   "outputs": [],
   "source": [
    "class LLMModel:\n",
    "    def __init__(self, model_name):\n",
    "        self.model_name = model_name\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", torch_dtype=torch.float16, use_safetensors=True)\n",
    "        self.model.eval()\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
    "        testenc = load_dataset('wikitext', 'wikitext-2-raw-v1', split='test')\n",
    "        testenc = self.tokenizer(\"\\n\\n\".join(testenc['text']), return_tensors='pt')\n",
    "        self.testenc = testenc.input_ids.to(self.model.device)\n",
    "\n",
    "        self.model_changed = False\n",
    "\n",
    "    def _evaluate(self):\n",
    "        nsamples = 10\n",
    "        nlls = []\n",
    "        for i in tqdm(range(nsamples), desc=\"evaluating...\"):\n",
    "            batch = self.testenc[:, (i * 2048):((i + 1) * 2048)].to(self.model.device)\n",
    "            with torch.no_grad():\n",
    "                lm_logits = self.model(batch).logits\n",
    "            shift_logits = lm_logits[:, :-1, :].contiguous().float()\n",
    "            shift_labels = self.testenc[:, (i * 2048):((i + 1) * 2048)][:, 1:]\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "            neg_log_likelihood = loss.float() * 2048\n",
    "            nlls.append(neg_log_likelihood)\n",
    "\n",
    "        return torch.exp(torch.stack(nlls).sum() / (nsamples * 2048))\n",
    "\n",
    "    def get_model_size(self, data_width=16, group_size=-1):\n",
    "        if group_size != -1:\n",
    "            data_width += (16 + 4) / group_size\n",
    "\n",
    "        num_elements = 0\n",
    "        for param in self.model.parameters():\n",
    "            num_elements += param.numel()\n",
    "        return num_elements * data_width\n",
    "\n",
    "    def model_delete(self):\n",
    "        del self.model\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    def model_evaluate(self, data_width, group_size):\n",
    "        model_perplexity = self._evaluate()\n",
    "        model_size = self.get_model_size(data_width=data_width, group_size=group_size)\n",
    "        print(f\"\\nmodel perplexity: {model_perplexity:.2f}\")\n",
    "        print(f\"model size: {model_size/1024/1024/8:.2f} MiB\")\n",
    "        return model_perplexity\n",
    "\n",
    "    def model_reset(self):\n",
    "        if self.model_changed:\n",
    "            self.model_delete()\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(self.model_name, device_map=\"auto\", torch_dtype=torch.float16, use_safetensors=True)\n",
    "            self.model.eval()\n",
    "            self.model_changed = False\n",
    "\n",
    "    def model_change(self, model: nn.Module):\n",
    "        self.model_delete()\n",
    "        self.model = model\n",
    "        self.model.eval()\n",
    "        self.model_changed = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926ee638",
   "metadata": {
    "id": "926ee638"
   },
   "source": [
    "먼저 FP32 모델의 혼란도(perflexity)와 모델 크기를 평가해봅시다.\n",
    "\n",
    "LLaMA-65B 모델의 디코딩 단계에서 단일 배치 추론을 수행할 때, 우리는 $[1, 8192] \\times [8192, 8192]$ 형태의 GEMV(General Matrix-Vector Multiplication)연산을 수행해야 합니다.\n",
    "\n",
    "NVIDIA A100 80G의 경우, **half-precision(FP16)** 에서의 성능은 312TFLOPS이며, memory bandwidth는 약 2000GB/s 입니다. 이를 바탕으로, **계산 집약도(computation intensity)** 를 계산할 수 있습니다:\n",
    "\n",
    "$$\n",
    "\\frac{\\text{FLOP}}{\\text{Byte}} = \\frac{2\\times 8192^2}{8192^2} << \\frac{3.12\\times 10^{11}}{2\\times 10^9}\n",
    "$$\n",
    "\n",
    "이는 매우 메모리 제약적(Memory-bounded)(~$10^2$ gap)으로, 저비트 가중치 양자화가 필요한 이유입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b723b52b",
   "metadata": {
    "id": "b723b52b"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dced98c042f3441f9b0363db546abd8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/651 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\miniconda3\\envs\\env_aias_test\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in D:\\data\\hf_cache\\hub\\models--facebook--opt-125m. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efac670bc04347b19cf0263d0ff122ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/251M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0f27aca7ba0467bb1aaae6d470132d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90284d54ecc1402eb8c2c9e0c4536984",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/685 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d17ee9358f14d61bf35c73279e3e507",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a55d1594f6d74f6c8a58bd823bfb4d15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f7b9a7debdc420c95fdaaef0442b3cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/441 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7205270097a4a72a2171bf309779892",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\miniconda3\\envs\\env_aias_test\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in D:\\data\\hf_cache\\hub\\datasets--wikitext. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b14e4b1058914bb6af96af92e5aaa2a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/733k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa17edd88ac648e28f5ff6e4613634d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/6.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98a02cf47cbb41e89208ced3158c3030",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation-00000-of-00001.parquet:   0%|          | 0.00/657k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57fd6988bcb1467dac5a2328d34ae328",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/4358 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d383b2dc87da4e6d9ad5654ca7146498",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/36718 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5784c98815d94bab9a3fddae32945c18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/3760 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating...: 100%|██████████| 10/10 [00:01<00:00,  7.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "model perplexity: 28.67\n",
      "model size: 480.08 MiB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(28.6723, device='cuda:0')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = \"facebook/opt-125m\"\n",
    "llm_model = LLMModel(model_path)\n",
    "\n",
    "# Evaluate the model\n",
    "llm_model.model_evaluate(data_width=32, group_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7e1c7d",
   "metadata": {
    "id": "2e7e1c7d"
   },
   "source": [
    "# 4.1. Weight-only Quantization (AWQ)\n",
    "\n",
    "AWQ (activation aware weight only quantization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61fe61f6",
   "metadata": {
    "id": "61fe61f6"
   },
   "source": [
    "대형 언어 모델(LLM)은 다양한 작업에서 뛰어난 성능을 보여주고 있지만, 엄청난 모델 크기로 인해 하드웨어적 장벽(메모리 크기)이 높아지고, 토큰 생성 속도가 느려집니다(메모리 대역폭). LLM의 크기와 계산량은 기하급수적으로 증가하고 있는 반면, 메모리 대역폭은 느리게 증가하고 있습니다. 이 격차는 LLM 성능에서 중요한 병목 현상입니다. 이번 실습에서는 **새로운 양자화 알고리즘(AWQ)**을 사용하여 LLM의 메모리 사용량을 줄이고 추론 속도를 가속화하는 방법을 탐구할 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9be846",
   "metadata": {
    "id": "2a9be846"
   },
   "source": [
    "## AWQ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "VB2bNIrKySLZ",
   "metadata": {
    "id": "VB2bNIrKySLZ"
   },
   "source": [
    "Uniform quantization 은 실수 값을 range $[\\beta, \\alpha]$에서 $[0, 2^{b} - 1]$로 매핑하는 것입니다.\n",
    "\n",
    "Notation:\n",
    "\n",
    "- Quantized Weight: $w_q$\n",
    "\n",
    "- Scale factor: $s_q$\n",
    "\n",
    "- Zero Point: $z$\n",
    "\\begin{equation}\n",
    "s_q = \\frac{\\alpha - \\beta}{2^{b} - 1} \\tag{1},\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "z = -\\text{Round}(\\beta * scale) \\tag{2}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "w_q = \\text{Clamp}(\\text{Round}(\\frac{w}{s_q}) + z) \\tag{3},\n",
    "\\end{equation}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "au0OJ3lV_irH",
   "metadata": {
    "id": "au0OJ3lV_irH"
   },
   "source": [
    "## Pseudo Quantization\n",
    "아래 코드는 의사 양자화(pseudo quantization)을 위한 것입니다.\n",
    "\n",
    "\n",
    "Pseudo Quantization는 모델의 가중치를 실제로 양자화하지 않고, 양자화의 영향을 시뮬레이션하기 위해 사용됩니다. (즉, 가장 가까운 양자화된 값으로 반올림한 다음, **다시 부동 소수점으로 복원(dequantizing)하는** 것입니다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d63486a4",
   "metadata": {
    "id": "d63486a4"
   },
   "outputs": [],
   "source": [
    "# core quantization method (simulated quantization)\n",
    "def pseudo_quantize_tensor(w, n_bit=4, q_group_size=-1):\n",
    "    org_w_shape = w.shape\n",
    "    if q_group_size > 0:\n",
    "        assert org_w_shape[-1] % q_group_size == 0\n",
    "        w = w.reshape(-1, q_group_size)\n",
    "\n",
    "    assert w.dim() == 2\n",
    "\n",
    "    # Calculate the maximum (\\alpha) and minimum values (\\beta) in the tensor.\n",
    "    max_val = w.amax(dim=1, keepdim=True)\n",
    "    assert max_val.dim() == 2 and max_val.size(0) == w.size(0) and max_val.size(1) == 1\n",
    "    min_val = w.amin(dim=1, keepdim=True)\n",
    "    assert min_val.dim() == 2 and min_val.size(0) == w.size(0) and min_val.size(1) == 1\n",
    "\n",
    "    # Calculate the scale factor and zero point.  (Formula 1 & 2)\n",
    "    max_int = 2 ** n_bit - 1\n",
    "    scales = (max_val - min_val).clamp(min=1e-5) / max_int\n",
    "    assert scales.shape == max_val.shape\n",
    "    zeros = (-torch.round(min_val / scales)).clamp_(0, max_int)\n",
    "    assert scales.shape == min_val.shape\n",
    "\n",
    "    assert torch.isnan(scales).sum() == 0\n",
    "    assert torch.isnan(w).sum() == 0\n",
    "\n",
    "    # Quantize W: Map values in the range [\\beta, \\alpha] to lie within [0, 2^b - 1] (Formula 3)\n",
    "    w = torch.clamp(torch.round(w / scales) + zeros, 0, max_int)\n",
    "    assert w.dim() == 2 and w.size(0) == scales.size(0) and w.size(1) == q_group_size\n",
    "\n",
    "    # Dequantize W (pseudo quantization, the inverse transformation of Formula 3)\n",
    "    w = (w - zeros) * scales\n",
    "    assert w.dim() == 2 and w.size(0) == scales.size(0) and w.size(1) == q_group_size\n",
    "\n",
    "    assert torch.isnan(w).sum() == 0\n",
    "\n",
    "    w = w.reshape(org_w_shape)\n",
    "    return w\n",
    "\n",
    "@torch.no_grad()\n",
    "def pseudo_quantize_model_weight(\n",
    "    model, w_bit, q_group_size,\n",
    "):\n",
    "    for n, m in model.named_modules():\n",
    "        if isinstance(m, nn.Linear):\n",
    "            m.weight.data = pseudo_quantize_tensor(m.weight.data, n_bit=w_bit, q_group_size=q_group_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hU8nK-JY0iKA",
   "metadata": {
    "id": "hU8nK-JY0iKA"
   },
   "source": [
    "이제 quantized 3-bit 모델의 혼란도(perplexity)와 크기를 평가해 봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc985610",
   "metadata": {
    "id": "fc985610"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating...: 100%|██████████| 10/10 [00:00<00:00, 24.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "model perplexity: 64.88\n",
      "model size: 47.12 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(64.8840, device='cuda:0')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_model.model_reset()\n",
    "pseudo_quantize_model_weight(llm_model.model, w_bit=3, q_group_size=128)\n",
    "llm_model.model_changed = True\n",
    "\n",
    "# Evaluate the model\n",
    "llm_model.model_evaluate(data_width=3, group_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "KrDoPrp7Ncma",
   "metadata": {
    "id": "KrDoPrp7Ncma"
   },
   "source": [
    "모델 크기가 줄어든 것은 확인할 수 있지만, 혼란도(perplexity)는 상당히 증가했습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-BAC3SPY0swu",
   "metadata": {
    "id": "-BAC3SPY0swu"
   },
   "source": [
    "논문에서의 관찰에 따르면 LLM의 활성화(activations)에서 일부 채널에 **아웃라이어(outliers)**가 소량 발생하고 있습니다. 특정 채널에 아웃라이어가 있는 경우, 이는 **모든 토큰에서 지속적으로 나타납니다.**\n",
    "\n",
    "주어진 토큰에 대한 채널 간의 분산(variance)은 크지만(일부 채널의 활성화는 매우 크고, 대부분은 작습니다), 특정 채널의 크기(magnitude)가 토큰 간에 가지는 분산은 작습니다(아웃라이어 채널은 지속적으로 큽니다).\n",
    "\n",
    "\n",
    "AWQ(Activation Aware Weight Quantization)의 기법에 따르면, 활성화(activation) 아웃라이어에 해당하는 가중치 채널은 더 두드러지며, 이러한 두드러진 가중치를 보존하는 것이 성능 향상으로 이어질 수 있습니다. 다음으로, 두드러진 가중치를 찾고 원래 값으로 유지하여 혼란도(perplexity)의 변화를 관찰해 보겠습니다.\n",
    "\n",
    "아래 코드는 calibration 데이터셋을 로드하여 활성화 아웃라이어를 얻고 두드러진 가중치를 식별하는 데 사용됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb52620d",
   "metadata": {
    "id": "fb52620d"
   },
   "outputs": [],
   "source": [
    "def get_calib_dataset(tokenizer=None, n_samples=256, block_size=512):\n",
    "    dataset = load_dataset(\"mit-han-lab/pile-val-backup\", split=\"validation\")\n",
    "    dataset = dataset.shuffle(seed=42)\n",
    "    samples = []\n",
    "    n_run = 0\n",
    "    for data in dataset:\n",
    "        line = data[\"text\"]\n",
    "        line = line.strip()\n",
    "        line_encoded = tokenizer.encode(line)\n",
    "        if len(line_encoded) > block_size:\n",
    "            continue\n",
    "        sample = torch.tensor([line_encoded])\n",
    "        if sample.numel() == 0:\n",
    "            continue\n",
    "        samples.append(sample)\n",
    "        n_run += 1\n",
    "        if n_run == n_samples:\n",
    "            break\n",
    "\n",
    "    # now concatenate all samples and split according to block size\n",
    "    cat_samples = torch.cat(samples, dim=1)\n",
    "    n_split = cat_samples.shape[1] // block_size\n",
    "    print(f\" * Split into {n_split} blocks\")\n",
    "    return [cat_samples[:, i*block_size:(i+1)*block_size] for i in range(n_split)]\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_calib_feat(model, tokenizer):\n",
    "    input_dict = dict()\n",
    "    def stat_input_max_hook(m, x, y, name):\n",
    "        if isinstance(x, tuple):\n",
    "            x = x[0]\n",
    "        x_max = x.view(-1, x.shape[-1]).abs().mean(dim=0).cpu().detach()\n",
    "        if name not in input_dict:\n",
    "            input_dict[name] = [x_max]\n",
    "        else:\n",
    "            input_dict[name] += [x_max]\n",
    "\n",
    "    hooks = []\n",
    "    for name, m in model.named_modules():\n",
    "        if isinstance(m, nn.Linear):\n",
    "            hooks.append(\n",
    "                m.register_forward_hook(\n",
    "                    partial(stat_input_max_hook, name=name)))\n",
    "\n",
    "    print(\"Collecting activation scales...\")\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    samples = get_calib_dataset(tokenizer)\n",
    "    pbar = tqdm(samples)\n",
    "    for input_ids in pbar:\n",
    "        input_ids = input_ids.to(device)\n",
    "        model(input_ids)\n",
    "\n",
    "    for hook in hooks:\n",
    "        hook.remove()\n",
    "    return input_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bfd04606",
   "metadata": {
    "id": "bfd04606"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HTTP Error 429 thrown while requesting HEAD https://huggingface.co/facebook/opt-125m/resolve/main/config.json\n",
      "Retrying in 1s [Retry 1/5].\n",
      "HTTP Error 429 thrown while requesting HEAD https://huggingface.co/facebook/opt-125m/resolve/main/config.json\n",
      "Retrying in 2s [Retry 2/5].\n",
      "HTTP Error 429 thrown while requesting HEAD https://huggingface.co/facebook/opt-125m/resolve/main/config.json\n",
      "Retrying in 4s [Retry 3/5].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting activation scales...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "416fd1fc46ea43caa2ee25de8afc4025",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/167 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\miniconda3\\envs\\env_aias_test\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in D:\\data\\hf_cache\\hub\\datasets--mit-han-lab--pile-val-backup. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee67b0fc4d3447a1a88aef1b94e8a677",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "val.jsonl.zst:   0%|          | 0.00/471M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e6a4730f3fb4f48805678568a5e6e07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/214670 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Split into 127 blocks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 127/127 [00:04<00:00, 26.78it/s]\n"
     ]
    }
   ],
   "source": [
    "llm_model.model_reset()\n",
    "input_feat = get_calib_feat(llm_model.model, llm_model.tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "X_pPpL0FGful",
   "metadata": {
    "id": "X_pPpL0FGful"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "127\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n"
     ]
    }
   ],
   "source": [
    "print(type(input_feat['model.decoder.layers.0.self_attn.q_proj']))\n",
    "print(len(input_feat['model.decoder.layers.0.self_attn.q_proj']))\n",
    "print(input_feat['model.decoder.layers.0.self_attn.q_proj'][0].shape)\n",
    "print(sum(input_feat['model.decoder.layers.0.self_attn.q_proj']).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "h5GGLwczKghz",
   "metadata": {
    "id": "h5GGLwczKghz"
   },
   "source": [
    "# [실습 1] Scale 1% salient channels\n",
    "\n",
    "1%의 가중치를 FP16으로 유지하면 모델 크기(총 비트 수로 측정)를 크게 늘리지 않고도 양자화 성능을 향상시킬 수 있지만, 이러한 혼합 정밀도 데이터 유형은 시스템 구현을 어렵게 만듭니다.\n",
    "\n",
    "따라서 중요한 가중치를 실제로 FP16으로 유지하지 않고 중요한 가중치를 보호할 수 있는 방법을 찾아야 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "srM0CaQw3xyc",
   "metadata": {
    "id": "srM0CaQw3xyc"
   },
   "source": [
    "AWQ의 방법론에 따르면, 중요한 가중치 채널을 단순히 스케일링하여(특정한 값을 곱해 주어) 보호할 수 있습니다. 원리는 다음과 같습니다:\n",
    "\n",
    "- Linear layer channel $\\mathbf{y} = \\mathbf{w}x$ (from $\\mathbf{W}x$)일 때, 우리가 주목해야 할 것은 양자화 함수 $Q(\\mathbf{w})x$으로 발생하는 quantization error입니다.\n",
    "\n",
    "- Quantization function $Q(\\mathbf{w})$ = $Δ\\cdot Round(\\frac{\\mathbf{w}}{Δ})$, $Δ = \\frac{\\max(|w|)}{2^{N - 1}}$.\n",
    "\n",
    "- Quantization error $Err(Q(\\mathbf{w}) x) = Δ\\cdot RoundErr(\\frac{\\mathbf{w}}{Δ})\\cdot x$\n",
    "- 스케일링 된  Quantization error $Err(Q(\\mathbf{w} \\cdot s)(\\frac{x}{s})) = Δ\\cdot RoundErr(\\frac{\\mathbf{w}}{Δ})\\cdot x\\cdot \\mathbf{\\frac{1}{s}}$.\n",
    "- $RoundErr$ 는 언제나 ~0.25 입니다 (0-0.5 사이의 평균이므로).\n",
    "- 그룹의 크기가 충분히 클 때(e.g., 128), 하나의 채널을 스케일링하는 것은 일반적으로 그룹 내 최대 값을 증가시키지 않습니다 (즉, $Δ$ 는 변하지 않습니다).\n",
    "- 그러므로, $Err(Q(\\mathbf{w} \\cdot s)(\\frac{x}{s})) = Δ\\cdot RoundErr(\\frac{\\mathbf{w}}{Δ})\\cdot x\\cdot \\mathbf{\\frac{1}{s}}$ < $Δ\\cdot RoundErr(\\frac{\\mathbf{w}}{Δ})\\cdot x = Err(Q(\\mathbf{w}) x)$.\n",
    "\n",
    "아래 코드를 완성하여 중요한 가중치 채널을 스케일링하고, 양자화 한 다음, 다시 스케일을 줄인 후 혼란도(perplexity)의 변화를 관찰해보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "TdO4KtnsNKf5",
   "metadata": {
    "id": "TdO4KtnsNKf5"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def pseudo_quantize_model_weight_scaleup(\n",
    "    model, w_bit, q_group_size, input_feat, scale_factor\n",
    "):\n",
    "    for n, m in model.named_modules():\n",
    "        if isinstance(m, nn.Linear):\n",
    "            importance = sum(input_feat[n]).float()\n",
    "            # 1퍼센트 채널의 개수\n",
    "            num_samples = int(len(importance) * 0.01)\n",
    "\n",
    "            ############### YOUR CODE STARTS HERE ###############\n",
    "\n",
    "            # Step 1: importance를 기준으로 1%의 중요한 채널을 찾으세요  (hint: use torch.topk())\n",
    "            # hint : torch.topk() 함수를 사용하세요. torch.topk() 함수는 PyTorch에서 텐서의 값 중 상위 k개의 값과 그들의 인덱스를 반환하는 함수입니다. torch.topk()[0]는 값을, torch.topk()[1]은 인덱스를 반환합니다.\n",
    "            outlier_mask = torch.topk(importance, k=num_samples)[1]\n",
    "\n",
    "            ############### YOUR CODE ENDS HERE #################\n",
    "            assert outlier_mask.dim() == 1\n",
    "\n",
    "            # 스케일 팩터를 적용하는 것을 시뮬레이션하기 위해, 양자화 전에 스케일 팩터를 곱하고, 양자화 후에 스케일 팩터로 나눕니다.\n",
    "            # scale_factor를 이용해 중요한 가중치 채널의 값을 확대합니다.\n",
    "            m.weight.data[:, outlier_mask] *= scale_factor\n",
    "\n",
    "            m.weight.data = pseudo_quantize_tensor(m.weight.data, n_bit=w_bit, q_group_size=q_group_size)\n",
    "\n",
    "            ############### YOUR CODE STARTS HERE ###############\n",
    "\n",
    "            # Step 2: scale_factor를 이용해 중요한 가중치 채널의 값을 다시 축소하세요.\n",
    "            m.weight.data[:, outlier_mask] /= scale_factor\n",
    "\n",
    "            ############### YOUR CODE ENDS HERE #################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "GoTh5CzuPhtV",
   "metadata": {
    "id": "GoTh5CzuPhtV"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating...: 100%|██████████| 10/10 [00:00<00:00, 25.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "model perplexity: 53.28\n",
      "model size: 47.12 MiB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(53.2786, device='cuda:0')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_model.model_reset()\n",
    "pseudo_quantize_model_weight_scaleup(llm_model.model, w_bit=3, q_group_size=128, input_feat=input_feat, scale_factor=2)\n",
    "llm_model.model_changed = True\n",
    "\n",
    "# Evaluate the model\n",
    "llm_model.model_evaluate(data_width=3, group_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc63e22",
   "metadata": {
    "id": "ffc63e22"
   },
   "source": [
    "스케일링을 통해서 중요한 가중치를 보호함과 동시에, 모든 가중치를 3bit로 유지할 수 있었습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b245f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_path = \"facebook/opt-125m\"\n",
    "# llm_model = LLMModel(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4SBUNQc_MDcP",
   "metadata": {
    "id": "4SBUNQc_MDcP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scale_factor=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating...: 100%|██████████| 10/10 [00:00<00:00, 23.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "model perplexity: 64.88\n",
      "model size: 47.12 MiB\n",
      "scale_factor=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating...: 100%|██████████| 10/10 [00:00<00:00, 21.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "model perplexity: 53.28\n",
      "model size: 47.12 MiB\n",
      "scale_factor=3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating...: 100%|██████████| 10/10 [00:00<00:00, 20.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "model perplexity: 69.95\n",
      "model size: 47.12 MiB\n",
      "scale_factor=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating...: 100%|██████████| 10/10 [00:00<00:00, 21.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "model perplexity: 154.36\n",
      "model size: 47.12 MiB\n"
     ]
    }
   ],
   "source": [
    "for scale_factor in [1,2,3,4]:\n",
    "    llm_model.model_reset()\n",
    "    pseudo_quantize_model_weight_scaleup(llm_model.model, w_bit=3, q_group_size=128, input_feat=input_feat, scale_factor=scale_factor)\n",
    "    llm_model.model_changed = True\n",
    "\n",
    "    # Evaluate the model\n",
    "    print(f\"scale_factor={scale_factor}\")\n",
    "    llm_model.model_evaluate(data_width=3, group_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kDIYt2xJEt84",
   "metadata": {
    "id": "kDIYt2xJEt84"
   },
   "source": [
    "코드에서 서로 다른 스케일링 팩터 $s$(예: 1, 2, 3, 4)를 시도하고 혼란도(perplexity)의 변화를 관찰해보세요.\n",
    "\n",
    "혼란도(perplexity)가 먼저 감소하다가 다시 증가하는 것을 관찰했나요?\n",
    "\n",
    "너무 큰 팩터로 스케일링하면 그룹 내 최대 값이 증가할 수 있습니다(즉,$Δ$가 증가함).\n",
    "\n",
    "이는 다른 채널의 양자화에 영향을 미칠 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ueFx8RCHF2zu",
   "metadata": {
    "id": "ueFx8RCHF2zu"
   },
   "source": [
    "# [실습 2] Scale factor search\n",
    "\n",
    "지금까지 우리는 스케일링 팩터$s$를 직접 정의해 주었습니다.\n",
    "\n",
    "그러나 Fine-tuning의 불안정성 때문에, 미리 정의된 검색 공간 내에서 최적의\n",
    "$s$를 찾는 것이 더 나은 선택이 될 것입니다. 우리는 중요한 가중치를 보호하면서 다른 값을 고려하기 위해 검색 공간 내에서 최적의 스케일을 찾을 수 있습니다.\n",
    "\n",
    "실제로, 논문에서는 활성화만 고려하는 것으로도 좋은 결과를 얻을 수 있음을 관찰할 수 있습니다.\n",
    "\n",
    "우리는 스케일링 팩터 $s$를 활성화의 L1-norm (즉, acviation matrix의 절댓값들의 평균)의 $\\alpha$제곱으로 설정할 것입니다.\n",
    "\n",
    "$\\alpha$의 값은 grid search를 통해 적절한 값으로 검색합니다.\n",
    "\n",
    "검색을 위한 코드를 추가하고 실행하여 혼란도(perplexity)를 관찰하세요."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bA4b4L2OL05N",
   "metadata": {
    "id": "bA4b4L2OL05N"
   },
   "source": [
    "$$\n",
    "𝐋(\\mathbf{s})=\\lVert Q(\\mathbf{W}\\cdot \\mathbf{s})  (\\mathbf{s^{-1}} \\cdot \\mathbf{X}) - \\mathbf{W}\\mathbf{X}  \\rVert,  \\quad\\mathbf{s}= \\mathbf{s_X}^{\\alpha},  \\mathbf{s_X} = \\|X\\|_1\n",
    "$$\n",
    "$$\n",
    "\\mathbf{s}^* = \\text{argmin}_{\\mathbf{s}} 𝐋(\\mathbf{s}),\\quad \\alpha^*=\\text{argmin}_{\\alpha} 𝐋(\\mathbf{s_X}^{\\alpha})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ff_sv6k0R2Eb",
   "metadata": {
    "id": "ff_sv6k0R2Eb"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def scale_ln_fcs(ln, fcs, scales):\n",
    "    if not isinstance(fcs, list):\n",
    "        fcs = [fcs]\n",
    "\n",
    "    scales = scales.to(ln.weight.device)\n",
    "\n",
    "    ln.weight.div_(scales)\n",
    "    if hasattr(ln, 'bias') and ln.bias is not None:\n",
    "        ln.bias.div_(scales)\n",
    "\n",
    "    for fc in fcs:\n",
    "        fc.weight.mul_(scales.view(1, -1))\n",
    "\n",
    "    for p in ln.parameters():\n",
    "        assert torch.isnan(p).sum() == 0\n",
    "    for fc in fcs:\n",
    "        for p in fc.parameters():\n",
    "            assert torch.isnan(p).sum() == 0\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def scale_fc_fc(fc1, fc2, scales):\n",
    "    assert isinstance(fc1, nn.Linear)\n",
    "    assert isinstance(fc2, nn.Linear)\n",
    "\n",
    "    scales = scales.to(fc1.weight.device)\n",
    "\n",
    "    # fc1.weight.div_(scales.view(-1, 1))\n",
    "    fc1.weight[-scales.size(0):].div_(scales.view(-1, 1))\n",
    "    if fc1.bias is not None:\n",
    "        fc1.bias.div_(scales.view(-1))\n",
    "\n",
    "    fc2.weight.mul_(scales.view(1, -1))\n",
    "\n",
    "    for p in fc1.parameters():\n",
    "        assert torch.isnan(p).sum() == 0\n",
    "    for p in fc2.parameters():\n",
    "        assert torch.isnan(p).sum() == 0\n",
    "\n",
    "@torch.no_grad()\n",
    "def auto_scale_block(module, name, w_bit,\n",
    "                     q_group_size,\n",
    "                     input_feat):\n",
    "\n",
    "    # find the best scale ratio\n",
    "    def _search_module_scale(block, linears2scale: list, x, kwargs={}):\n",
    "\n",
    "        x = x.to(next(block.parameters()).device)\n",
    "        with torch.no_grad():\n",
    "            org_out = block(x, **kwargs)\n",
    "            if isinstance(org_out, tuple):\n",
    "                org_out = org_out[0]\n",
    "\n",
    "        s_x = x.view(-1, x.shape[-1]).abs().mean(0)\n",
    "        s_x = torch.clamp(s_x, 1e-5)\n",
    "\n",
    "\n",
    "        # Step 1: best_error, best_ratio, 및 best_scales를 초기화\n",
    "        best_error = torch.inf\n",
    "        best_ratio = -1\n",
    "        best_scales = 0\n",
    "\n",
    "\n",
    "        n_grid = 20\n",
    "        history = []\n",
    "\n",
    "        org_sd = {k: v.cpu() for k, v in block.state_dict().items()}\n",
    "        for ratio in range(n_grid):\n",
    "            # ratio is the \\alpha in the formula\n",
    "            ratio = ratio * 1 / n_grid\n",
    "\n",
    "            ############### YOUR CODE STARTS HERE ###############\n",
    "\n",
    "            # Step 2: 공식에 따라 스케일 계산\n",
    "            scales = s_x ** ratio\n",
    "            # scales = --- IGNORE ---\n",
    "\n",
    "            ############### YOUR CODE ENDS HERE #################\n",
    "            assert scales.shape == s_x.shape\n",
    "\n",
    "            scales = scales / (scales.max() * scales.min()).sqrt().view(1, -1)\n",
    "\n",
    "            for fc in linears2scale:\n",
    "\n",
    "                scales = scales.to(fc.weight.device)\n",
    "\n",
    "                # scale_factor를 이용해 중요한 가중치 채널의 값을 확대합니다.\n",
    "                fc.weight.mul_(scales)\n",
    "\n",
    "                fc.weight.data = pseudo_quantize_tensor(fc.weight.data, w_bit, q_group_size)\n",
    "\n",
    "                ############### YOUR CODE STARTS HERE ###############\n",
    "\n",
    "                # Step 3: scale_factor를 이용해 중요한 가중치 채널의 값을 다시 축소하세요.\n",
    "                fc.weight.data\n",
    "\n",
    "                ############### YOUR CODE ENDS HERE #################\n",
    "\n",
    "            out = block(x, **kwargs)\n",
    "            if isinstance(out, tuple):\n",
    "                out = out[0]\n",
    "\n",
    "            loss = (org_out - out).float().pow(2).mean().item()  # float prevents overflow\n",
    "            history.append(loss)\n",
    "            is_best = loss < best_error\n",
    "            if is_best:\n",
    "                best_error = loss\n",
    "                best_ratio = ratio\n",
    "                best_scales = scales\n",
    "            block.load_state_dict(org_sd)\n",
    "\n",
    "        if best_ratio == -1:\n",
    "            print(history)\n",
    "            raise Exception\n",
    "\n",
    "        best_scales = best_scales.view(-1)\n",
    "\n",
    "        assert torch.isnan(best_scales).sum() == 0, best_scales\n",
    "        return best_scales.detach()\n",
    "\n",
    "    # attention input\n",
    "    inp = input_feat[name + '.self_attn.out_proj']\n",
    "    inp = torch.cat([x.unsqueeze(0) for x in inp], dim=0).unsqueeze(0)\n",
    "    qkv = [module.self_attn.q_proj, module.self_attn.k_proj, module.self_attn.v_proj]\n",
    "    final_scales = _search_module_scale(module.self_attn, qkv, inp)\n",
    "    scale_ln_fcs(module.self_attn_layer_norm, qkv, final_scales)\n",
    "\n",
    "    # attn out\n",
    "    inp = input_feat[name + '.self_attn.out_proj']\n",
    "    inp = torch.cat([x.unsqueeze(0) for x in inp], dim=0)\n",
    "    final_scales = _search_module_scale(module.self_attn.out_proj, [module.self_attn.out_proj], inp)\n",
    "    scale_fc_fc(module.self_attn.v_proj, module.self_attn.out_proj, final_scales)\n",
    "\n",
    "    # fc1\n",
    "    inp = input_feat[name + '.fc1']\n",
    "    inp = torch.cat([x.unsqueeze(0) for x in inp], dim=0)\n",
    "    final_scales = _search_module_scale(module.fc1, [module.fc1], inp)\n",
    "    scale_ln_fcs(module.final_layer_norm, module.fc1, final_scales)\n",
    "\n",
    "    # fc2\n",
    "    inp = input_feat[name + '.fc2']\n",
    "    inp = torch.cat([x.unsqueeze(0) for x in inp], dim=0)\n",
    "    final_scales = _search_module_scale(module.fc2, [module.fc2], inp)\n",
    "    scale_fc_fc(module.fc1, module.fc2, final_scales)\n",
    "\n",
    "@torch.no_grad()\n",
    "def pseudo_quantize_model_weight_auto_scale(\n",
    "    model, w_bit, q_group_size, input_feat\n",
    "):\n",
    "    from transformers.models.opt.modeling_opt import OPTDecoderLayer\n",
    "\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, OPTDecoderLayer):\n",
    "            auto_scale_block(module, name, w_bit, q_group_size, input_feat)\n",
    "\n",
    "    for n, m in model.named_modules():\n",
    "        if isinstance(m, nn.Linear):\n",
    "            m.weight.data = pseudo_quantize_tensor(m.weight.data, n_bit=w_bit, q_group_size=q_group_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cXuQUykZMdKa",
   "metadata": {
    "id": "cXuQUykZMdKa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating...: 100%|██████████| 10/10 [00:00<00:00, 25.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "model perplexity: 61.74\n",
      "model size: 47.12 MiB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(61.7358, device='cuda:0')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_model.model_reset()\n",
    "pseudo_quantize_model_weight_auto_scale(llm_model.model, w_bit=3, q_group_size=128, input_feat=input_feat)\n",
    "llm_model.model_changed = True\n",
    "\n",
    "# Evaluate and delete the model\n",
    "llm_model.model_evaluate(data_width=3, group_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8edb0c1",
   "metadata": {
    "id": "c8edb0c1"
   },
   "source": [
    "# 4.2. Weight and Activation Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ae31c3",
   "metadata": {
    "id": "60ae31c3"
   },
   "source": [
    "대형 언어 모델(LLM)은 다양한 작업에서 뛰어난 성능을 보여주고 있지만, 엄청난 모델 크기로 인해 하드웨어적 장벽(메모리 크기)이 높아지고, 토큰 생성 속도가 느려집니다(메모리 대역폭). LLM의 크기와 계산량은 기하급수적으로 증가하고 있는 반면, 메모리 대역폭은 느리게 증가하고 있습니다. 이 격차는 LLM 성능에서 중요한 병목 현상입니다. 이번 실습에서는 **새로운 양자화 알고리즘(AWQ)**을 사용하여 LLM의 메모리 사용량을 줄이고 추론 속도를 가속화하는 방법을 탐구할 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235839a0",
   "metadata": {
    "id": "235839a0"
   },
   "source": [
    "이전 수업에서는 양자화(Quantization)의 기본 방법들을 배웠습니다.\n",
    "\n",
    "양자화에는 두 가지 유형이 있습니다:\n",
    "\n",
    "- 가중치(weight)와 활성화(activation) 모두 양자화\n",
    "    - 계산 한계가 있는 시나리오에서 더 유리합니다: 예를 들어 컨텍스트 단계나 대규모 배치 추론\n",
    "    - 예시: SmoothQuant(W8A8 quantization)\n",
    "- 가중치(weight)만 양자화\n",
    "    - 메모리 한계가 있는 시나리오에서 더 유리합니다: 예를 들어 디코딩 단계나 단일 배치 추론.\n",
    "    - 예시: AWQ(W4A16 quantization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be754200",
   "metadata": {
    "id": "be754200"
   },
   "source": [
    "이 노트북에서는 OPT-125m 모델을 사용하여 SmoothQuant가 가중치와 활성화 모두에 8비트를 사용하여 FP16 모델과 동일한 정확도를 달성할 수 있음을 보여줍니다. SmoothQuant는 Linear layer에서 완전한 INT8 GEMM을 가능하게 하고, 이상값을 나타내기 위해 고정밀도 숫자를 요구하지 않습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cce38d88",
   "metadata": {
    "id": "cce38d88"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from transformers.models.opt.modeling_opt import OPTDecoderLayer\n",
    "from transformers.models.bloom.modeling_bloom import BloomBlock\n",
    "from transformers.models.llama.modeling_llama import LlamaDecoderLayer, LlamaRMSNorm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f061090a",
   "metadata": {
    "id": "f061090a"
   },
   "source": [
    "Uniform quantization 은 실수 값을 range$[\\beta, \\alpha]$에서 $[0, 2^{b} - 1]$로 매핑하는 것입니다.\n",
    "\n",
    "Notation:\n",
    "\n",
    "- Quantized Weight: $w_q$\n",
    "\n",
    "- Scale factor: $s_q$\n",
    "\n",
    "- Zero Point: $z$\n",
    "\\begin{equation}\n",
    "s_q = \\frac{\\alpha - \\beta}{2^{b} - 1} \\tag{1},\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "z = -\\text{Round}(\\beta * scale) \\tag{2}\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "w_q = \\text{Clamp}(\\text{Round}(\\frac{w}{s_q}) + z) \\tag{3},\n",
    "\\end{equation}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8dafc5",
   "metadata": {
    "id": "fd8dafc5"
   },
   "source": [
    "## Pseudo Quantization\n",
    "아래 코드는 의사 양자화(pseudo quantization)을 위한 클래스입니다.\n",
    "\n",
    "Pseudo Quantization는 모델의 weight와 activation을 실제로 양자화하지 않고, 양자화의 영향을 시뮬레이션하기 위해 사용됩니다. (즉, 가장 가까운 양자화된 값으로 반올림한 다음, **다시 부동 소수점으로 복원(dequantizing)**하는 것입니다.)\n",
    "\n",
    "이 노트북에서는 실제 연산에서는 FP16을 사용하여 8비트 dynamic weight and activation qaunitzation을 시뮬레이션 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "af56c647",
   "metadata": {
    "id": "af56c647"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from functools import partial\n",
    "\n",
    "class W8A8Linear(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features,\n",
    "        out_features,\n",
    "        bias=True,\n",
    "        act_quant=\"per_token\",\n",
    "        quantize_output=False,\n",
    "        quantize_bits=8\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "\n",
    "        self.register_buffer(\n",
    "            \"weight\",\n",
    "            torch.randn(\n",
    "                self.out_features,\n",
    "                self.in_features,\n",
    "                dtype=torch.float16,\n",
    "                requires_grad=False,\n",
    "            ),\n",
    "        )\n",
    "        if bias:\n",
    "            self.register_buffer(\n",
    "                \"bias\",\n",
    "                torch.zeros(\n",
    "                    (1, self.out_features), dtype=torch.float16, requires_grad=False\n",
    "                ),\n",
    "            )\n",
    "        else:\n",
    "            self.register_buffer(\"bias\", None)\n",
    "\n",
    "        if act_quant == \"per_token\":\n",
    "            self.act_quant_name = \"per_token\"\n",
    "            self.act_quant = partial(quantize_activation_per_token_absmax, n_bits=8)\n",
    "        elif act_quant == \"per_tensor\":\n",
    "            self.act_quant_name = \"per_tensor\"\n",
    "            self.act_quant = partial(quantize_activation_per_tensor_absmax, n_bits=8)\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid act_quant: {act_quant}\")\n",
    "\n",
    "        if quantize_output:\n",
    "            self.output_quant_name = self.act_quant_name\n",
    "            self.output_quant = self.act_quant\n",
    "        else:\n",
    "            self.output_quant_name = \"None\"\n",
    "            self.output_quant = lambda x: x\n",
    "\n",
    "        self.quantize_bits = quantize_bits\n",
    "\n",
    "    def to(self, *args, **kwargs):\n",
    "        super(W8A8Linear, self).to(*args, **kwargs)\n",
    "        self.weight = self.weight.to(*args, **kwargs)\n",
    "        if self.bias is not None:\n",
    "            self.bias = self.bias.to(*args, **kwargs)\n",
    "        return self\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, x):\n",
    "        q_x = self.act_quant(x)\n",
    "        y = torch.functional.F.linear(q_x, self.weight, self.bias)\n",
    "        q_y = self.output_quant(y)\n",
    "        return q_y\n",
    "\n",
    "    @staticmethod\n",
    "    def from_float(\n",
    "        module, weight_quant=\"per_channel\", act_quant=\"per_token\", quantize_output=False, quantize_bits=8\n",
    "    ):\n",
    "        assert isinstance(module, torch.nn.Linear)\n",
    "        new_module = W8A8Linear(\n",
    "            module.in_features,\n",
    "            module.out_features,\n",
    "            module.bias is not None,\n",
    "            act_quant=act_quant,\n",
    "            quantize_output=quantize_output,\n",
    "        )\n",
    "        if weight_quant == \"per_channel\":\n",
    "            new_module.weight = quantize_weight_per_channel_absmax(\n",
    "                module.weight, n_bits=new_module.quantize_bits\n",
    "            )  # use 8-bit integer for weight\n",
    "        elif weight_quant == \"per_tensor\":\n",
    "            new_module.weight = quantize_weight_per_tensor_absmax(\n",
    "                module.weight, n_bits=new_module.quantize_bits\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid weight_quant: {weight_quant}\")\n",
    "        new_module.weight_quant_name = weight_quant\n",
    "        if module.bias is not None:\n",
    "            new_module.bias = module.bias\n",
    "        return new_module\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"W8A8Linear({self.in_features}, {self.out_features}, bias={self.bias is not None}, weight_quant={self.weight_quant_name}, act_quant={self.act_quant_name}, output_quant={self.output_quant_name})\"\n",
    "\n",
    "@torch.no_grad()\n",
    "def quantize_weight_per_channel_absmax(w, n_bits=8):\n",
    "    # w: (out_features, in_features)\n",
    "    scales = w.abs().max(dim=-1, keepdim=True)[0]\n",
    "    q_max = 2 ** (n_bits - 1) - 1\n",
    "    scales.clamp_(min=1e-5).div_(q_max)\n",
    "    w.div_(scales).round_().mul_(scales)\n",
    "    return w\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def quantize_weight_per_tensor_absmax(w, n_bits=8):\n",
    "    # w: (out_features, in_features)\n",
    "    scales = w.abs().max()\n",
    "    q_max = 2 ** (n_bits - 1) - 1\n",
    "    scales.clamp_(min=1e-5).div_(q_max)\n",
    "    w.div_(scales).round_().mul_(scales)\n",
    "    return w\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def quantize_activation_per_token_absmax(t, n_bits=8):\n",
    "    t_shape = t.shape\n",
    "    t.view(-1, t_shape[-1])\n",
    "    scales = t.abs().max(dim=-1, keepdim=True)[0]\n",
    "    q_max = 2 ** (n_bits - 1) - 1\n",
    "    scales.clamp_(min=1e-5).div_(q_max)\n",
    "    t.div_(scales).round_().mul_(scales)\n",
    "    return t\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def quantize_activation_per_tensor_absmax(t, n_bits=8):\n",
    "    t_shape = t.shape\n",
    "    t.view(-1, t_shape[-1])\n",
    "    scales = t.abs().max()\n",
    "    q_max = 2 ** (n_bits - 1) - 1\n",
    "    scales.clamp_(min=1e-5).div_(q_max)\n",
    "    t.div_(scales).round_().mul_(scales)\n",
    "    return t\n",
    "\n",
    "def quantize_opt(\n",
    "    model, weight_quant=\"per_tensor\", act_quant=\"per_tensor\", quantize_bmm_input=True, quantize_bits=8\n",
    "):\n",
    "    from transformers.models.opt.modeling_opt import (\n",
    "        OPTAttention,\n",
    "        OPTDecoderLayer,\n",
    "    )\n",
    "\n",
    "    for name, m in model.model.named_modules():\n",
    "        if isinstance(m, OPTDecoderLayer):\n",
    "            m.fc1 = W8A8Linear.from_float(\n",
    "                m.fc1, weight_quant=weight_quant, act_quant=act_quant, quantize_bits=8\n",
    "            )\n",
    "            m.fc2 = W8A8Linear.from_float(\n",
    "                m.fc2, weight_quant=weight_quant, act_quant=act_quant, quantize_bits=8\n",
    "            )\n",
    "        elif isinstance(m, OPTAttention):\n",
    "            # Her we simulate quantizing BMM inputs by quantizing the output of q_proj, k_proj, v_proj\n",
    "            m.q_proj = W8A8Linear.from_float(\n",
    "                m.q_proj,\n",
    "                weight_quant=weight_quant,\n",
    "                act_quant=act_quant,\n",
    "                quantize_output=quantize_bmm_input, quantize_bits=8\n",
    "            )\n",
    "            m.k_proj = W8A8Linear.from_float(\n",
    "                m.k_proj,\n",
    "                weight_quant=weight_quant,\n",
    "                act_quant=act_quant,\n",
    "                quantize_output=quantize_bmm_input, quantize_bits=8\n",
    "            )\n",
    "            m.v_proj = W8A8Linear.from_float(\n",
    "                m.v_proj,\n",
    "                weight_quant=weight_quant,\n",
    "                act_quant=act_quant,\n",
    "                quantize_output=quantize_bmm_input, quantize_bits=8\n",
    "            )\n",
    "            m.out_proj = W8A8Linear.from_float(\n",
    "                m.out_proj, weight_quant=weight_quant, act_quant=act_quant, quantize_bits=8\n",
    "            )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87c4b54",
   "metadata": {
    "id": "a87c4b54"
   },
   "source": [
    "이제 quantized 8-bit 모델의 혼란도(perplexity)와 크기를 평가해 봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "37ca07f9",
   "metadata": {
    "id": "37ca07f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPTForCausalLM(\n",
      "  (model): OPTModel(\n",
      "    (decoder): OPTDecoder(\n",
      "      (embed_tokens): Embedding(50272, 768, padding_idx=1)\n",
      "      (embed_positions): OPTLearnedPositionalEmbedding(2050, 768)\n",
      "      (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (layers): ModuleList(\n",
      "        (0-11): 12 x OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): W8A8Linear(768, 768, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (v_proj): W8A8Linear(768, 768, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (q_proj): W8A8Linear(768, 768, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (out_proj): W8A8Linear(768, 768, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): W8A8Linear(768, 3072, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (fc2): W8A8Linear(3072, 768, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50272, bias=False)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating...: 100%|██████████| 10/10 [00:01<00:00,  9.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "model perplexity: 31.64\n",
      "model size: 121.77 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(31.6428, device='cuda:0')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_model.model_reset()\n",
    "model_w8a8 = quantize_opt(llm_model.model, quantize_bits=8)\n",
    "print(model_w8a8)\n",
    "llm_model.model_change(model_w8a8)\n",
    "\n",
    "# Evaluate and delete the model\n",
    "llm_model.model_evaluate(data_width=8, group_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790d536c",
   "metadata": {
    "id": "790d536c"
   },
   "source": [
    "모델 크기가 줄어든 것은 확인할 수 있지만, 혼란도(perplexity)는 약간 증가했습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc12e9b",
   "metadata": {
    "id": "bcc12e9b"
   },
   "source": [
    "AWQ의 관찰에서와 마찬가지로, LLM의 활성화(activations)에서 일부 채널에 **아웃라이어(outliers)**가 소량 발생하고 있습니다. 특정 채널에 아웃라이어가 있는 경우, 이는 **모든 토큰에서 지속적으로 나타납니다.**\n",
    "\n",
    "주어진 토큰에 대한 채널 간의 분산(variance)은 크지만(일부 채널의 활성화는 매우 크고, 대부분은 작습니다), 특정 채널의 크기(magnitude)가 토큰 간에 가지는 분산은 작습니다(아웃라이어 채널은 지속적으로 큽니다).\n",
    "\n",
    "Smoothquant 논문의 관찰에 따르면, 이러한 현상은 activation에서만 발견되는 현상이며, weight에서는 발견되지 않습니다.\n",
    "\n",
    "그렇기 떄문에, AWQ 기법과 같이 weight에 대해서는 4bit 정도의 낮은 정밀도로 quantization이 가능하지만, activation에서는 매우 큰 정확도 하락이 발생합니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50a1ebc",
   "metadata": {
    "id": "c50a1ebc"
   },
   "source": [
    "## Migrate the quantization difficulty from activations to weights\n",
    "\n",
    "양자화 오류(quantization error)를 줄이기 위해서는 모든 채널에 대해 유효 양자화 비트수를 증가시켜야 합니다.\n",
    "\n",
    "그러나 연산 과정에서 activation은 채널 차원이 아닌 토큰 차원에서 행렬 곱셈이 이루어지기 때문에, per-channel quantization을 도입하는 것으로는 속도의 향상을 불러올 수 없습니다.\n",
    "\n",
    "대신 Smoothquant에서는 activation을 per-channel smoothing fator $\\mathbf{s}$로 나누어 \"smooth\"하는 방법을 제안합니다.\n",
    "\n",
    "이 방법은 각 activation 채널에 독립적인 스케일링 인자를 적용하여 입력 활성화의 이상치(outliers)를 평활화하고, 이로 인해 양자화 과정에서 발생할 수 있는 오류와 정확도 손실을 최소화합니다.\n",
    "\n",
    "즉, 각 활성화 채널의 스케일을 조정함으로써 전체 행렬의 양자화가 더욱 효과적이고 안정적으로 이루어질 수 있도록 합니다.\n",
    "\n",
    "$$\n",
    "Y = (X \\text{diag}(s)^{-1}) \\cdot (\\text{diag}(s)W) = \\hat{X}\\hat{W}\n",
    "$$\n",
    "\n",
    "\n",
    "여기서 입력 𝑋는 일반적으로 이전의 선형 연산(예: Linear layer, Layer Normalization 등)에서 생성되므로, 우리는 스케일링 팩터 $s$를 이전 레이어의 파라미터에 오프라인으로 미리 결합할 수 있습니다. 이렇게 하면 추가적인 스케일링으로 인한 커널 호출 오버헤드가 발생하지 않습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561dac5e",
   "metadata": {
    "id": "561dac5e"
   },
   "source": [
    "# [실습 3] Quantization difficulty migration\n",
    "\n",
    "실습을 통해 weight에는 s를 곱하고, activation에는 s를 나누어 quantization 난이도를 분배해 준 다음, quantization을 진행해 혼란도(perplexity)의 변화를 관찰해보세요.\n",
    "\n",
    "일반적인 Transformer의 레이어 구조는 다음과 같습니다:\n",
    "\n",
    "1. Self-Attention Block\n",
    "\n",
    "    Input → LayerNorm → Query/Key/Value 생성 (FC) → Attention 연산 → Softmax → Attention Output\n",
    "\n",
    "2. Feed-Forward Network (FFN)\n",
    "\n",
    "    Attention Output → LayerNorm → FC1 → 활성화 함수 (ReLU, GELU 등) → FC2 → Output\n",
    "\n",
    "그러므로 Transformer의 레이어에서 LayerNorm의 가중치에 스케일링 팩터 $s$를 나누고, FC의 가중치에 스케일링 팩터 $s$를 곱하면 weight에는 s를 곱하고, activation에는 s를 나누는 효과를 얻을 수 있습니다.\n",
    "\n",
    "아래 코드를 수정하여 LayerNorm의 가중치에 스케일링 팩터 $s$를 나누고, FC의 가중치에 스케일링 팩터 $s$를 곱해주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8d58d548",
   "metadata": {
    "id": "8d58d548"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def smooth_lm_by_scale(model, scale):\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, OPTDecoderLayer):\n",
    "            attn_ln = module.self_attn_layer_norm\n",
    "            qkv = [\n",
    "                module.self_attn.q_proj,\n",
    "                module.self_attn.k_proj,\n",
    "                module.self_attn.v_proj,\n",
    "            ]\n",
    "            smooth_ln_fcs_by_scale(attn_ln, qkv, scale)\n",
    "\n",
    "            ffn_ln = module.final_layer_norm\n",
    "            fc1 = module.fc1\n",
    "            smooth_ln_fcs_by_scale(ffn_ln, fc1, scale)\n",
    "\n",
    "@torch.no_grad()\n",
    "def smooth_ln_fcs_by_scale(ln, fcs, scale):\n",
    "    if not isinstance(fcs, list):\n",
    "        fcs = [fcs]\n",
    "    assert isinstance(ln, nn.LayerNorm)\n",
    "    for fc in fcs:\n",
    "        assert isinstance(fc, nn.Linear)\n",
    "    ############### YOUR CODE STARTS HERE ###############\n",
    "    # Step 1: layernorm의 weight와 bias를 scale로 나누어주세요. (hint: div_()함수를 통해 tensor 전체를 특정한 값으로 나누어 줄 수 있습니다.)\n",
    "    ln.weight.div_(scale)\n",
    "    ln.bias.div_(scale)\n",
    "    ############### YOUR CODE ENDS HERE #################\n",
    "\n",
    "    for fc in fcs:\n",
    "        ############### YOUR CODE STARTS HERE ###############\n",
    "        # Step 2: fc의 weight에 scale을 곱해주세요. (hint: mul_()함수를 통해 tensor 전체에 특정한 값을 곱해 줄 수 있습니다.)\n",
    "        fc.weight.mul_(scale)\n",
    "        ############### YOUR CODE ENDS HERE #################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "edc223d5",
   "metadata": {
    "id": "edc223d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPTForCausalLM(\n",
      "  (model): OPTModel(\n",
      "    (decoder): OPTDecoder(\n",
      "      (embed_tokens): Embedding(50272, 768, padding_idx=1)\n",
      "      (embed_positions): OPTLearnedPositionalEmbedding(2050, 768)\n",
      "      (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (layers): ModuleList(\n",
      "        (0-11): 12 x OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): W8A8Linear(768, 768, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (v_proj): W8A8Linear(768, 768, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (q_proj): W8A8Linear(768, 768, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (out_proj): W8A8Linear(768, 768, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): W8A8Linear(768, 3072, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (fc2): W8A8Linear(3072, 768, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50272, bias=False)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating...: 100%|██████████| 10/10 [00:00<00:00, 10.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "model perplexity: 31.65\n",
      "model size: 121.77 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(31.6455, device='cuda:0')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_model.model_reset()\n",
    "smooth_lm_by_scale(llm_model.model, 5)\n",
    "model_smoothquant_scale = quantize_opt(llm_model.model)\n",
    "print(model_smoothquant_scale)\n",
    "llm_model.model_change(model_smoothquant_scale)\n",
    "\n",
    "# Evaluate the model\n",
    "llm_model.model_evaluate(data_width=8, group_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fef5e4c",
   "metadata": {
    "id": "4fef5e4c"
   },
   "source": [
    "스케일링을 통해서 가중치와 활성화의 양자화 난이도를 적절히 분배하고, 가중치와 활성화를 모두 8bit로 유지할 수 있었습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b015ccf",
   "metadata": {
    "id": "1b015ccf"
   },
   "source": [
    "이번에는 코드에서 서로 다른 다양한 스케일링 팩터 $s$(예: 0.001, 0.01, 1)을 시도하고 혼란도(perplexity)의 변화를 관찰해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2131db92",
   "metadata": {
    "id": "2131db92"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scale_factor=0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating...: 100%|██████████| 10/10 [00:01<00:00,  9.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "model perplexity: 31.71\n",
      "model size: 121.77 MiB\n",
      "scale_factor=0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating...: 100%|██████████| 10/10 [00:01<00:00,  9.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "model perplexity: 31.62\n",
      "model size: 121.77 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HTTP Error 429 thrown while requesting HEAD https://huggingface.co/facebook/opt-125m/resolve/main/config.json\n",
      "Retrying in 1s [Retry 1/5].\n",
      "HTTP Error 429 thrown while requesting HEAD https://huggingface.co/facebook/opt-125m/resolve/main/config.json\n",
      "Retrying in 2s [Retry 2/5].\n",
      "HTTP Error 429 thrown while requesting HEAD https://huggingface.co/facebook/opt-125m/resolve/main/config.json\n",
      "Retrying in 4s [Retry 3/5].\n",
      "HTTP Error 429 thrown while requesting HEAD https://huggingface.co/facebook/opt-125m/resolve/main/config.json\n",
      "Retrying in 8s [Retry 4/5].\n",
      "HTTP Error 429 thrown while requesting HEAD https://huggingface.co/facebook/opt-125m/resolve/main/config.json\n",
      "Retrying in 8s [Retry 5/5].\n",
      "HTTP Error 429 thrown while requesting HEAD https://huggingface.co/facebook/opt-125m/resolve/main/config.json\n"
     ]
    },
    {
     "ename": "HfHubHTTPError",
     "evalue": "429 Client Error: Too Many Requests for url: https://huggingface.co/api/models/facebook/opt-125m",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\envs\\env_aias_test\\Lib\\site-packages\\huggingface_hub\\utils\\_http.py:409\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    408\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 409\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    410\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\envs\\env_aias_test\\Lib\\site-packages\\requests\\models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[1;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mHTTPError\u001b[0m: 429 Client Error: Too Many Requests for url: https://huggingface.co/api/models/facebook/opt-125m",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mHfHubHTTPError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m scale_factor \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m0.001\u001b[39m,\u001b[38;5;241m0.01\u001b[39m,\u001b[38;5;241m0.1\u001b[39m]:\n\u001b[1;32m----> 2\u001b[0m     \u001b[43mllm_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_reset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m     smooth_lm_by_scale(llm_model\u001b[38;5;241m.\u001b[39mmodel,scale_factor)\n\u001b[0;32m      4\u001b[0m     model \u001b[38;5;241m=\u001b[39m quantize_opt(llm_model\u001b[38;5;241m.\u001b[39mmodel)\n",
      "Cell \u001b[1;32mIn[3], line 54\u001b[0m, in \u001b[0;36mLLMModel.model_reset\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_changed:\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_delete()\n\u001b[1;32m---> 54\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat16\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_safetensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_changed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\envs\\env_aias_test\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:600\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    598\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mconfig_class \u001b[38;5;241m==\u001b[39m config\u001b[38;5;241m.\u001b[39msub_configs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext_config\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    599\u001b[0m         config \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget_text_config()\n\u001b[1;32m--> 600\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    601\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    602\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    603\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    604\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    605\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    606\u001b[0m )\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\envs\\env_aias_test\\Lib\\site-packages\\transformers\\modeling_utils.py:316\u001b[0m, in \u001b[0;36mrestore_default_torch_dtype.<locals>._wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    314\u001b[0m old_dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mget_default_dtype()\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    318\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_default_dtype(old_dtype)\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\envs\\env_aias_test\\Lib\\site-packages\\transformers\\modeling_utils.py:4910\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   4900\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   4901\u001b[0m     gguf_file\n\u001b[0;32m   4902\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   4903\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m ((\u001b[38;5;28misinstance\u001b[39m(device_map, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisk\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisk\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map)\n\u001b[0;32m   4904\u001b[0m ):\n\u001b[0;32m   4905\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   4906\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOne or more modules is configured to be mapped to disk. Disk offload is not supported for models \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   4907\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloaded from GGUF files.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   4908\u001b[0m     )\n\u001b[1;32m-> 4910\u001b[0m checkpoint_files, sharded_metadata \u001b[38;5;241m=\u001b[39m \u001b[43m_get_resolved_checkpoint_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4911\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4912\u001b[0m \u001b[43m    \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4913\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvariant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvariant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4914\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgguf_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgguf_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4915\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_tf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4916\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_flax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4917\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_safetensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_safetensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4918\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4919\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4920\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4921\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4922\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4923\u001b[0m \u001b[43m    \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4924\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4925\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4926\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_auto_class\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   4927\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransformers_explicit_filename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransformers_explicit_filename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4928\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4930\u001b[0m is_sharded \u001b[38;5;241m=\u001b[39m sharded_metadata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   4931\u001b[0m is_quantized \u001b[38;5;241m=\u001b[39m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\envs\\env_aias_test\\Lib\\site-packages\\transformers\\modeling_utils.py:1193\u001b[0m, in \u001b[0;36m_get_resolved_checkpoint_files\u001b[1;34m(pretrained_model_name_or_path, subfolder, variant, gguf_file, from_tf, from_flax, use_safetensors, cache_dir, force_download, proxies, local_files_only, token, user_agent, revision, commit_hash, is_remote_code, transformers_explicit_filename)\u001b[0m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m use_safetensors:\n\u001b[0;32m   1192\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m revision \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmain\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 1193\u001b[0m         resolved_archive_file, revision, is_sharded \u001b[38;5;241m=\u001b[39m \u001b[43mauto_conversion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1194\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcached_file_kwargs\u001b[49m\n\u001b[0;32m   1195\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1196\u001b[0m     cached_file_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrevision\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m revision\n\u001b[0;32m   1197\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m resolved_archive_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\envs\\env_aias_test\\Lib\\site-packages\\transformers\\safetensors_conversion.py:105\u001b[0m, in \u001b[0;36mauto_conversion\u001b[1;34m(pretrained_model_name_or_path, ignore_errors_during_conversion, **cached_file_kwargs)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    104\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ignore_errors_during_conversion:\n\u001b[1;32m--> 105\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\envs\\env_aias_test\\Lib\\site-packages\\transformers\\safetensors_conversion.py:84\u001b[0m, in \u001b[0;36mauto_conversion\u001b[1;34m(pretrained_model_name_or_path, ignore_errors_during_conversion, **cached_file_kwargs)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     83\u001b[0m     api \u001b[38;5;241m=\u001b[39m HfApi(token\u001b[38;5;241m=\u001b[39mcached_file_kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m\"\u001b[39m), headers\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser-agent\u001b[39m\u001b[38;5;124m\"\u001b[39m: http_user_agent()})\n\u001b[1;32m---> 84\u001b[0m     sha \u001b[38;5;241m=\u001b[39m \u001b[43mget_conversion_pr_reference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mapi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcached_file_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     86\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sha \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     87\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\envs\\env_aias_test\\Lib\\site-packages\\transformers\\safetensors_conversion.py:59\u001b[0m, in \u001b[0;36mget_conversion_pr_reference\u001b[1;34m(api, model_id, **kwargs)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_conversion_pr_reference\u001b[39m(api: HfApi, model_id: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m---> 59\u001b[0m     private \u001b[38;5;241m=\u001b[39m \u001b[43mapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mprivate\n\u001b[0;32m     61\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempting to create safetensors variant\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     62\u001b[0m     pr_title \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAdding `safetensors` variant of this model\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\envs\\env_aias_test\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[0;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\envs\\env_aias_test\\Lib\\site-packages\\huggingface_hub\\hf_api.py:2638\u001b[0m, in \u001b[0;36mHfApi.model_info\u001b[1;34m(self, repo_id, revision, timeout, securityStatus, files_metadata, expand, token)\u001b[0m\n\u001b[0;32m   2636\u001b[0m     params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpand\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m expand\n\u001b[0;32m   2637\u001b[0m r \u001b[38;5;241m=\u001b[39m get_session()\u001b[38;5;241m.\u001b[39mget(path, headers\u001b[38;5;241m=\u001b[39mheaders, timeout\u001b[38;5;241m=\u001b[39mtimeout, params\u001b[38;5;241m=\u001b[39mparams)\n\u001b[1;32m-> 2638\u001b[0m \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2639\u001b[0m data \u001b[38;5;241m=\u001b[39m r\u001b[38;5;241m.\u001b[39mjson()\n\u001b[0;32m   2640\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ModelInfo(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdata)\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\envs\\env_aias_test\\Lib\\site-packages\\huggingface_hub\\utils\\_http.py:482\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _format(HfHubHTTPError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    480\u001b[0m \u001b[38;5;66;03m# Convert `HTTPError` into a `HfHubHTTPError` to display request information\u001b[39;00m\n\u001b[0;32m    481\u001b[0m \u001b[38;5;66;03m# as well (request id and/or server error message)\u001b[39;00m\n\u001b[1;32m--> 482\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m _format(HfHubHTTPError, \u001b[38;5;28mstr\u001b[39m(e), response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mHfHubHTTPError\u001b[0m: 429 Client Error: Too Many Requests for url: https://huggingface.co/api/models/facebook/opt-125m"
     ]
    }
   ],
   "source": [
    "for scale_factor in [0.001,0.01,0.1]:\n",
    "    llm_model.model_reset()\n",
    "    smooth_lm_by_scale(llm_model.model,scale_factor)\n",
    "    model = quantize_opt(llm_model.model)\n",
    "    llm_model.model_change(model)\n",
    "\n",
    "    # Evaluate the model\n",
    "    print(f\"scale_factor={scale_factor}\")\n",
    "    llm_model.model_evaluate(data_width=8, group_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10278aff",
   "metadata": {
    "id": "10278aff"
   },
   "source": [
    "## [실습 4] Scale factor sampling\n",
    "\n",
    "스케일링 팩터 $s$값의 설정에 따라 혼란도(perplexity)가 먼저 변화하는 것을 관찰했나요?\n",
    "\n",
    "\n",
    "우리의 목표는 각 채널별 스케일링 팩터 s를 선택하여  X̂ = Xdiag(s)⁻¹가 양자화하기 쉽도록 만드는 것입니다.\n",
    "\n",
    "양자화 오류를 줄이기 위해 모든 채널의 유효 양자화 비트를 늘려야 합니다.\n",
    "\n",
    "가장 간단한 선택은 채널별로 서로 다른 스케일링 팩터를 설정하는 것입니다.\n",
    "\n",
    "스케일링 팩터를 weight의 최대값으로 설정하면 weight의 양자화 난이도가 쉬워집니다. 그러나 activation의 양자화는 어려워집니다.\n",
    "\n",
    "반대로, 스케일링 팩터를 activation의 최대값으로 설정하면 weight의 양자화가 어려워집니다.\n",
    "\n",
    "우리는 weight와 activation의 양자화 난이도 사이에서 균형을 맞추기 위해 스케일링 팩터 s를 다음과 같이 설정합니다.\n",
    "\n",
    "$s = \\max(|X|)^{\\alpha} / \\max(|W|)^{1-\\alpha}$\n",
    "\n",
    "아래 코드를 수정하여 스케일링 팩터 s를 위의 식과 같이 설정해주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "AQtTutXMuIYD",
   "metadata": {
    "id": "AQtTutXMuIYD"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def smooth_ln_fcs(ln, fcs, act_scales, alpha=0.5):\n",
    "    if not isinstance(fcs, list):\n",
    "        fcs = [fcs]\n",
    "    assert isinstance(ln, nn.LayerNorm)\n",
    "    for fc in fcs:\n",
    "        assert isinstance(fc, nn.Linear)\n",
    "        assert ln.weight.numel() == fc.in_features == act_scales.numel()\n",
    "\n",
    "    device, dtype = fcs[0].weight.device, fcs[0].weight.dtype\n",
    "    act_scales = act_scales.to(device=device, dtype=dtype)\n",
    "    weight_scales = torch.cat(\n",
    "        [fc.weight.abs().max(dim=0, keepdim=True)[0] for fc in fcs], dim=0\n",
    "    )\n",
    "    weight_scales = weight_scales.max(dim=0)[0].clamp(min=1e-5)\n",
    "\n",
    "    scales = (\n",
    "        ############### YOUR CODE STARTS HERE ###############\n",
    "        #Activation Scales 값과 Weight Scales 값에 alpha를 적절히 거듭제곱해주어야 합니다.\n",
    "        #Hint: pow()함수를 통해서 거듭제곱을 사용할 수 있습니다.\n",
    "        (act_scales ** alpha) / (weight_scales ** (1 - alpha))\n",
    "\n",
    "        ############### YOUR CODE ENDS HERE #################\n",
    "    )\n",
    "\n",
    "    scales.clamp(min=1e-5).to(device).to(dtype)\n",
    "\n",
    "    ln.weight.div_(scales)\n",
    "    ln.bias.div_(scales)\n",
    "\n",
    "    for fc in fcs:\n",
    "        fc.weight.mul_(scales.view(1, -1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sZKrDAfywHK4",
   "metadata": {
    "id": "sZKrDAfywHK4"
   },
   "source": [
    "여기서 활성화 범위는 동적이며 입력 샘플에 따라 달라집니다.\n",
    "\n",
    "사전 훈련 데이터 세트의 보정 샘플을 사용하여 활성화 채널의 크기를 추정해보겠습니다.\n",
    "\n",
    "아래 코드를 실행하면 512개의 사전 훈련 샘플 데이터 세트를 통해 자동으로 적절한 스케일링 팩터 $s$값을 찾아 양자화를 진행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "178b1225",
   "metadata": {
    "id": "178b1225"
   },
   "outputs": [],
   "source": [
    "import functools\n",
    "def get_act_scales(model, tokenizer, dataset_path, num_samples=512, seq_len=512):\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    act_scales = {}\n",
    "\n",
    "    def stat_tensor(name, tensor):\n",
    "        hidden_dim = tensor.shape[-1]\n",
    "        tensor = tensor.view(-1, hidden_dim).abs().detach()\n",
    "        comming_max = torch.max(tensor, dim=0)[0].float().cpu()\n",
    "        if name in act_scales:\n",
    "            act_scales[name] = torch.max(act_scales[name], comming_max)\n",
    "        else:\n",
    "            act_scales[name] = comming_max\n",
    "\n",
    "    def stat_input_hook(m, x, y, name):\n",
    "        if isinstance(x, tuple):\n",
    "            x = x[0]\n",
    "        stat_tensor(name, x)\n",
    "\n",
    "    hooks = []\n",
    "    for name, m in model.named_modules():\n",
    "        if isinstance(m, nn.Linear):\n",
    "            hooks.append(\n",
    "                m.register_forward_hook(functools.partial(stat_input_hook, name=name))\n",
    "            )\n",
    "\n",
    "    dataset = load_dataset(\"json\", data_files=dataset_path, split=\"train\")\n",
    "    dataset = dataset.shuffle(seed=42)\n",
    "\n",
    "    for i in tqdm(range(num_samples)):\n",
    "        input_ids = tokenizer(\n",
    "            dataset[i][\"text\"], return_tensors=\"pt\", max_length=seq_len, truncation=True\n",
    "        ).input_ids.to(device)\n",
    "        model(input_ids)\n",
    "\n",
    "    for h in hooks:\n",
    "        h.remove()\n",
    "\n",
    "    return act_scales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c1dee862",
   "metadata": {
    "id": "c1dee862"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def smooth_lm(model, scales, alpha=0.5):\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, OPTDecoderLayer):\n",
    "            attn_ln = module.self_attn_layer_norm\n",
    "            qkv = [\n",
    "                module.self_attn.q_proj,\n",
    "                module.self_attn.k_proj,\n",
    "                module.self_attn.v_proj,\n",
    "            ]\n",
    "            qkv_input_scales = scales[name + \".self_attn.q_proj\"]\n",
    "            smooth_ln_fcs(attn_ln, qkv, qkv_input_scales, alpha)\n",
    "\n",
    "            ffn_ln = module.final_layer_norm\n",
    "            fc1 = module.fc1\n",
    "            fc1_input_scales = scales[name + \".fc1\"]\n",
    "            smooth_ln_fcs(ffn_ln, fc1, fc1_input_scales, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be97158c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HTTP Error 429 thrown while requesting HEAD https://huggingface.co/facebook/opt-125m/resolve/main/config.json\n",
      "Retrying in 1s [Retry 1/5].\n",
      "HTTP Error 429 thrown while requesting HEAD https://huggingface.co/facebook/opt-125m/resolve/main/config.json\n",
      "Retrying in 2s [Retry 2/5].\n",
      "HTTP Error 429 thrown while requesting HEAD https://huggingface.co/facebook/opt-125m/resolve/main/config.json\n",
      "Retrying in 4s [Retry 3/5].\n",
      "HTTP Error 429 thrown while requesting HEAD https://huggingface.co/facebook/opt-125m/resolve/main/config.json\n",
      "Retrying in 8s [Retry 4/5].\n"
     ]
    }
   ],
   "source": [
    "# model_path = \"facebook/opt-125m\"\n",
    "# llm_model = LLMModel(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deaf4309",
   "metadata": {
    "id": "deaf4309"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84c9273664eb43beb6fdfac249c50eca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "llm_model.model_reset()\n",
    "\n",
    "act_scales = get_act_scales(\n",
    "        llm_model.model, llm_model.tokenizer, datapath, 512, 512)\n",
    "smooth_lm(llm_model.model, act_scales)\n",
    "model_sampled = quantize_opt(llm_model.model)\n",
    "llm_model.model_change(model_sampled)\n",
    "\n",
    "# Evaluate the model\n",
    "llm_model.model_evaluate(data_width=8, group_size=128)\n",
    "\n",
    "del llm_model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eDeUfFGQyRNv",
   "metadata": {
    "id": "eDeUfFGQyRNv"
   },
   "source": [
    "# Rotation Based Quantization\n",
    "최근에는 LLM Quantization의 Outlier 문제를 해결하기 위해 Rotation Matrix를 곱해서 Outlier를 제거하는 방법들이 제시되고 있습니다. (QuaRot, SpinQuant 등)\n",
    "\n",
    "![SpinQuant](https://raw.githubusercontent.com/facebookresearch/SpinQuant/refs/heads/main/SpinQuant.png)\n",
    "\n",
    "해당 기법들은 직교 행렬의 아래와 같은 특성을 활용합니다.\n",
    "\n",
    "1. $$ R\\,R^{T} = I $$\n",
    "\n",
    "2. 벡터 V에 직교 행렬 R을 곱하면, 길이는 같으나 방향이 바뀐 벡터 V'를 구할 수 있음\n",
    "\n",
    "\n",
    "\n",
    "## 수식 유도\n",
    "\n",
    "1. 원래 연산  \n",
    "   $$\n",
    "     y = x\\,W\n",
    "   $$\n",
    "\n",
    "2. 직교 행렬 $R$ 도입  \n",
    "   $$\n",
    "     y = x\\,I\\,W = x\\,(RR^{T})\\,W = (x\\,R)\\,(R^{T}\\,W)\n",
    "   $$\n",
    "\n",
    "3. 새로운 변수로 정의\n",
    "   $$\n",
    "     x' = x\\,R^{T}\n",
    "     \\quad\n",
    "     W' = R\\,W\n",
    "   $$\n",
    "   이때  \n",
    "   $$\n",
    "     y = x'\\,W'\n",
    "     = x\\,W\n",
    "   $$  \n",
    "   가 수식적으로 보존됨\n",
    "  \n",
    "## 장점\n",
    "  - 회전을 통해 분포를 분산시켜 Outlier 현상을 완화  \n",
    "  - 사전에 Weight에 곱해두는 것을 통해 적용 가능 및 온라인 연산 제거 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kclGHvQOFbmk",
   "metadata": {
    "id": "kclGHvQOFbmk"
   },
   "outputs": [],
   "source": [
    "llm_model = LLMModel(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RB8t1atrqqaT",
   "metadata": {
    "id": "RB8t1atrqqaT"
   },
   "outputs": [],
   "source": [
    "def get_orthogonal_matrix(size, mode=\"random\", dtype=torch.float32, device=\"cpu\"):\n",
    "    if mode == \"random\":\n",
    "        A = torch.randn(size, size, dtype=torch.float32, device=device)\n",
    "        Q, R = torch.linalg.qr(A)\n",
    "        Q *= torch.sign(torch.diag(R)).unsqueeze(0)\n",
    "        return Q.to(dtype)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown mode: {mode}\")\n",
    "\n",
    "dummy_vector = torch.randn(100)\n",
    "dummy_vector[50] = 30\n",
    "\n",
    "rotation_matrix = get_orthogonal_matrix(100)\n",
    "rotated_vector = rotation_matrix @ dummy_vector\n",
    "\n",
    "def plot_vector(vec, title):\n",
    "    plt.figure(figsize=(5, 2))\n",
    "    plt.plot(vec.abs().numpy(), linewidth=2)\n",
    "    plt.title(title, fontsize=14)\n",
    "    plt.xlabel('Index', fontsize=12)\n",
    "    plt.ylabel('Absolute Value', fontsize=12)\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(rotation_matrix)\n",
    "print(torch.round(rotation_matrix @ rotation_matrix.T))\n",
    "plot_vector(dummy_vector, 'Original Vector (Absolute Values)')\n",
    "plot_vector(rotated_vector, 'Rotated Vector (Absolute Values)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9oSOzVPXd5lo",
   "metadata": {
    "id": "9oSOzVPXd5lo"
   },
   "outputs": [],
   "source": [
    "def quantize_tinyllama(\n",
    "    model, weight_quant=\"per_tensor\", act_quant=\"per_tensor\", quantize_bmm_input=True, quantize_bits=4\n",
    "):\n",
    "    \"\"\"\n",
    "    Quantize TinyLlama model using W8A8Linear layers.\n",
    "    \"\"\"\n",
    "    for name, m in model.model.named_modules():\n",
    "        if isinstance(m, torch.nn.Linear):\n",
    "            # Skip lm_head as it's handled separately\n",
    "            if \"lm_head\" in name:\n",
    "                continue\n",
    "\n",
    "            # Create quantized linear layer\n",
    "            quantized_layer = W8A8Linear.from_float(\n",
    "                m, weight_quant=weight_quant, act_quant=act_quant, quantize_bits=4\n",
    "            )\n",
    "\n",
    "            # Replace the original layer with quantized one\n",
    "            # We need to find the parent module and replace the child\n",
    "            parent_name = \".\".join(name.split(\".\")[:-1])\n",
    "            child_name = name.split(\".\")[-1]\n",
    "\n",
    "            if parent_name:\n",
    "                parent = model.model.get_submodule(parent_name)\n",
    "                setattr(parent, child_name, quantized_layer)\n",
    "            else:\n",
    "                # Root level module\n",
    "                setattr(model.model, child_name, quantized_layer)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "TrHadElhixtg",
   "metadata": {
    "id": "TrHadElhixtg"
   },
   "source": [
    "## Layernorm ↔ Linear Fusion\n",
    "\n",
    "실제 모델에서는 Rotation Matrix 사이에 Layernorm이 존재하는 경우가 많습니다.\n",
    "\n",
    "![LayerNorm](https://miro.medium.com/v2/resize:fit:1252/1*kC-cWBWDEZpkSCtYIUsj4w.png)\n",
    "\n",
    "Normalization Layer의 영향으로 Rotation Matrix가 곱해지지 못해 정상적으로 제거되지 못하고 모델의 연산이 부정확해지는 문제가 발생할 수 있습니다.\n",
    "\n",
    "따라서, Rotation 적용 이전에 Normalization Layer를 Linear Layer와 Fusion해야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jFe23ot0ePl4",
   "metadata": {
    "id": "jFe23ot0ePl4"
   },
   "outputs": [],
   "source": [
    "# Layer Norm Fusion Functions for TinyLlama\n",
    "def fuse_ln_linear(layernorm, linear_layers):\n",
    "    \"\"\"\n",
    "    Fuse the linear operations in Layernorm into the adjacent linear blocks.\n",
    "    \"\"\"\n",
    "    for linear in linear_layers:\n",
    "        linear_dtype = linear.weight.dtype\n",
    "\n",
    "        # Calculating new weight and bias\n",
    "        W_ = linear.weight.data.double()\n",
    "        linear.weight.data = (W_ * layernorm.weight.double()).to(linear_dtype)\n",
    "\n",
    "        if hasattr(layernorm, \"bias\") and layernorm.bias is not None:\n",
    "            if linear.bias is None:\n",
    "                linear.bias = torch.nn.Parameter(\n",
    "                    torch.zeros(linear.out_features, dtype=torch.float64)\n",
    "                )\n",
    "            linear.bias.data = linear.bias.data.double() + torch.matmul(\n",
    "                W_, layernorm.bias.double()\n",
    "            )\n",
    "            linear.bias.data = linear.bias.data.to(linear_dtype)\n",
    "\n",
    "def fuse_layer_norms_tinyllama(model):\n",
    "    \"\"\"\n",
    "    Fuse layer norms for TinyLlama model structure.\n",
    "    \"\"\"\n",
    "    # Embedding fusion\n",
    "    for W in [model.model.embed_tokens]:\n",
    "        W_ = W.weight.data.double()\n",
    "        W.weight.data = (W_ - W_.mean(dim=-1, keepdim=True)).to(W.weight.data.dtype)\n",
    "\n",
    "    layers = [layer for layer in model.model.layers]\n",
    "\n",
    "    # Fuse the linear operations in Layernorm into the adjacent linear blocks.\n",
    "    for layer in layers:\n",
    "        # fuse the input layernorms into the linear layers\n",
    "        fuse_ln_linear(\n",
    "            layer.input_layernorm,\n",
    "            [layer.self_attn.q_proj, layer.self_attn.k_proj, layer.self_attn.v_proj]\n",
    "        )\n",
    "        fuse_ln_linear(\n",
    "            layer.post_attention_layernorm,\n",
    "            [layer.mlp.gate_proj, layer.mlp.up_proj]\n",
    "        )\n",
    "\n",
    "        # Set layernorm weights to ones\n",
    "        W_norm = layer.input_layernorm.weight.data\n",
    "        layer.input_layernorm.weight.data = torch.ones_like(W_norm)\n",
    "        W_norm = layer.post_attention_layernorm.weight.data\n",
    "        layer.post_attention_layernorm.weight.data = torch.ones_like(W_norm)\n",
    "\n",
    "    # Fuse final norm into lm_head\n",
    "    fuse_ln_linear(\n",
    "        model.model.norm,\n",
    "        [model.lm_head],\n",
    "    )\n",
    "    W_norm = model.model.norm.weight.data\n",
    "    model.model.norm.weight.data = torch.ones_like(W_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hvTcVLnimz2D",
   "metadata": {
    "id": "hvTcVLnimz2D"
   },
   "source": [
    "## [실습 5] Rotate Matrix 적용\n",
    "\n",
    "QuaRot는 R1만 사용하여 Roation을 적용합니다.\n",
    "\n",
    "그를 보완한 SpinQuant에서는 R2, R3, R4 등 다양한 Rotation Matrix를 적용하여 Quantization 정확도를 높입니다.\n",
    "\n",
    "그러나, R2는 R1과 유사하게 적용 가능하고, R3와 R4는 On-line에서 구해지는 Matrix이기 때문에 구현 난이도를 낮추기 위해 QuaRot을 구현하는 것으로 하겠습니다.\n",
    "\n",
    "QuaRot 혹은 SpinQuant 그림을 참고하셔서 각각의 연산에 R1이 어떻게 적용될 것인지 구현해보시기 바랍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "huCR1D1d3kf6",
   "metadata": {
    "id": "huCR1D1d3kf6"
   },
   "outputs": [],
   "source": [
    "def rotate_model_weight(\n",
    "    model, R1\n",
    "):\n",
    "    for n, m in model.named_modules():\n",
    "      ############### YOUR CODE STARTS HERE ###############\n",
    "      # Pytorch에서 @ 연산이 Dot Product 임을 사용하시기 바랍니다.\n",
    "      # nn.Linear 연산의 Parameter는 W^T 형태로 저장되어 있다는 것을 유의하시기 바랍니다.\n",
    "      # Embedding Parameter Shape : (Num_Tokens, Hidden_dim)\n",
    "      # Linear Parameter Shape : (Output_Channel, Input_Channel)\n",
    "      # Roation Matrix Shape : (Hidden_dim, Hidden_dim)\n",
    "\n",
    "      if isinstance(m, nn.Embedding):\n",
    "        W_ = m.weight.data\n",
    "        m.weight.data =\n",
    "\n",
    "      if isinstance(m, nn.Linear):\n",
    "        if \"o_proj\" in n or \"down_proj\" in n:\n",
    "          # Att Out Proj, FFN Down Proj\n",
    "          W_ = m.weight.data\n",
    "          m.weight.data =\n",
    "\n",
    "        else:\n",
    "          # QKV Proj, FFN Up Proj, FFN Gate Proj\n",
    "          W_ = m.weight.data\n",
    "          m.weight.data =\n",
    "\n",
    "      ############### YOUR CODE ENDS HERE #################\n",
    "\n",
    "      torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "JiIhAnRdda4P",
   "metadata": {
    "id": "JiIhAnRdda4P"
   },
   "outputs": [],
   "source": [
    "print(\"\\nOriginal 모델 성능 측정 중...\")\n",
    "llm_model.model_reset()\n",
    "original_perplexity = llm_model.model_evaluate(data_width=16, group_size=128)\n",
    "output_orig = llm_model.model(llm_model.testenc[:,:500].to(llm_model.model.device), output_hidden_states=True)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Mbdzg5mVIL3j",
   "metadata": {
    "id": "Mbdzg5mVIL3j"
   },
   "outputs": [],
   "source": [
    "Q_BITS = 8\n",
    "\n",
    "print(\"\\nQuantization만 적용한 모델 성능 측정 중...\")\n",
    "llm_model.model_reset()\n",
    "model = quantize_tinyllama(llm_model.model, quantize_bits=Q_BITS)\n",
    "llm_model.model_change(model)\n",
    "quantized_only_perplexity = llm_model.model_evaluate(data_width=Q_BITS, group_size=128)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zzcHRRb1ILlA",
   "metadata": {
    "id": "zzcHRRb1ILlA"
   },
   "outputs": [],
   "source": [
    "print(\"\\nRotation + Quantization 적용한 모델 성능 측정 중...\")\n",
    "llm_model.model_reset()\n",
    "fuse_layer_norms_tinyllama(llm_model.model)\n",
    "hidden_size = llm_model.model.config.hidden_size\n",
    "R1_random = get_orthogonal_matrix(hidden_size, mode=\"random\", dtype=llm_model.model.dtype, device=llm_model.model.device)\n",
    "rotate_model_weight(llm_model.model, R1_random)\n",
    "model = quantize_tinyllama(llm_model.model, quantize_bits=Q_BITS)\n",
    "llm_model.model_change(model)\n",
    "rotation_quantized_perplexity = llm_model.model_evaluate(data_width=Q_BITS, group_size=128)\n",
    "output_rotated = llm_model.model(llm_model.testenc[:,:500].to(llm_model.model.device), output_hidden_states=True)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rSiV8ayaew7s",
   "metadata": {
    "id": "rSiV8ayaew7s"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_2d(data, title):\n",
    "    # Get dimensions of hidden states - handle different shapes\n",
    "    if len(data.shape) == 3:  # (batch, seq_len, hidden_dim)\n",
    "        data = data[0]  # Take first batch\n",
    "    elif len(data.shape) == 2:  # (seq_len, hidden_dim)\n",
    "        pass\n",
    "    else:\n",
    "        print(f\"Unexpected data shape: {data.shape}\")\n",
    "        return\n",
    "\n",
    "    seq_len, hidden_dim = data.shape\n",
    "\n",
    "    mean_activations = np.mean(np.abs(data), axis=0)\n",
    "\n",
    "    # 2D Plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(np.arange(hidden_dim), mean_activations, color='blue', linewidth=1.5)\n",
    "\n",
    "    plt.xlabel(\"Hidden Dimension\", fontsize=12)\n",
    "    plt.ylabel(\"Absolute Value\", fontsize=12)\n",
    "    plt.title(f\"Hidden States Mean Activation - {title}\", fontsize=14)\n",
    "\n",
    "    plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 사용 예시\n",
    "plot_2d(output_orig.hidden_states[16][:, 100:].detach().cpu().numpy(), \"Original\")\n",
    "plot_2d(output_rotated.hidden_states[16][:, 100:].detach().cpu().numpy(), \"Rotated\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "env_aias_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
