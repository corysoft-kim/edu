{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1pRgZQnaFW4Z"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vG_sipzEFjNq"
      },
      "source": [
        "## Experimental Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "puMBx4l2H54U"
      },
      "outputs": [],
      "source": [
        "# Math and data preprocessing libraries\n",
        "import math\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# For handling dataset\n",
        "import yfinance as yf\n",
        "from datetime import date\n",
        "\n",
        "# For visualization\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# For deep learning\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# For evaluation\n",
        "from sklearn.metrics \\\n",
        "import root_mean_squared_error, mean_absolute_percentage_error"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMNP-pLAGdG5"
      },
      "source": [
        "## Configure GPU Settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "5CspPH_cPA12"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if \\\n",
        "                      torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNuDhwHnFcXk"
      },
      "source": [
        "# Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nP_hTLiGiOs"
      },
      "source": [
        "## Dataset Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cRI1ZEcOH-U3"
      },
      "outputs": [],
      "source": [
        "start_date = '2020-01-01'\n",
        "end_date = '2024-12-31'\n",
        "\n",
        "df = yf.download('GOOG', start=start_date, end=end_date)\n",
        "\n",
        "# Inspect the data\n",
        "print()\n",
        "print(df.head())\n",
        "print(df.info())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ikh-k5HVGkt2"
      },
      "source": [
        "## Data Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vkMX-385IEul"
      },
      "outputs": [],
      "source": [
        "ncols = 1\n",
        "nrows = int(round(df.shape[1] / ncols, 0))\n",
        "fig, ax = plt.subplots(nrows=nrows, ncols=ncols, \\\n",
        "                       sharex=True, figsize=(14, 7))\n",
        "for i, ax in enumerate(fig.axes):\n",
        "    sns.lineplot(data=df.iloc[:, i], ax=ax)\n",
        "    ax.tick_params(axis=\"x\", rotation=30, \\\n",
        "                   labelsize=10, length=0)\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGIcJfd-Goys"
      },
      "source": [
        "## Train-Test Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Mj7quYmII7l"
      },
      "outputs": [],
      "source": [
        "# Train test split\n",
        "train_ratio = 0.8\n",
        "training_data_len = math.ceil(len(df) * train_ratio)\n",
        "\n",
        "# Splitting the dataset\n",
        "train_data = df[:training_data_len][['Open']]\n",
        "test_data = df[training_data_len:][['Open']]\n",
        "print(train_data.shape)\n",
        "print(test_data.shape)\n",
        "\n",
        "# (1006, 1)\n",
        "# (251, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmp2GJGFGr62"
      },
      "source": [
        "## Scaling the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JzNm0jsoPY5V"
      },
      "outputs": [],
      "source": [
        "# Scaling dataset\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "train_scaled = scaler.fit_transform(train_data.values)\n",
        "test_scaled = scaler.transform(test_data.values)\n",
        "\n",
        "for v in train_data.values[:5, 0]:\n",
        "  print(f'{v:6.3f}', end=' ')\n",
        "print()\n",
        "# 66.681 66.995 67.101 69.484 69.193\n",
        "\n",
        "for v in train_scaled[:5, 0]:\n",
        "  print(f'{v:6.3f}', end=' ')\n",
        "print()\n",
        "# 0.144  0.147  0.148  0.172  0.169"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2-unQePGuHS"
      },
      "source": [
        "## Sliding Window"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LWKuF3_fIPCT"
      },
      "outputs": [],
      "source": [
        "sequence_length = 50  # Number of time steps to look back\n",
        "\n",
        "def convert_data_into_tensors(data_seq):\n",
        "  features, labels = [], []\n",
        "  for i in range(len(data_seq) - sequence_length):\n",
        "      features.append(data_seq[i:i + sequence_length])\n",
        "      labels.append(data_seq[i + sequence_length, 0])\n",
        "  features, labels = np.array(features), np.array(labels)\n",
        "\n",
        "  features = torch.tensor(features, dtype=torch.float32)\n",
        "  labels = torch.tensor(labels, dtype=torch.float32)\n",
        "  return features, labels\n",
        "\n",
        "X_train, y_train = convert_data_into_tensors(train_scaled)\n",
        "X_test, y_test = convert_data_into_tensors(test_scaled)\n",
        "\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_test.shape)\n",
        "\n",
        "# torch.Size([956, 50, 1])\n",
        "# torch.Size([956])\n",
        "# torch.Size([201, 50, 1])\n",
        "# torch.Size([201])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dmGc2meGv-0"
      },
      "source": [
        "## Batching the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-j78Zt_lQkrR"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "\n",
        "def to_loader(x, y, batch_size, shuffle):\n",
        "  dataset = torch.utils.data.TensorDataset(x, y)\n",
        "  return torch.utils.data.DataLoader(dataset, batch_size, shuffle)\n",
        "\n",
        "train_loader = to_loader(X_train, y_train, batch_size, shuffle=True)\n",
        "test_loader = to_loader(X_test, y_test, batch_size, shuffle=False)\n",
        "\n",
        "print(train_loader)\n",
        "print(test_loader)\n",
        "\n",
        "# <torch.utils.data.dataloader.DataLoader object at 0x78eacd13fdd0>\n",
        "# <torch.utils.data.dataloader.DataLoader object at 0x78eacd13fd50>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSgCfqUMGyM-"
      },
      "source": [
        "# Practice"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fX9yGT4vsT60"
      },
      "outputs": [],
      "source": [
        "# set random seed\n",
        "\n",
        "manual_seed = 42\n",
        "torch.manual_seed(manual_seed)\n",
        "np.random.seed(manual_seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENbYd2DHG2C9"
      },
      "source": [
        "## Model: LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KFeekwSYJV_-"
      },
      "outputs": [],
      "source": [
        "input_size = X_train.shape[-1]\n",
        "num_layers = 2\n",
        "hidden_size = 64\n",
        "\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        # (lstm): LSTM(1, 64, num_layers=2, batch_first=True)\n",
        "\n",
        "        self.linear = nn.Linear(hidden_size, 1)\n",
        "        # (linear): Linear(in_features=64, out_features=1, bias=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, _ = self.lstm(x)\n",
        "        return self.linear(out)\n",
        "\n",
        "model = LSTMModel(input_size, hidden_size, num_layers).to(device)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sGZzbUkUdnJI"
      },
      "source": [
        "### Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7GIRcvu8doPg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "outputId": "accd373e-e2c2-4151-f4db-f0f7cf9def77"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (ipython-input-1-1914031051.py, line 16)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-1-1914031051.py\"\u001b[0;36m, line \u001b[0;32m16\u001b[0m\n\u001b[0;31m    batch_x, batch_y =\u001b[0m\n\u001b[0m                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "def train(model, train_loader, test_loader):\n",
        "  train_hist = []\n",
        "  test_hist = []\n",
        "  num_epochs = 10\n",
        "  loss_fn = nn.MSELoss(reduction='mean')\n",
        "  optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    total_train_loss = 0.0\n",
        "    total_test_loss = 0.0\n",
        "\n",
        "    # train\n",
        "    model.train()\n",
        "    for batch_x, batch_y in train_loader:\n",
        "      # (1) Move input and target tensors to the device (e.g., GPU)\n",
        "      batch_x, batch_y =\n",
        "\n",
        "      # (2) Pass the input (batch_x) through the model\n",
        "      #     The model outputs shape [batch_size, sequence_length, 1]\n",
        "      #     Take the prediction at the last timestep (index -1) and feature index 0\n",
        "      #     → model(batch_x)[:, -1, 0]\n",
        "      pred =\n",
        "\n",
        "      # (3) Compute the loss between pred and batch_y by using loss_fn\n",
        "      loss =\n",
        "\n",
        "      # (4) Clear previous gradients to avoid accumulation\n",
        "      optimizer.\n",
        "\n",
        "      # (5) Perform backpropagation to compute gradients\n",
        "      loss.\n",
        "\n",
        "      # (6) Update the model parameters using the optimizer\n",
        "      optimizer.\n",
        "\n",
        "      total_train_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_train_loss / len(train_loader)\n",
        "    train_hist.append(avg_loss)\n",
        "\n",
        "    # evaluate\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "      for test_x, test_y in test_loader:\n",
        "        # TODO\n",
        "        # (1) Move input and target tensors to the device (e.g., GPU)\n",
        "        test_x, test_y =\n",
        "\n",
        "        # (2) Pass the input (test_x) through the model\n",
        "        #     The model outputs shape [batch_size, sequence_length, 1]\n",
        "        #     Take the prediction at the last timestep (index -1) and feature index 0\n",
        "        #     → model(test_x)[:, -1, 0]\n",
        "        test_pred =\n",
        "\n",
        "        # (3) Compute the loss between test_pred and test_y by using loss_fn\n",
        "        test_loss =\n",
        "\n",
        "        total_test_loss += test_loss.item()\n",
        "\n",
        "    avg_test_loss = total_test_loss / len(test_loader)\n",
        "    test_hist.append(avg_test_loss)\n",
        "\n",
        "    print(f'Epoch {epoch + 1:2d}/{num_epochs} - Training Loss: {avg_loss:.4f}, Test Loss: {avg_test_loss:.4f}')\n",
        "\n",
        "  x = np.linspace(1, num_epochs, num_epochs)\n",
        "  plt.plot(x, train_hist, scalex=True, label=\"Training loss\")\n",
        "  plt.plot(x, test_hist, label=\"Test loss\")\n",
        "  plt.legend()\n",
        "\n",
        "train(model, train_loader, test_loader)\n",
        "\n",
        "# Epoch  1/10 - Training Loss: 0.1736, Test Loss: 0.4874\n",
        "# Epoch  2/10 - Training Loss: 0.0388, Test Loss: 0.0877\n",
        "# Epoch  3/10 - Training Loss: 0.0078, Test Loss: 0.0072\n",
        "# Epoch  4/10 - Training Loss: 0.0031, Test Loss: 0.0097\n",
        "# Epoch  5/10 - Training Loss: 0.0023, Test Loss: 0.0123\n",
        "# Epoch  6/10 - Training Loss: 0.0020, Test Loss: 0.0188\n",
        "# Epoch  7/10 - Training Loss: 0.0020, Test Loss: 0.0111\n",
        "# Epoch  8/10 - Training Loss: 0.0021, Test Loss: 0.0165\n",
        "# Epoch  9/10 - Training Loss: 0.0018, Test Loss: 0.0225\n",
        "# Epoch 10/10 - Training Loss: 0.0017, Test Loss: 0.0177"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aGm-CGeoeFr7"
      },
      "source": [
        "### Forecasting Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pbT_GFD0Jm-l"
      },
      "outputs": [],
      "source": [
        "def plot_forecasting(model, X_test, y_test):\n",
        "  model.eval()\n",
        "  num_forecast_steps = 30\n",
        "  input_data = X_test[-num_forecast_steps].cpu().numpy().squeeze()\n",
        "\n",
        "  forecasted_values = []\n",
        "  with torch.no_grad():\n",
        "    for i in range(2 * num_forecast_steps):\n",
        "      input_tensor = torch.as_tensor(input_data).view(1, -1, 1).to(device)\n",
        "      predicted = model(input_tensor)[0, -1, 0].item()\n",
        "\n",
        "      forecasted_values.append(predicted)\n",
        "      input_data = np.roll(input_data, shift=-1)\n",
        "\n",
        "      if i < num_forecast_steps:\n",
        "        input_data[-1] = y_test[-num_forecast_steps + i]\n",
        "      else:\n",
        "        input_data[-1] = predicted\n",
        "\n",
        "  df_out = df.copy()\n",
        "  last_date = df_out.index[-1]\n",
        "  future_dates = pd.date_range(start=last_date + pd.DateOffset(1), periods=num_forecast_steps)\n",
        "  combined_dates = df_out.index.append(future_dates)\n",
        "  plt.rcParams['figure.figsize'] = [14, 4]\n",
        "  plt.plot(test_data.index[-100:-30], test_data[-100:-30], label = \"test_data\", color = \"b\")\n",
        "  plt.plot(test_data.index[-30:], test_data.iloc[-30:], label='actual values', color='green')\n",
        "\n",
        "  forecasted_cases = scaler.inverse_transform(np.expand_dims(forecasted_values, axis=0)).flatten()\n",
        "  plt.plot(combined_dates[-60:], forecasted_cases, label='forecasted values', color='red')\n",
        "\n",
        "  plt.xlabel('Time Step')\n",
        "  plt.ylabel('Value')\n",
        "  plt.legend()\n",
        "  plt.title('Time Series Forecasting')\n",
        "  plt.grid(True)\n",
        "\n",
        "plot_forecasting(model, X_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3LrQd9YbLN51"
      },
      "source": [
        "### Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IEG5G4F0RVgz"
      },
      "outputs": [],
      "source": [
        "def test(model, X_test, y_test):\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "      test_predictions = []\n",
        "      for batch_X_test in X_test:\n",
        "          batch_X_test = batch_X_test.to(device).unsqueeze(0)\n",
        "          test_predictions.append(model(batch_X_test) \\\n",
        "                                  .cpu().numpy().flatten()[0])\n",
        "\n",
        "  test_predictions = np.array(test_predictions)\n",
        "  y_test = y_test.cpu().numpy()\n",
        "\n",
        "  rmse = root_mean_squared_error(, ) # TODO\n",
        "  mape = mean_absolute_percentage_error(, ) # TODO\n",
        "\n",
        "  return rmse, mape\n",
        "\n",
        "rmse, mape = test(model, X_test, y_test)\n",
        "print(f'RMSE: {rmse:.4f}')\n",
        "print(f'MAPE: {mape:.4f}')\n",
        "\n",
        "# RMSE: 1.1711\n",
        "# MAPE: 0.9778"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hDH_9Stfgod"
      },
      "source": [
        "#Other Models: CNN, RNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UfPFt2AogG3e"
      },
      "outputs": [],
      "source": [
        "class Conv1DModel(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size):\n",
        "    super(Conv1DModel, self).__init__()\n",
        "    # TODO\n",
        "    self.conv1d = nn.Conv1d(in_channels=, out_channels=, \\\n",
        "                            kernel_size=2, stride=1)\n",
        "    self.fc = nn.Linear(, 1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x.transpose(1, 2)\n",
        "    x = self.conv1d(x)\n",
        "    x = x.transpose(1, 2)\n",
        "    return self.fc(x)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_size = X_train.shape[-1]\n",
        "num_layers = 2\n",
        "hidden_size = 64\n",
        "\n",
        "model = Conv1DModel(input_size, hidden_size).to(device)\n",
        "\n",
        "print(model)"
      ],
      "metadata": {
        "id": "h5eeVKLa6hj7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(model, train_loader, test_loader)"
      ],
      "metadata": {
        "id": "u00fzH7q6rLD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_forecasting(model, X_test, y_test)"
      ],
      "metadata": {
        "id": "61d95Auk6r0l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rmse, mape = test(model, X_test, y_test)\n",
        "print(f'RMSE: {rmse:.4f}')\n",
        "print(f'MAPE: {mape:.4f}')"
      ],
      "metadata": {
        "id": "AaE20tMt6sst"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d2ZoZFdXgbuJ"
      },
      "outputs": [],
      "source": [
        "class RNNModel(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, num_layers):\n",
        "      super(RNNModel, self).__init__()\n",
        "      # TODO\n",
        "      self.rnn = nn.RNN(, , , batch_first=True)\n",
        "      self.fc = nn.Linear(, 1)\n",
        "\n",
        "  def forward(self, x):\n",
        "      # TODO\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pC9Jcr00Rm5A"
      },
      "outputs": [],
      "source": [
        "input_size = X_train.shape[-1]\n",
        "num_layers = 2\n",
        "hidden_size = 64\n",
        "\n",
        "model = RNNModel(input_size, hidden_size, num_layers).to(device)\n",
        "\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yJRNM6kqSJyR"
      },
      "outputs": [],
      "source": [
        "train(model, train_loader, test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W8yaMFHqnTAp"
      },
      "outputs": [],
      "source": [
        "plot_forecasting(model, X_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fQUm3Y3aSUPA"
      },
      "outputs": [],
      "source": [
        "rmse, mape = test(model, X_test, y_test)\n",
        "print(f'RMSE: {rmse:.4f}')\n",
        "print(f'MAPE: {mape:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPRe_fgMhr_e"
      },
      "source": [
        "# Optional (Encoder-Decoder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pCWoPPfdpRgG"
      },
      "source": [
        "## Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JNkWlImXo__C"
      },
      "outputs": [],
      "source": [
        "sequence_length = 50\n",
        "target_len = 10\n",
        "\n",
        "def create_enc_dec_sequences(data):\n",
        "    features, labels = [], []\n",
        "    for i in range(len(data) - sequence_length - target_len):\n",
        "        features.append(data[i:i + sequence_length])\n",
        "        labels.append(data[i + sequence_length : \\\n",
        "                           i + sequence_length + target_len])\n",
        "\n",
        "    features = np.array(features, dtype=np.float32)\n",
        "    labels = np.array(labels, dtype=np.float32)\n",
        "\n",
        "    features = torch.tensor(features, dtype=torch.float32)\n",
        "    labels = torch.tensor(labels, dtype=torch.float32)\n",
        "    return features, labels\n",
        "\n",
        "X_train_, y_train_ = create_enc_dec_sequences(train_scaled)\n",
        "X_test_, y_test_ = create_enc_dec_sequences(test_scaled)\n",
        "print(X_train_.shape)\n",
        "print(y_train_.shape)\n",
        "print(X_test_.shape)\n",
        "print(y_test_.shape)\n",
        "\n",
        "train_loader_ = to_loader(X_train_, y_train_, batch_size, shuffle=True)\n",
        "test_loader_ = to_loader(X_test_, y_test_, batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKI9BCgziDy2"
      },
      "source": [
        "## Encoder-Decoder Structure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Js1_Tm0CfKFp"
      },
      "outputs": [],
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, num_layers):\n",
        "    # TODO\n",
        "\n",
        "  def forward(self, x):\n",
        "    # TODO\n",
        "\n",
        "class DecoderRNN(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, num_layers):\n",
        "    # TODO\n",
        "\n",
        "  def forward(self, x, h):\n",
        "    # TODO\n",
        "\n",
        "class RNNRNN(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, num_layers):\n",
        "    super(RNNRNN, self).__init__()\n",
        "    self.encoder = EncoderRNN(input_size, hidden_size, num_layers)\n",
        "    self.decoder = DecoderRNN(input_size, hidden_size, num_layers)\n",
        "\n",
        "  def forward(self, source, target_len):\n",
        "    h = self.encoder(source)\n",
        "    predictions = []\n",
        "    input = source[:, -1, :].unsqueeze(1)\n",
        "    for t in range(target_len):\n",
        "      out, h = self.decoder(input, h)\n",
        "      predictions.append(out.squeeze(1))\n",
        "      input = out\n",
        "    outputs = torch.stack(predictions, dim=1)\n",
        "\n",
        "    return outputs\n",
        "\n",
        "model_ = RNNRNN(input_size, hidden_size, num_layers).to(device)\n",
        "print(model_)\n",
        "# RNNRNN(\n",
        "#   (encoder): EncoderRNN(\n",
        "#     (rnn): RNN(1, 64, num_layers=2, batch_first=True)\n",
        "#   )\n",
        "#   (decoder): DecoderRNN(\n",
        "#     (rnn): RNN(1, 64, num_layers=2, batch_first=True)\n",
        "#     (fc): Linear(in_features=64, out_features=1, bias=True)\n",
        "#   )\n",
        "# )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCnceaLKt6c-"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UWnDIvSVt8QV"
      },
      "outputs": [],
      "source": [
        "def train_(model, train_loader, test_loader):\n",
        "  train_hist = []\n",
        "  test_hist = []\n",
        "  num_epochs = 10\n",
        "  loss_fn = nn.MSELoss(reduction='mean')\n",
        "  optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    total_train_loss = 0.0\n",
        "    total_test_loss = 0.0\n",
        "\n",
        "    # train\n",
        "    model.train()\n",
        "    for batch_x, batch_y in train_loader:\n",
        "      batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
        "      pred = model(batch_x, batch_y.shape[1])\n",
        "      loss = loss_fn(pred, batch_y)\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      total_train_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_train_loss / len(train_loader)\n",
        "    train_hist.append(avg_loss)\n",
        "\n",
        "    # evaluate\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "      for test_x, test_y in test_loader:\n",
        "        test_x, test_y = test_x.to(device), test_y.to(device)\n",
        "        test_pred = model(test_x, target_len)\n",
        "        test_loss = loss_fn(test_pred, test_y)\n",
        "        total_test_loss += test_loss.item()\n",
        "\n",
        "    avg_test_loss = total_test_loss / len(test_loader)\n",
        "    test_hist.append(avg_test_loss)\n",
        "\n",
        "    print(f'Epoch {epoch + 1:2d}/{num_epochs} - Training Loss: {avg_loss:.4f}, Test Loss: {avg_test_loss:.4f}')\n",
        "\n",
        "  x = np.linspace(1, num_epochs, num_epochs)\n",
        "  plt.plot(x, train_hist, scalex=True, label=\"Training loss\")\n",
        "  plt.plot(x, test_hist, label=\"Test loss\")\n",
        "  plt.legend()\n",
        "\n",
        "train_(model_, train_loader_, test_loader_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-3zYmsl7r5V"
      },
      "source": [
        "## Plot Forecasting\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KgPbNea67ut6"
      },
      "outputs": [],
      "source": [
        "def plot_forecasting_(model, X_test, y_test):\n",
        "  model.eval()\n",
        "  input_tensor = X_test[-1].unsqueeze(0).to(device)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    predicted = model(input_tensor, target_len)\n",
        "    forecasted_values = predicted.squeeze(0).cpu().numpy()\n",
        "\n",
        "  forecasted_values = scaler.inverse_transform(forecasted_values.reshape(-1, 1))\n",
        "  actual_values = scaler.inverse_transform(y_test[-1].cpu().numpy().reshape(-1, 1))\n",
        "\n",
        "  plt.rcParams['figure.figsize'] = [14, 4]\n",
        "\n",
        "  plt.plot(test_data.index[-60:-10], test_data[-60:-10], label=\"Test Data\", color=\"b\")\n",
        "  plt.plot(test_data.index[-10:], test_data.iloc[-10:], label='Actual Values', color='green')\n",
        "\n",
        "  plt.plot(test_data.index[-10:], forecasted_values, label='Forecasted Values', color='red')\n",
        "\n",
        "  plt.xlabel('Time Step')\n",
        "  plt.ylabel('Value')\n",
        "  plt.legend()\n",
        "  plt.title('Time Series Forecasting')\n",
        "  plt.grid(True)\n",
        "\n",
        "plot_forecasting_(model_, X_test_, y_test_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1VaQqPNgBku"
      },
      "source": [
        "## Test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tV4iFbr4gCc1"
      },
      "outputs": [],
      "source": [
        "def test_(model, X_test, y_test):\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "      test_predictions = []\n",
        "      for batch_X_test in X_test:\n",
        "          batch_X_test = batch_X_test.to(device).unsqueeze(0)\n",
        "          test_predictions.append(model(batch_X_test, target_len).cpu().numpy().flatten())\n",
        "\n",
        "  test_predictions = np.array(test_predictions)\n",
        "  y_test = y_test.cpu().numpy().reshape(-1, target_len)\n",
        "\n",
        "  rmse = root_mean_squared_error(y_test, test_predictions)\n",
        "  mape = mean_absolute_percentage_error(y_test, test_predictions)\n",
        "\n",
        "  return rmse, mape\n",
        "\n",
        "rmse, mape = test_(model_, X_test_, y_test_)\n",
        "print(f'RMSE: {rmse:.4f}')\n",
        "print(f'MAPE: {mape:.4f}')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}