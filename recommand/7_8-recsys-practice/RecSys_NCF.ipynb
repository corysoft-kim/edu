{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Base setting"
      ],
      "metadata": {
        "id": "x6D5knAtSVGR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch # 실습 환경에선 주석처리 가능.\n",
        "!pip install torch-geometric"
      ],
      "metadata": {
        "id": "4BGXRkwQSUVR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch_geometric.data import download_url, extract_zip\n",
        "\n",
        "from sklearn import model_selection, metrics, preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "import random\n",
        "from tqdm.notebook import tqdm\n",
        "import math\n",
        "from collections import defaultdict\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "metadata": {
        "id": "zLgmHV4vQaDj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# url = 'https://files.grouplens.org/datasets/movielens/ml-latest-small.zip'\n",
        "# extract_zip(download_url(url, '.'), '.')\n",
        "\n",
        "# ratings_path = './ml-latest-small/ratings.csv' # need to fix\n",
        "ratings_path = '/content/ratings.csv'\n",
        "df = pd.read_csv(ratings_path)\n",
        "\n",
        "print(len(df)) # number of data\n",
        "print(df['userId'].nunique()) # number of users\n",
        "print(df['movieId'].nunique()) # number of movies"
      ],
      "metadata": {
        "id": "wB2qNOANQYGS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# need to fix\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ],
      "metadata": {
        "id": "H3BXIuCSTpOX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neural Collaborative filtering"
      ],
      "metadata": {
        "id": "G7BiWYw7Rori"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Data_preprocessing"
      ],
      "metadata": {
        "id": "WuKqJ7hfCFYw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate Dataloader object (used as input data for model)\n",
        "class MovieLens:\n",
        "  def __init__(self, users, movies, ratings):\n",
        "    self.users = users\n",
        "    self.movies = movies\n",
        "    self.ratings = ratings\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.users)\n",
        "\n",
        "  def __getitem__(self,item):\n",
        "    '''\n",
        "    item = randomly selected indexes for (user,item) pairs\n",
        "    '''\n",
        "    users = self.users[item]\n",
        "    movies = self.movies[item]\n",
        "    ratings = self.ratings[item]\n",
        "    return {'users': torch.tensor(users, dtype = torch.long).to(device),\n",
        "            'movies': torch.tensor(movies, dtype = torch.long).to(device),\n",
        "            'ratings': torch.tensor(ratings, dtype=torch.long).to(device)}"
      ],
      "metadata": {
        "id": "KQ_cDx-35su-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert actual index for user and items as consecutive integer\n",
        "lbl_user = preprocessing.LabelEncoder()\n",
        "lbl_movie = preprocessing.LabelEncoder()\n",
        "\n",
        "df.userId = lbl_user.fit_transform(df.userId.values)\n",
        "df.movieId = lbl_movie.fit_transform(df.movieId.values)\n",
        "\n",
        "# devide original dataframe into train, test dataframe (9:1)\n",
        "df_train, df_test = model_selection.train_test_split(df, test_size=0.1, random_state=42, stratify=df.rating.values)\n",
        "\n",
        "train_dataset = MovieLens(users = df_train.userId.values, movies = df_train.movieId.values, ratings = df_train.rating.values)\n",
        "test_dataset = MovieLens(users = df_test.userId.values, movies = df_test.movieId.values, ratings = df_test.rating.values)\n",
        "\n",
        "print(len(train_dataset))\n",
        "print(len(test_dataset))"
      ],
      "metadata": {
        "id": "g3qYW4Z59kXR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Model setting"
      ],
      "metadata": {
        "id": "TNsApMI8CRsd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Embedding layer + NCF layer\n",
        "# 2-layer(including output layer)\n",
        "\n",
        "# hidden layer + active layer(RELU) + output\n",
        "# latent vector dim: 32\n",
        "# input dim: 64\n",
        "\n",
        "class Neural_Collaborative_Filtering(nn.Module):\n",
        "  def __init__(self, n_users, n_movies):\n",
        "    '''\n",
        "    n_users = # of users\n",
        "    n_movies = # of movies\n",
        "    '''\n",
        "    super().__init__()\n",
        "    # convert all users and items into 32-dim learnable latent factors(vectors)\n",
        "    self.user_embedding = nn.Embedding(n_users, 32)\n",
        "    self.movie_embedding = nn.Embedding(n_movies, 32)\n",
        "\n",
        "    # Neural network for rating prediction\n",
        "    self.fc1 = nn.Linear(64,32) # 64 = user_i embedding + movie_j embedding\n",
        "    self.relu = nn.ReLU()\n",
        "    self.fc2= nn.Linear(32,1)\n",
        "\n",
        "  def forward(self, users, movies, ratings = None):\n",
        "    user_embedding = self.user_embedding(users)\n",
        "    movie_embedding = self.movie_embedding(movies)\n",
        "    input_embedding = torch.cat([user_embedding, movie_embedding], dim = 1)\n",
        "\n",
        "    hidden_feature = self.fc1(input_embedding)\n",
        "    hidden_feature = self.relu(hidden_feature)\n",
        "\n",
        "    output = self.fc2(hidden_feature) # output is prediction\n",
        "\n",
        "    return output"
      ],
      "metadata": {
        "id": "przHl9EB6YOj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Training"
      ],
      "metadata": {
        "id": "tzBZBGjFM280"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set batch\n",
        "train_loader = DataLoader(dataset = train_dataset, batch_size=128, shuffle=True, drop_last=False)\n",
        "test_loader = DataLoader(dataset = test_dataset, batch_size=128, shuffle=True, drop_last = False)"
      ],
      "metadata": {
        "id": "BPE4BRXJC25J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model instance generation\n",
        "model = Neural_Collaborative_Filtering(n_users = len(lbl_user.classes_), n_movies = len(lbl_movie.classes_)).to(device)\n",
        "\n",
        "# Optimizer, Objective function\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
        "loss_func = nn.MSELoss(reduction= 'mean')"
      ],
      "metadata": {
        "id": "gCzxw4QtDcC7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 10\n",
        "total_loss = 0\n",
        "iter_cnt = 0\n",
        "all_losses_list = []\n",
        "\n",
        "model.train()\n",
        "\n",
        "for epoch in range(1, epochs+1):\n",
        "  total_loss = 0\n",
        "  epoch_check = 0\n",
        "  for i, train_data in enumerate(train_loader):\n",
        "    '''\n",
        "    train_data = {'users':[], 'items':[], 'ratings':[]}\n",
        "    '''\n",
        "    batch_size = len(train_data['users'])\n",
        "    prediction = model(train_data['users'], train_data['movies'])\n",
        "    ground_truth = train_data['ratings'].view(batch_size,-1).to(torch.float32)\n",
        "\n",
        "    loss = loss_func(prediction, ground_truth)\n",
        "    total_loss = total_loss + (loss.item() * batch_size)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    iter_cnt = iter_cnt + 1\n",
        "    epoch_check += batch_size\n",
        "\n",
        "    if iter_cnt % 100 == 0 and iter_cnt != 0:\n",
        "      avg_iter_loss = loss.item()\n",
        "      batch_num = int((iter_cnt/100) % 7) if int((iter_cnt/100) % 7) else 7\n",
        "      print(f\"epoch {epoch} - (batch {batch_num}) loss : {(avg_iter_loss)}\")\n",
        "\n",
        "\n",
        "    if epoch_check % (batch_size * len(train_loader)) == 0 and epoch_check != 0:\n",
        "      avg_loss = total_loss / epoch_check\n",
        "      print(f\"Epoch {epoch} Avg_loss : {avg_loss}\")\n",
        "      all_losses_list.append(avg_loss)\n"
      ],
      "metadata": {
        "id": "Q4w-0UPwFVIp",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show Loss change\n",
        "plt.figure()\n",
        "\n",
        "title_font = {\n",
        "    'fontsize': 16,\n",
        "    'fontweight': 'bold'\n",
        "}\n",
        "\n",
        "plt.title('Loss change',fontdict=title_font)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "\n",
        "plt.plot(all_losses_list)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xehKsp1ZLlm6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Evaluation"
      ],
      "metadata": {
        "id": "Pyfd2hZKM-J5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RMSE"
      ],
      "metadata": {
        "id": "UK1-0b21cYsa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "model_output_list = []\n",
        "target_rating_list = []\n",
        "\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "  for i, batched_data in enumerate(test_loader):\n",
        "    model_output = model(batched_data['users'],batched_data['movies'])\n",
        "    model_output_batch = model_output.cpu().numpy().squeeze(axis=1).tolist()\n",
        "    model_output_list += (model_output_batch)\n",
        "\n",
        "    target_rating = batched_data['ratings']\n",
        "    target_rating_batch = target_rating.cpu().numpy().tolist()\n",
        "    target_rating_list += target_rating_batch\n",
        "\n",
        "mse = mean_squared_error(target_rating_list, model_output_list)\n",
        "rms = np.sqrt(mse)\n",
        "print(f\"rms: {rms}\")"
      ],
      "metadata": {
        "id": "OPalxv4TKVuC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Recall@k & Precision@k"
      ],
      "metadata": {
        "id": "cjwQHcTJcb-6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "user_est_true = defaultdict(list)\n",
        "\n",
        "with torch.no_grad():\n",
        "  for i, batched_data in enumerate(test_loader):\n",
        "    users = batched_data['users']\n",
        "    movies = batched_data['movies']\n",
        "    ratings = batched_data['ratings']\n",
        "\n",
        "    model_output = model(batched_data['users'], batched_data[\"movies\"])\n",
        "\n",
        "    for i in range(len(users)):\n",
        "      user_id = users[i].item()\n",
        "      movie_id = movies[i].item()\n",
        "      pred_rating = model_output[i][0].item()\n",
        "      true_rating = ratings[i].item()\n",
        "\n",
        "      user_est_true[user_id].append((pred_rating, true_rating))"
      ],
      "metadata": {
        "id": "RkfNe6fINiJf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  precisions = dict()\n",
        "  recalls = dict()\n",
        "\n",
        "  # recall@K\n",
        "  k= 10\n",
        "  threshold = 3.5 # relevant item criterion\n",
        "\n",
        "  for user_id, user_ratings in user_est_true.items():\n",
        "    user_ratings.sort(key=lambda x: x[0], reverse =True)\n",
        "\n",
        "    # get the number for real relevant items = denominator of recall@k\n",
        "    n_real_relevant= sum((true_r >= threshold) for (_, true_r) in user_ratings)\n",
        "\n",
        "    # k recommended ratings\n",
        "    recommended_k = user_ratings[:k]\n",
        "\n",
        "    # get the number of recommented item that is actually relevant with real relevant.\n",
        "    n_real_relevant_in_top_k = sum((true_r >= threshold) for (est, true_r) in recommended_k)\n",
        "\n",
        "    # precision@k\n",
        "    precisions[user_id] = n_real_relevant_in_top_k / 10\n",
        "    # recall@k\n",
        "    if n_real_relevant:\n",
        "      recalls[user_id] = n_real_relevant_in_top_k / n_real_relevant\n",
        "\n",
        "    else:\n",
        "      recalls[user_id] = 0\n"
      ],
      "metadata": {
        "id": "BSfWNw5URxtD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Precision and recall can then be averaged over all users\n",
        "print(f\"precision @ {k}: {sum(prec for prec in precisions.values()) / len(precisions)}\")\n",
        "\n",
        "print(f\"recall @ {k} : {sum(rec for rec in recalls.values()) / len(recalls)}\")"
      ],
      "metadata": {
        "id": "L97FW56wXwKx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "bENzgIxt6VLF"
      }
    }
  ]
}