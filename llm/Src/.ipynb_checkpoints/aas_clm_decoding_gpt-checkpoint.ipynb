{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "484e7614",
   "metadata": {
    "id": "484e7614"
   },
   "source": [
    "# Text Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552c0ec7",
   "metadata": {
    "id": "552c0ec7"
   },
   "source": [
    "사전학습된 CLM(Causal Language Model)을 이용하여 자연어 문장을 생성하는 방법에 대해 살펴 보겠습니다.  \n",
    "GPT2-XL은 GPT2의 1.5B 파라미터 버전으로 트랜스포머 기반의 CLM 입니다.  \n",
    "Greedy Search Decoding, Beam Search Decoding, Random Sampling, Top-K/Top-P Sampling 방법 등을 실습합니다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6686293",
   "metadata": {
    "id": "f6686293"
   },
   "source": [
    "## 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8a65e53",
   "metadata": {
    "id": "b8a65e53"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HF_HOME'] = 'D:/HF/cache'\n",
    "os.environ['HF_DATASETS'] = 'D:/HF/datasets'\n",
    "os.environ['HF_HUB_DISABLE_SYMLINKS_WARNING'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96f9020",
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "d96f9020",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==4.38.2\n",
      "  Downloading transformers-4.38.2-py3-none-any.whl.metadata (130 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.7/130.7 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.38.2) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers==4.38.2) (0.26.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.38.2) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.38.2) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.38.2) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.38.2) (2024.9.11)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.38.2) (2.32.3)\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers==4.38.2)\n",
      "  Downloading tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.38.2) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.38.2) (4.66.6)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.38.2) (2024.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.38.2) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.38.2) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.38.2) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.38.2) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.38.2) (2024.8.30)\n",
      "Downloading transformers-4.38.2-py3-none-any.whl (8.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m40.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m46.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tokenizers, transformers\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.20.3\n",
      "    Uninstalling tokenizers-0.20.3:\n",
      "      Successfully uninstalled tokenizers-0.20.3\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.46.3\n",
      "    Uninstalling transformers-4.46.3:\n",
      "      Successfully uninstalled transformers-4.46.3\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "sentence-transformers 3.2.1 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.38.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed tokenizers-0.15.2 transformers-4.38.2\n"
     ]
    }
   ],
   "source": [
    "# !pip install transformers==4.51.1\n",
    "# !pip install datasets==3.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3773fc08-0a3d-4424-bf0e-09a7c3263d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "from tqdm.notebook import tqdm as notebook_tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92aff98e-7c83-4448-8d6c-dbef27c1ff0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Aug  1 11:01:50 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 553.62                 Driver Version: 553.62         CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                     TCC/WDDM  | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A10-24Q               WDDM  |   00000002:00:00.0 Off |                    0 |\n",
      "| N/A    0C    P8             N/A /  N/A  |    1230MiB /  24512MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A      4072    C+G   ...oogle\\Chrome\\Application\\chrome.exe      N/A      |\n",
      "|    0   N/A  N/A      8744    C+G   ...nt.CBS_cw5n1h2txyewy\\SearchHost.exe      N/A      |\n",
      "|    0   N/A  N/A     11044    C+G   ...2txyewy\\StartMenuExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A     12088    C+G   C:\\Windows\\explorer.exe                     N/A      |\n",
      "|    0   N/A  N/A     12096    C+G   C:\\Windows\\System32\\ShellHost.exe           N/A      |\n",
      "|    0   N/A  N/A     15184    C+G   ...CBS_cw5n1h2txyewy\\TextInputHost.exe      N/A      |\n",
      "|    0   N/A  N/A     15456    C+G   ...cal\\Microsoft\\OneDrive\\OneDrive.exe      N/A      |\n",
      "|    0   N/A  N/A     15700    C+G   ...ekyb3d8bbwe\\PhoneExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A     16944    C+G   ...__8wekyb3d8bbwe\\WindowsTerminal.exe      N/A      |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a24d27",
   "metadata": {
    "id": "13a24d27"
   },
   "source": [
    "### **GPT2-XL**: 48-layer, 1600-hidden, 25-heads, 1,558M parameters, English model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12d65463",
   "metadata": {
    "id": "12d65463"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d4550e3adca4a5c9ceef05959c34448",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/6.43G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "368b0e68c3af459d856ceb324a3c5e02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_ckpt = \"gpt2-xl\"\n",
    "\n",
    "# [실습] 다음 코드를 완성하세요!!\n",
    "# 사전학습 모델 'gpt2-xl'에 사용된 Tokenizer를 가져옵니다.\n",
    "# Causal LM 'gpt2-xl'을 device로 가져옵니다.\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_ckpt).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ccc3eaf",
   "metadata": {
    "id": "0ccc3eaf"
   },
   "source": [
    "## 1. Greedy Search Decoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39bc3e37",
   "metadata": {
    "id": "39bc3e37"
   },
   "source": [
    "가장 간단한 디코딩 방식은 각 타임스텝에서 가장 확률이 높은 토큰만을 선택하는 방법입니다.  \n",
    "Generate() 함수가 있지만, LLM을 이용한 텍스트 생성 과정을 이해하기 위해 직접 구현해 보겠습니다.  \n",
    "매 타임스텝마다 마지막 토큰에 대한 Logit을 선택하고, SoftMax를 통해 확률값을 얻을 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b9fb0dd",
   "metadata": {
    "id": "0b9fb0dd"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "input_txt = \"Transformers are the\"\n",
    "input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "iterations = []\n",
    "\n",
    "n_steps = 8\n",
    "choices_per_step = 5\n",
    "\n",
    "with torch.no_grad():\n",
    "    for _ in range(n_steps):\n",
    "        iteration = dict()\n",
    "        iteration[\"Input\"] = tokenizer.decode(input_ids[0])\n",
    "        output = model(input_ids=input_ids)\n",
    "        # 첫번째 배치의 마지막 토큰의 로짓을 선택해 소프트맥스를 적용합니다.\n",
    "        next_token_logits = output.logits[0, -1, :]\n",
    "        next_token_probs = torch.softmax(next_token_logits, dim=-1)\n",
    "        sorted_ids = torch.argsort(next_token_probs, dim=-1, descending=True)\n",
    "        # 가장 높은 확률의 토큰을 저장합니다.\n",
    "        for choice_idx in range(choices_per_step):\n",
    "            token_id = sorted_ids[choice_idx]\n",
    "            token_prob = next_token_probs[token_id].cpu().numpy()\n",
    "            token_choice = (\n",
    "                f\"{tokenizer.decode(token_id)} ({100 * token_prob: .2f}%)\"\n",
    "            )\n",
    "            iteration[f\"Choice {choice_idx+1}\"] = token_choice\n",
    "        # 예측한 다음 토큰을 입력에 추가합니다.\n",
    "        input_ids = torch.cat([input_ids, sorted_ids[None, 0, None]], dim=-1)\n",
    "        iterations.append(iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb514c96",
   "metadata": {
    "id": "eb514c96",
    "outputId": "54ebd7e5-1491-40b7-a95e-22d6d9a592ec"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Input</th>\n",
       "      <th>Choice 1</th>\n",
       "      <th>Choice 2</th>\n",
       "      <th>Choice 3</th>\n",
       "      <th>Choice 4</th>\n",
       "      <th>Choice 5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Transformers are the</td>\n",
       "      <td>most ( 8.53%)</td>\n",
       "      <td>only ( 4.96%)</td>\n",
       "      <td>best ( 4.65%)</td>\n",
       "      <td>Transformers ( 4.37%)</td>\n",
       "      <td>ultimate ( 2.16%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Transformers are the most</td>\n",
       "      <td>popular ( 16.78%)</td>\n",
       "      <td>powerful ( 5.37%)</td>\n",
       "      <td>common ( 4.96%)</td>\n",
       "      <td>famous ( 3.72%)</td>\n",
       "      <td>successful ( 3.20%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Transformers are the most popular</td>\n",
       "      <td>toy ( 10.63%)</td>\n",
       "      <td>toys ( 7.23%)</td>\n",
       "      <td>Transformers ( 6.60%)</td>\n",
       "      <td>of ( 5.46%)</td>\n",
       "      <td>and ( 3.76%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Transformers are the most popular toy</td>\n",
       "      <td>line ( 34.38%)</td>\n",
       "      <td>in ( 18.20%)</td>\n",
       "      <td>of ( 11.71%)</td>\n",
       "      <td>brand ( 6.10%)</td>\n",
       "      <td>line ( 2.69%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Transformers are the most popular toy line</td>\n",
       "      <td>in ( 46.28%)</td>\n",
       "      <td>of ( 15.09%)</td>\n",
       "      <td>, ( 4.94%)</td>\n",
       "      <td>on ( 4.40%)</td>\n",
       "      <td>ever ( 2.72%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Transformers are the most popular toy line in</td>\n",
       "      <td>the ( 65.99%)</td>\n",
       "      <td>history ( 12.42%)</td>\n",
       "      <td>America ( 6.91%)</td>\n",
       "      <td>Japan ( 2.44%)</td>\n",
       "      <td>North ( 1.40%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Transformers are the most popular toy line in the</td>\n",
       "      <td>world ( 69.26%)</td>\n",
       "      <td>United ( 4.55%)</td>\n",
       "      <td>history ( 4.29%)</td>\n",
       "      <td>US ( 4.23%)</td>\n",
       "      <td>U ( 2.30%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Transformers are the most popular toy line in ...</td>\n",
       "      <td>, ( 39.73%)</td>\n",
       "      <td>. ( 30.64%)</td>\n",
       "      <td>and ( 9.87%)</td>\n",
       "      <td>with ( 2.32%)</td>\n",
       "      <td>today ( 1.74%)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Input            Choice 1  \\\n",
       "0                               Transformers are the       most ( 8.53%)   \n",
       "1                          Transformers are the most   popular ( 16.78%)   \n",
       "2                  Transformers are the most popular       toy ( 10.63%)   \n",
       "3              Transformers are the most popular toy      line ( 34.38%)   \n",
       "4         Transformers are the most popular toy line        in ( 46.28%)   \n",
       "5      Transformers are the most popular toy line in       the ( 65.99%)   \n",
       "6  Transformers are the most popular toy line in the     world ( 69.26%)   \n",
       "7  Transformers are the most popular toy line in ...         , ( 39.73%)   \n",
       "\n",
       "             Choice 2                Choice 3                Choice 4  \\\n",
       "0       only ( 4.96%)           best ( 4.65%)   Transformers ( 4.37%)   \n",
       "1   powerful ( 5.37%)         common ( 4.96%)         famous ( 3.72%)   \n",
       "2       toys ( 7.23%)   Transformers ( 6.60%)             of ( 5.46%)   \n",
       "3        in ( 18.20%)            of ( 11.71%)          brand ( 6.10%)   \n",
       "4        of ( 15.09%)              , ( 4.94%)             on ( 4.40%)   \n",
       "5   history ( 12.42%)        America ( 6.91%)          Japan ( 2.44%)   \n",
       "6     United ( 4.55%)        history ( 4.29%)             US ( 4.23%)   \n",
       "7         . ( 30.64%)            and ( 9.87%)           with ( 2.32%)   \n",
       "\n",
       "               Choice 5  \n",
       "0     ultimate ( 2.16%)  \n",
       "1   successful ( 3.20%)  \n",
       "2          and ( 3.76%)  \n",
       "3         line ( 2.69%)  \n",
       "4         ever ( 2.72%)  \n",
       "5        North ( 1.40%)  \n",
       "6            U ( 2.30%)  \n",
       "7        today ( 1.74%)  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54999129",
   "metadata": {
    "id": "54999129",
    "outputId": "19d79218-0a9a-4d47-c2cc-924c931eeaa7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers are the most popular toy line in the world,\n"
     ]
    }
   ],
   "source": [
    "max_length = 12\n",
    "\n",
    "encoded_input = tokenizer(input_txt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# [실습] 다음 코드를 완성하세요!!\n",
    "# 입력에 대한 Greedy Search: max_length, do_sample 파라미터를 설정합니다.\n",
    "output_greedy = model.generate(**encoded_input, pad_token_id=tokenizer.eos_token_id, max_length=max_length, do_sample=False)\n",
    "print(tokenizer.decode(output_greedy[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb9e857",
   "metadata": {
    "id": "9fb9e857"
   },
   "source": [
    "## 2. Beam Search Decoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30cbbdc",
   "metadata": {
    "id": "d30cbbdc"
   },
   "source": [
    "- **log_probs_from_logits()**: 하나의 토큰에 대한 로그 확률을 제공합니다.\n",
    "- **sequence_logprob()**: 시퀀스에 대한 전체 로그 확률값을 계산합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a83f1981",
   "metadata": {
    "id": "a83f1981"
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def log_probs_from_logits(logits, labels):\n",
    "    logp = F.log_softmax(logits, dim=-1)\n",
    "    logp_label = torch.gather(logp, 2, labels.unsqueeze(2)).squeeze(-1)\n",
    "    return logp_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a895abc9",
   "metadata": {
    "id": "a895abc9"
   },
   "outputs": [],
   "source": [
    "def sequence_logprob(model, labels, input_len=0):\n",
    "    with torch.no_grad():\n",
    "        output = model(labels)\n",
    "        log_probs = log_probs_from_logits(\n",
    "            output.logits[:, :-1, :], labels[:, 1:])\n",
    "        seq_log_prob = torch.sum(log_probs[:, input_len:])\n",
    "    return seq_log_prob.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f633bfa5",
   "metadata": {
    "id": "f633bfa5"
   },
   "source": [
    "빔서치 디코딩은 확률이 가장 높은 상위 num_beam 갯수 만큼의 다음 토큰 시퀀스를 추적합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8218ee46",
   "metadata": {
    "id": "8218ee46"
   },
   "outputs": [],
   "source": [
    "max_length = 128\n",
    "input_txt = \"\"\"In a shocking finding, scientist discovered \\\n",
    "a herd of unicorns living in a remote, previously unexplored \\\n",
    "valley, in the Andes Mountains. Even more surprising to the \\\n",
    "researchers was the fact that the unicorns spoke perfect English.\\n\\n\n",
    "\"\"\"\n",
    "encoded_input = tokenizer(input_txt, return_tensors=\"pt\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57d06f65",
   "metadata": {
    "id": "57d06f65",
    "outputId": "ea153386-32b0-41b2-ce06-42a35e8eb4f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n",
      "\n",
      "\n",
      "The discovery of the unicorns was made by a team of scientists from the University of California, Santa Cruz, and the National Geographic Society.\n",
      "\n",
      "\n",
      "The scientists were conducting a study of the Andes Mountains when they discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English\n",
      "\n",
      "Log Probability: -55.23\n"
     ]
    }
   ],
   "source": [
    "# [실습] 다음 코드를 완성하세요!!\n",
    "# 입력에 대한 Beam Search: max_length, num_beam, do_sample 파라미터 등을 설정하세요.\n",
    "output_beam = model.generate(**encoded_input, pad_token_id=tokenizer.eos_token_id, max_length=max_length, num_beams=5, do_sample=False)\n",
    "logp = sequence_logprob(model, output_beam, input_len=len(encoded_input[\"input_ids\"][0]))\n",
    "\n",
    "print(tokenizer.decode(output_beam[0]))\n",
    "print(f\"\\nLog Probability: {logp:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1fd5966",
   "metadata": {
    "id": "d1fd5966"
   },
   "source": [
    "- **no_repeat_ngram_size**: 동일한 텍스트가 반복 생성되는 문제를 해결하기 위하여 n-gram penalty를 부과할 수도 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a1f5d4e7",
   "metadata": {
    "id": "a1f5d4e7",
    "outputId": "e97fb0a7-da87-4b11-ef5b-deaf9989df9b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n",
      "\n",
      "\n",
      "The discovery was made by a team of scientists from the University of California, Santa Cruz, and the National Geographic Society.\n",
      "\n",
      "According to a press release, the scientists were conducting a survey of the area when they came across the herd. They were surprised to find that they were able to converse with the animals in English, even though they had never seen a unicorn in person before. The researchers were\n",
      "\n",
      "Log Probability: -93.12\n"
     ]
    }
   ],
   "source": [
    "# [실습] 다음 코드를 완성하세요!!\n",
    "# 이전 Beam Search 결과에 no_repeat_ngram_size 설정 추가\n",
    "output_beam = model.generate(**encoded_input, pad_token_id=tokenizer.eos_token_id, max_length=max_length, num_beams=5, do_sample=False, no_repeat_ngram_size=2)\n",
    "logp = sequence_logprob(model, output_beam, input_len=len(encoded_input[\"input_ids\"][0]))\n",
    "\n",
    "print(tokenizer.decode(output_beam[0]))\n",
    "print(f\"\\nLog Probability: {logp:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0af2c04",
   "metadata": {
    "id": "d0af2c04"
   },
   "source": [
    "## 3. Random Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819c7ccc",
   "metadata": {
    "id": "819c7ccc"
   },
   "source": [
    "각 타임스텝 내 모델이 출력한 전체 어휘사전에 대한 확률분포에서 랜덤하게 샘플링하는 방법입니다.  \n",
    "- **temperature**: 소프트맥스 함수를 적용하기 전에 로짓의 스케일을 조정하는 Temperature 파라미터를 추가하면 출력의 다양성을 제어할 수 있습니다.  \n",
    "T << 1 일때 낮은 확률의 토큰들을 억제하며, T >> 1 일때는 분포가 평평해져서 각 토큰의 확률들이 동일해집니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a242a2ea",
   "metadata": {
    "id": "a242a2ea",
    "outputId": "3dd617f9-624b-494d-88d6-8a857957ecce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n",
      "\n",
      "\n",
      "However, these researchers, who studied this ancient species known with the term \"Equidistadico\"), not only did not recognize them by just hearing but could tell if one's vocalizations actually resonating properly through an instrument known just call this group that of The Language Of The Gods... and of course the animals are definitely more civilized then most horses\n",
      "\n",
      "\n",
      "Scientists believe that because no man has dared tread\n"
     ]
    }
   ],
   "source": [
    "# [실습] 다음 코드를 완성하세요!!\n",
    "# Random Sampleing 방법을 통한 문장 생성: max_length, do_sample, temperature 파라미터를 설정하세요.\n",
    "output_temp = model.generate(**encoded_input, pad_token_id=tokenizer.eos_token_id, max_length=max_length, do_sample=True, temperature=2.0)\n",
    "\n",
    "print(tokenizer.decode(output_temp[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a50f7b42",
   "metadata": {
    "id": "a50f7b42",
    "outputId": "066c7642-341c-47bf-8651-b48e1d6730cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n",
      "\n",
      "\n",
      "The researchers, from the University of California, Riverside, studied the rare population of the animals, called the Andean unicorn, and found that they are actually the largest group of unicorns in the world.\n",
      "\n",
      "\n",
      "The research team, led by Dr. Elizabeth Blackburn, studied the animals, and found that they are the largest herd of unicorns in the world. They also discovered that the animals have a\n"
     ]
    }
   ],
   "source": [
    "# [실습] 다음 코드를 완성하세요!!\n",
    "# Temperature를 변경하여 문장 생성: max_length, do_sample, temperature 파라미터를 설정하세요.\n",
    "output_temp = model.generate(**encoded_input, pad_token_id=tokenizer.eos_token_id, max_length=max_length, do_sample=True, temperature=0.5)\n",
    "\n",
    "print(tokenizer.decode(output_temp[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913df480",
   "metadata": {
    "id": "913df480"
   },
   "source": [
    "## 4. Top-K and Top-P Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c192b2",
   "metadata": {
    "id": "78c192b2"
   },
   "source": [
    "두 방법 모두 샘플링에 사용할 토큰의 갯수를 줄인다는 개념에 기초하고 있습니다.  \n",
    "\n",
    "- Top-K 샘플링: 확률이 가장 높은 K개 토큰에서만 샘플링하고 확률이 낮은 토큰을 제외함으로써,\n",
    "확률 분포의 롱테일을 잘라내고 확률이 가장 높은 토큰에서만 샘플링하는 방법입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "64099b54",
   "metadata": {
    "id": "64099b54",
    "outputId": "0aae3add-3539-4bcf-c3c1-70569c917666"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n",
      "\n",
      "\n",
      "Since this unique research was done in the 1970s, it may be difficult to find the unicorns to do another study, especially if these creatures are still roaming around. \"People have found little pockets of the unicorns in Ecuador, Colombia, Venezuela, and Bolivia, but I have been searching since 1972 for an area that might hold an entire herd,\" said Dr. Carl Schaller, the head\n"
     ]
    }
   ],
   "source": [
    "# [실습] 다음 코드를 완성하세요!!\n",
    "# Top-K 샘플링을 적용하여 문장 생성: max_length, do_sample, top_k 파라미터를 설정하세요.\n",
    "output_topk = model.generate(**encoded_input, pad_token_id=tokenizer.eos_token_id, max_length=max_length, do_sample=True, top_k=50)\n",
    "\n",
    "print(tokenizer.decode(output_topk[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6448e0",
   "metadata": {
    "id": "1f6448e0"
   },
   "source": [
    "- Top-P 샘플링: 고정된 컷오프 값을 사용하지 않고, 어디서 컷오프할 것인지 확률질량(Probability Mass) 조건을 지정합니다.  \n",
    "모든 토큰을 확률에 따라 내림차순으로 정렬하고, 누적 확률값에 도달할 때까지 토큰들을 하나씩 추가하게 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e808654e",
   "metadata": {
    "id": "e808654e",
    "outputId": "fbc5c412-2300-4c06-befc-b080c49a2f75"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n",
      "\n",
      "\n",
      "The species was named 'Andean unicorn' in honor of the mountain range. A study was then undertaken to find the source of the mysterious creatures and the results proved to be fascinating.\n",
      "\n",
      "\n",
      "The creatures were spotted by a biologist in the mountains in Ecuador in 2005 when he spotted them coming out of the undergrowth.\n",
      "\n",
      "\n",
      "They were filmed by a camera trap which recorded the animals talking and how they\n"
     ]
    }
   ],
   "source": [
    "# [실습] 다음 코드를 완성하세요!!\n",
    "# Top-P 샘플링을 적용하여 문장 생성: max_length, do_sample, top_k 파라미터를 설정하세요.\n",
    "output_topp = model.generate(**encoded_input, pad_token_id=tokenizer.eos_token_id, max_length=max_length, do_sample=True, top_p=0.90)\n",
    "\n",
    "print(tokenizer.decode(output_topp[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f47ef4",
   "metadata": {
    "id": "29f47ef4"
   },
   "source": [
    "- Ref. Natural Language Processing with Transformers"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "",
   "version": ""
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
