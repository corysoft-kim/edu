{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "484e7614",
   "metadata": {
    "id": "484e7614"
   },
   "source": [
    "# Text Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552c0ec7",
   "metadata": {
    "id": "552c0ec7"
   },
   "source": [
    "사전학습된 CLM(Causal Language Model)을 이용하여 자연어 문장을 생성하는 방법에 대해 살펴 보겠습니다.  \n",
    "GPT2-XL은 GPT2의 1.5B 파라미터 버전으로 트랜스포머 기반의 CLM 입니다.  \n",
    "Greedy Search Decoding, Beam Search Decoding, Random Sampling, Top-K/Top-P Sampling 방법 등을 실습합니다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6686293",
   "metadata": {
    "id": "f6686293"
   },
   "source": [
    "## 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8a65e53",
   "metadata": {
    "id": "b8a65e53"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HF_HOME'] = 'D:/HF/cache'\n",
    "os.environ['HF_DATASETS'] = 'D:/HF/datasets'\n",
    "os.environ['HF_HUB_DISABLE_SYMLINKS_WARNING'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d96f9020",
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "d96f9020"
   },
   "outputs": [],
   "source": [
    "# !pip install transformers==4.51.1\n",
    "# !pip install datasets==3.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3773fc08-0a3d-4424-bf0e-09a7c3263d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "from tqdm.notebook import tqdm as notebook_tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92aff98e-7c83-4448-8d6c-dbef27c1ff0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Aug 18 14:15:59 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 553.62                 Driver Version: 553.62         CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                     TCC/WDDM  | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A10-24Q               WDDM  |   00000002:00:00.0 Off |                    0 |\n",
      "| N/A    0C    P8             N/A /  N/A  |    1106MiB /  24512MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A      5372    C+G   ...Programs\\Microsoft VS Code\\Code.exe      N/A      |\n",
      "|    0   N/A  N/A      6888    C+G   ...__8wekyb3d8bbwe\\WindowsTerminal.exe      N/A      |\n",
      "|    0   N/A  N/A      8824    C+G   ...5n1h2txyewy\\ShellExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A     10592    C+G   ...oogle\\Chrome\\Application\\chrome.exe      N/A      |\n",
      "|    0   N/A  N/A     10828    C+G   ...oogle\\Chrome\\Application\\chrome.exe      N/A      |\n",
      "|    0   N/A  N/A     11268    C+G   C:\\Windows\\explorer.exe                     N/A      |\n",
      "|    0   N/A  N/A     11292    C+G   C:\\Windows\\System32\\ShellHost.exe           N/A      |\n",
      "|    0   N/A  N/A     12676    C+G   ...2txyewy\\StartMenuExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A     12804    C+G   ...CBS_cw5n1h2txyewy\\TextInputHost.exe      N/A      |\n",
      "|    0   N/A  N/A     12808    C+G   ...nt.CBS_cw5n1h2txyewy\\SearchHost.exe      N/A      |\n",
      "|    0   N/A  N/A     13376    C+G   ...cal\\Microsoft\\OneDrive\\OneDrive.exe      N/A      |\n",
      "|    0   N/A  N/A     14164    C+G   ...crosoft\\Edge\\Application\\msedge.exe      N/A      |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a24d27",
   "metadata": {
    "id": "13a24d27"
   },
   "source": [
    "### **GPT2-XL**: 48-layer, 1600-hidden, 25-heads, 1,558M parameters, English model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12d65463",
   "metadata": {
    "id": "12d65463"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3b1a776e9e74346890bf3edb23ed150",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/6.43G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1aac4ef0155c457c893cbd75efc7eb40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_ckpt = \"gpt2-xl\"\n",
    "\n",
    "# [실습] 다음 코드를 완성하세요!!\n",
    "# 사전학습 모델 'gpt2-xl'에 사용된 Tokenizer를 가져옵니다.\n",
    "# Causal LM 'gpt2-xl'을 device로 가져옵니다.\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_ckpt).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ccc3eaf",
   "metadata": {
    "id": "0ccc3eaf"
   },
   "source": [
    "## 1. Greedy Search Decoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39bc3e37",
   "metadata": {
    "id": "39bc3e37"
   },
   "source": [
    "가장 간단한 디코딩 방식은 각 타임스텝에서 가장 확률이 높은 토큰만을 선택하는 방법입니다.  \n",
    "Generate() 함수가 있지만, LLM을 이용한 텍스트 생성 과정을 이해하기 위해 직접 구현해 보겠습니다.  \n",
    "매 타임스텝마다 마지막 토큰에 대한 Logit을 선택하고, SoftMax를 통해 확률값을 얻을 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b9fb0dd",
   "metadata": {
    "id": "0b9fb0dd"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "input_txt = \"Transformers are the\"\n",
    "input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "iterations = []\n",
    "\n",
    "n_steps = 8\n",
    "choices_per_step = 5\n",
    "\n",
    "with torch.no_grad():\n",
    "    for _ in range(n_steps):\n",
    "        iteration = dict()\n",
    "        iteration[\"Input\"] = tokenizer.decode(input_ids[0])\n",
    "        output = model(input_ids=input_ids)\n",
    "        # 첫번째 배치의 마지막 토큰의 로짓을 선택해 소프트맥스를 적용합니다.\n",
    "        next_token_logits = output.logits[0, -1, :]\n",
    "        next_token_probs = torch.softmax(next_token_logits, dim=-1)\n",
    "        sorted_ids = torch.argsort(next_token_probs, dim=-1, descending=True)\n",
    "        # 가장 높은 확률의 토큰을 저장합니다.\n",
    "        for choice_idx in range(choices_per_step):\n",
    "            token_id = sorted_ids[choice_idx]\n",
    "            token_prob = next_token_probs[token_id].cpu().numpy()\n",
    "            token_choice = (\n",
    "                f\"{tokenizer.decode(token_id)} ({100 * token_prob: .2f}%)\"\n",
    "            )\n",
    "            iteration[f\"Choice {choice_idx+1}\"] = token_choice\n",
    "        # 예측한 다음 토큰을 입력에 추가합니다.\n",
    "        input_ids = torch.cat([input_ids, sorted_ids[None, 0, None]], dim=-1)\n",
    "        iterations.append(iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb514c96",
   "metadata": {
    "id": "eb514c96",
    "outputId": "54ebd7e5-1491-40b7-a95e-22d6d9a592ec"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Input</th>\n",
       "      <th>Choice 1</th>\n",
       "      <th>Choice 2</th>\n",
       "      <th>Choice 3</th>\n",
       "      <th>Choice 4</th>\n",
       "      <th>Choice 5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Transformers are the</td>\n",
       "      <td>most ( 8.53%)</td>\n",
       "      <td>only ( 4.96%)</td>\n",
       "      <td>best ( 4.65%)</td>\n",
       "      <td>Transformers ( 4.37%)</td>\n",
       "      <td>ultimate ( 2.16%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Transformers are the most</td>\n",
       "      <td>popular ( 16.78%)</td>\n",
       "      <td>powerful ( 5.37%)</td>\n",
       "      <td>common ( 4.96%)</td>\n",
       "      <td>famous ( 3.72%)</td>\n",
       "      <td>successful ( 3.20%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Transformers are the most popular</td>\n",
       "      <td>toy ( 10.63%)</td>\n",
       "      <td>toys ( 7.23%)</td>\n",
       "      <td>Transformers ( 6.60%)</td>\n",
       "      <td>of ( 5.46%)</td>\n",
       "      <td>and ( 3.76%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Transformers are the most popular toy</td>\n",
       "      <td>line ( 34.38%)</td>\n",
       "      <td>in ( 18.20%)</td>\n",
       "      <td>of ( 11.71%)</td>\n",
       "      <td>brand ( 6.10%)</td>\n",
       "      <td>line ( 2.69%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Transformers are the most popular toy line</td>\n",
       "      <td>in ( 46.28%)</td>\n",
       "      <td>of ( 15.09%)</td>\n",
       "      <td>, ( 4.94%)</td>\n",
       "      <td>on ( 4.40%)</td>\n",
       "      <td>ever ( 2.72%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Transformers are the most popular toy line in</td>\n",
       "      <td>the ( 65.99%)</td>\n",
       "      <td>history ( 12.42%)</td>\n",
       "      <td>America ( 6.91%)</td>\n",
       "      <td>Japan ( 2.44%)</td>\n",
       "      <td>North ( 1.40%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Transformers are the most popular toy line in the</td>\n",
       "      <td>world ( 69.26%)</td>\n",
       "      <td>United ( 4.55%)</td>\n",
       "      <td>history ( 4.29%)</td>\n",
       "      <td>US ( 4.23%)</td>\n",
       "      <td>U ( 2.30%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Transformers are the most popular toy line in ...</td>\n",
       "      <td>, ( 39.73%)</td>\n",
       "      <td>. ( 30.64%)</td>\n",
       "      <td>and ( 9.87%)</td>\n",
       "      <td>with ( 2.32%)</td>\n",
       "      <td>today ( 1.74%)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Input            Choice 1  \\\n",
       "0                               Transformers are the       most ( 8.53%)   \n",
       "1                          Transformers are the most   popular ( 16.78%)   \n",
       "2                  Transformers are the most popular       toy ( 10.63%)   \n",
       "3              Transformers are the most popular toy      line ( 34.38%)   \n",
       "4         Transformers are the most popular toy line        in ( 46.28%)   \n",
       "5      Transformers are the most popular toy line in       the ( 65.99%)   \n",
       "6  Transformers are the most popular toy line in the     world ( 69.26%)   \n",
       "7  Transformers are the most popular toy line in ...         , ( 39.73%)   \n",
       "\n",
       "             Choice 2                Choice 3                Choice 4  \\\n",
       "0       only ( 4.96%)           best ( 4.65%)   Transformers ( 4.37%)   \n",
       "1   powerful ( 5.37%)         common ( 4.96%)         famous ( 3.72%)   \n",
       "2       toys ( 7.23%)   Transformers ( 6.60%)             of ( 5.46%)   \n",
       "3        in ( 18.20%)            of ( 11.71%)          brand ( 6.10%)   \n",
       "4        of ( 15.09%)              , ( 4.94%)             on ( 4.40%)   \n",
       "5   history ( 12.42%)        America ( 6.91%)          Japan ( 2.44%)   \n",
       "6     United ( 4.55%)        history ( 4.29%)             US ( 4.23%)   \n",
       "7         . ( 30.64%)            and ( 9.87%)           with ( 2.32%)   \n",
       "\n",
       "               Choice 5  \n",
       "0     ultimate ( 2.16%)  \n",
       "1   successful ( 3.20%)  \n",
       "2          and ( 3.76%)  \n",
       "3         line ( 2.69%)  \n",
       "4         ever ( 2.72%)  \n",
       "5        North ( 1.40%)  \n",
       "6            U ( 2.30%)  \n",
       "7        today ( 1.74%)  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "54999129",
   "metadata": {
    "id": "54999129",
    "outputId": "19d79218-0a9a-4d47-c2cc-924c931eeaa7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "humain is the only one who can save the world.\n"
     ]
    }
   ],
   "source": [
    "max_length = 12\n",
    "\n",
    "input_txt2 = \"humain is the\"\n",
    "encoded_input = tokenizer(input_txt2, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# [실습] 다음 코드를 완성하세요!!\n",
    "# 입력에 대한 Greedy Search: max_length, do_sample 파라미터를 설정합니다.\n",
    "output_greedy = model.generate(**encoded_input, pad_token_id=tokenizer.eos_token_id, max_length=max_length, do_sample=False)\n",
    "print(tokenizer.decode(output_greedy[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb9e857",
   "metadata": {
    "id": "9fb9e857"
   },
   "source": [
    "## 2. Beam Search Decoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30cbbdc",
   "metadata": {
    "id": "d30cbbdc"
   },
   "source": [
    "- **log_probs_from_logits()**: 하나의 토큰에 대한 로그 확률을 제공합니다.\n",
    "- **sequence_logprob()**: 시퀀스에 대한 전체 로그 확률값을 계산합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a83f1981",
   "metadata": {
    "id": "a83f1981"
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def log_probs_from_logits(logits, labels):\n",
    "    logp = F.log_softmax(logits, dim=-1)\n",
    "    logp_label = torch.gather(logp, 2, labels.unsqueeze(2)).squeeze(-1)\n",
    "    return logp_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a895abc9",
   "metadata": {
    "id": "a895abc9"
   },
   "outputs": [],
   "source": [
    "def sequence_logprob(model, labels, input_len=0):\n",
    "    with torch.no_grad():\n",
    "        output = model(labels)\n",
    "        log_probs = log_probs_from_logits(\n",
    "            output.logits[:, :-1, :], labels[:, 1:])\n",
    "        seq_log_prob = torch.sum(log_probs[:, input_len:])\n",
    "    return seq_log_prob.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f633bfa5",
   "metadata": {
    "id": "f633bfa5"
   },
   "source": [
    "빔서치 디코딩은 확률이 가장 높은 상위 num_beam 갯수 만큼의 다음 토큰 시퀀스를 추적합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8218ee46",
   "metadata": {
    "id": "8218ee46"
   },
   "outputs": [],
   "source": [
    "max_length = 128\n",
    "input_txt = \"\"\"In a shocking finding, scientist discovered \\\n",
    "a herd of unicorns living in a remote, previously unexplored \\\n",
    "valley, in the Andes Mountains. Even more surprising to the \\\n",
    "researchers was the fact that the unicorns spoke perfect English.\\n\\n\n",
    "\"\"\"\n",
    "encoded_input = tokenizer(input_txt, return_tensors=\"pt\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "57d06f65",
   "metadata": {
    "id": "57d06f65",
    "outputId": "ea153386-32b0-41b2-ce06-42a35e8eb4f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n",
      "\n",
      "\n",
      "The discovery of the unicorns was made by a team of scientists from the University of California, Santa Cruz, and the National Geographic Society.\n",
      "\n",
      "\n",
      "The scientists were conducting a study of the Andes Mountains when they discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English\n",
      "\n",
      "Log Probability: -55.23\n"
     ]
    }
   ],
   "source": [
    "# [실습] 다음 코드를 완성하세요!!\n",
    "# 입력에 대한 Beam Search: max_length, num_beam, do_sample 파라미터 등을 설정하세요.\n",
    "output_beam = model.generate(**encoded_input, pad_token_id=tokenizer.eos_token_id, max_length=max_length, num_beams=5, do_sample=False)\n",
    "logp = sequence_logprob(model, output_beam, input_len=len(encoded_input[\"input_ids\"][0]))\n",
    "\n",
    "print(tokenizer.decode(output_beam[0]))\n",
    "print(f\"\\nLog Probability: {logp:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1fd5966",
   "metadata": {
    "id": "d1fd5966"
   },
   "source": [
    "- **no_repeat_ngram_size**: 동일한 텍스트가 반복 생성되는 문제를 해결하기 위하여 n-gram penalty를 부과할 수도 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a1f5d4e7",
   "metadata": {
    "id": "a1f5d4e7",
    "outputId": "e97fb0a7-da87-4b11-ef5b-deaf9989df9b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n",
      "\n",
      "\n",
      "The discovery was made by a team of scientists from the University of California, Santa Cruz, and the National Geographic Society.\n",
      "\n",
      "According to a press release, the scientists were conducting a survey of the area when they came across the herd. They were surprised to find that they were able to converse with the animals in English, even though they had never seen a unicorn in person before. The researchers were\n",
      "\n",
      "Log Probability: -93.12\n"
     ]
    }
   ],
   "source": [
    "# [실습] 다음 코드를 완성하세요!!\n",
    "# 이전 Beam Search 결과에 no_repeat_ngram_size 설정 추가\n",
    "output_beam = model.generate(**encoded_input, pad_token_id=tokenizer.eos_token_id, max_length=max_length, num_beams=5, do_sample=False, no_repeat_ngram_size=2)\n",
    "logp = sequence_logprob(model, output_beam, input_len=len(encoded_input[\"input_ids\"][0]))\n",
    "\n",
    "print(tokenizer.decode(output_beam[0]))\n",
    "print(f\"\\nLog Probability: {logp:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0af2c04",
   "metadata": {
    "id": "d0af2c04"
   },
   "source": [
    "## 3. Random Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819c7ccc",
   "metadata": {
    "id": "819c7ccc"
   },
   "source": [
    "각 타임스텝 내 모델이 출력한 전체 어휘사전에 대한 확률분포에서 랜덤하게 샘플링하는 방법입니다.  \n",
    "- **temperature**: 소프트맥스 함수를 적용하기 전에 로짓의 스케일을 조정하는 Temperature 파라미터를 추가하면 출력의 다양성을 제어할 수 있습니다.  \n",
    "T << 1 일때 낮은 확률의 토큰들을 억제하며, T >> 1 일때는 분포가 평평해져서 각 토큰의 확률들이 동일해집니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a242a2ea",
   "metadata": {
    "id": "a242a2ea",
    "outputId": "3dd617f9-624b-494d-88d6-8a857957ecce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n",
      "\n",
      "\n",
      "Experts believe a high herd of elephants are a key in supporting an animal group like unicorn in maintaining stability and increasing food supply in the Andes forest as we discover in scientific research done recently in California to solve the problem and improve ecological environment. The study led a high altitude group the group of research assistants was assigned from Bolsana Biological Station to research, during their days visit San Francisco bay in May through\n"
     ]
    }
   ],
   "source": [
    "# [실습] 다음 코드를 완성하세요!!\n",
    "# Random Sampleing 방법을 통한 문장 생성: max_length, do_sample, temperature 파라미터를 설정하세요.\n",
    "output_temp = model.generate(**encoded_input, pad_token_id=tokenizer.eos_token_id, max_length=max_length, do_sample=True, temperature=2.0)\n",
    "\n",
    "print(tokenizer.decode(output_temp[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a50f7b42",
   "metadata": {
    "id": "a50f7b42",
    "outputId": "066c7642-341c-47bf-8651-b48e1d6730cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n",
      "\n",
      "\n",
      "The researchers from the University of Bristol, who made the discovery, have named the herd of unicorns \"Pelagornis robustus\".\n",
      "\n",
      "The researchers have named the herd of unicorns \"Pelagornis robustus\".\n",
      "\n",
      "The scientists were able to locate the unicorns by using a radio transmitter to detect the unicorns' unique calls.\n",
      "\n",
      "The scientists were able to\n"
     ]
    }
   ],
   "source": [
    "# [실습] 다음 코드를 완성하세요!!\n",
    "# Temperature를 변경하여 문장 생성: max_length, do_sample, temperature 파라미터를 설정하세요.\n",
    "output_temp = model.generate(**encoded_input, pad_token_id=tokenizer.eos_token_id, max_length=max_length, do_sample=True, temperature=0.5)\n",
    "\n",
    "print(tokenizer.decode(output_temp[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913df480",
   "metadata": {
    "id": "913df480"
   },
   "source": [
    "## 4. Top-K and Top-P Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c192b2",
   "metadata": {
    "id": "78c192b2"
   },
   "source": [
    "두 방법 모두 샘플링에 사용할 토큰의 갯수를 줄인다는 개념에 기초하고 있습니다.  \n",
    "\n",
    "- Top-K 샘플링: 확률이 가장 높은 K개 토큰에서만 샘플링하고 확률이 낮은 토큰을 제외함으로써,\n",
    "확률 분포의 롱테일을 잘라내고 확률이 가장 높은 토큰에서만 샘플링하는 방법입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "64099b54",
   "metadata": {
    "id": "64099b54",
    "outputId": "0aae3add-3539-4bcf-c3c1-70569c917666"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n",
      "\n",
      "\n",
      "One of the most surprising facts about the discovery was how close the unicorns lived to humans. As the group of researchers ventured down the mountain by foot, they encountered the herd of unicorns.\n",
      "\n",
      "\n",
      "Although all the members of the herd had distinct appearances, each one seemed to have the same size and the same color.\n",
      "\n",
      "\n",
      "Scientists believe that the unusual appearance of these unicorns may be attributed to\n"
     ]
    }
   ],
   "source": [
    "# [실습] 다음 코드를 완성하세요!!\n",
    "# Top-K 샘플링을 적용하여 문장 생성: max_length, do_sample, top_k 파라미터를 설정하세요.\n",
    "output_topk = model.generate(**encoded_input, pad_token_id=tokenizer.eos_token_id, max_length=max_length, do_sample=True, top_k=50)\n",
    "\n",
    "print(tokenizer.decode(output_topk[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6448e0",
   "metadata": {
    "id": "1f6448e0"
   },
   "source": [
    "- Top-P 샘플링: 고정된 컷오프 값을 사용하지 않고, 어디서 컷오프할 것인지 확률질량(Probability Mass) 조건을 지정합니다.  \n",
    "모든 토큰을 확률에 따라 내림차순으로 정렬하고, 누적 확률값에 도달할 때까지 토큰들을 하나씩 추가하게 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e808654e",
   "metadata": {
    "id": "e808654e",
    "outputId": "fbc5c412-2300-4c06-befc-b080c49a2f75"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n",
      "\n",
      "\n",
      "Researchers believe that the unicorns are the descendants of the unicorns who escaped the Spanish Inquisition.\n",
      "\n",
      "\n",
      "The scientists have been conducting scientific tests to study the animals' language.\n",
      "\n",
      "\n",
      "The unicorns are thought to be the last survivors of the extinct species that once roamed the Earth, and are the only living creatures capable of understanding human speech.\n",
      "\n",
      "\n",
      "They were discovered when researchers flew over the mountains\n"
     ]
    }
   ],
   "source": [
    "# [실습] 다음 코드를 완성하세요!!\n",
    "# Top-P 샘플링을 적용하여 문장 생성: max_length, do_sample, top_k 파라미터를 설정하세요.\n",
    "output_topp = model.generate(**encoded_input, pad_token_id=tokenizer.eos_token_id, max_length=max_length, do_sample=True, top_p=0.8)\n",
    "\n",
    "print(tokenizer.decode(output_topp[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f47ef4",
   "metadata": {
    "id": "29f47ef4"
   },
   "source": [
    "- Ref. Natural Language Processing with Transformers"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "",
   "version": ""
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
