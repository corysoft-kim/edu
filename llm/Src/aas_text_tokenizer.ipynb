{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6324095b",
   "metadata": {},
   "source": [
    "# Text Tokenizer\n",
    "\n",
    "자연어처리에 있어 토크나이저는 문장을 작은 단위의 토큰(단어 또는 부분단어)으로 분할하는 역할을 하며, \n",
    "본 실습에서는 대표적인 SubWord Tokenizer 방법들에 대해 살펴보도록 하겠습니다.  \n",
    "- BPE Tokenization (Algorithm) \n",
    "- Pretrained Tokenizer (WordPiece)  \n",
    "- Tokenizer Training "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a43cad",
   "metadata": {},
   "source": [
    "## 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b00614ee-2823-4d86-a888-37dff5acf359",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HF_HOME'] = 'D:/HF/cache'\n",
    "os.environ['HF_DATASETS'] = 'D:/HF/datasets'\n",
    "os.environ['HF_HUB_DISABLE_SYMLINKS_WARNING'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd10d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --user transformers==4.51.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "527dc457-2918-4f16-9113-3ed3d252a4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "from tqdm.notebook import tqdm as notebook_tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d89def1-d9ad-40f6-abb6-39d4e895e530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Aug 18 10:55:02 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 553.62                 Driver Version: 553.62         CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                     TCC/WDDM  | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A10-24Q               WDDM  |   00000002:00:00.0 Off |                    0 |\n",
      "| N/A    0C    P8             N/A /  N/A  |    1110MiB /  24512MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A      5372    C+G   ...Programs\\Microsoft VS Code\\Code.exe      N/A      |\n",
      "|    0   N/A  N/A      6888    C+G   ...__8wekyb3d8bbwe\\WindowsTerminal.exe      N/A      |\n",
      "|    0   N/A  N/A      8824    C+G   ...5n1h2txyewy\\ShellExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A     10592    C+G   ...oogle\\Chrome\\Application\\chrome.exe      N/A      |\n",
      "|    0   N/A  N/A     10828    C+G   ...oogle\\Chrome\\Application\\chrome.exe      N/A      |\n",
      "|    0   N/A  N/A     11268    C+G   C:\\Windows\\explorer.exe                     N/A      |\n",
      "|    0   N/A  N/A     11292    C+G   C:\\Windows\\System32\\ShellHost.exe           N/A      |\n",
      "|    0   N/A  N/A     12676    C+G   ...2txyewy\\StartMenuExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A     12804    C+G   ...CBS_cw5n1h2txyewy\\TextInputHost.exe      N/A      |\n",
      "|    0   N/A  N/A     12808    C+G   ...nt.CBS_cw5n1h2txyewy\\SearchHost.exe      N/A      |\n",
      "|    0   N/A  N/A     13376    C+G   ...cal\\Microsoft\\OneDrive\\OneDrive.exe      N/A      |\n",
      "|    0   N/A  N/A     14164    C+G   ...crosoft\\Edge\\Application\\msedge.exe      N/A      |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2446900",
   "metadata": {},
   "source": [
    "## 1. BPE Tokenization (Algorithm)\n",
    "\n",
    "BPE 토크나이저는 기본 단위(단일 문자)의 리스트로 시작해서 가장 빈번하게 함께 등장하는 토큰을 합쳐 어휘사전에 새롭게 추가하는 방식으로 원하는 어휘사전 크기에 도달할 때까지 점진적으로 새 토큰 만드는 과정을 반복합니다.  \n",
    "아래 실습에서는 논문(https://arxiv.org/pdf/1508.07909.pdf) 에 언급된 알고리즘을 그대로 구현해 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b0dc651",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, collections\n",
    "from IPython.display import display, Markdown, Latex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56ce7236",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_merges = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e81d62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {'l o w </w>' : 5,\n",
    "         'l o w e r </w>' : 2,\n",
    "         'n e w e s t </w>' : 6,\n",
    "         'w i d e s t </w>' : 3\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a658f418",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Iteration 1"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequences : {('l', 'o'): 7, ('o', 'w'): 7, ('w', '</w>'): 5, ('w', 'e'): 8, ('e', 'r'): 2, ('r', '</w>'): 2, ('n', 'e'): 6, ('e', 'w'): 6, ('e', 's'): 9, ('s', 't'): 9, ('t', '</w>'): 9, ('w', 'i'): 3, ('i', 'd'): 3, ('d', 'e'): 3}\n",
      "New merge: ('e', 's')\n",
      "Vocabulary: {'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w es t </w>': 6, 'w i d es t </w>': 3}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Iteration 2"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequences : {('l', 'o'): 7, ('o', 'w'): 7, ('w', '</w>'): 5, ('w', 'e'): 2, ('e', 'r'): 2, ('r', '</w>'): 2, ('n', 'e'): 6, ('e', 'w'): 6, ('w', 'es'): 6, ('es', 't'): 9, ('t', '</w>'): 9, ('w', 'i'): 3, ('i', 'd'): 3, ('d', 'es'): 3}\n",
      "New merge: ('es', 't')\n",
      "Vocabulary: {'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w est </w>': 6, 'w i d est </w>': 3}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Iteration 3"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequences : {('l', 'o'): 7, ('o', 'w'): 7, ('w', '</w>'): 5, ('w', 'e'): 2, ('e', 'r'): 2, ('r', '</w>'): 2, ('n', 'e'): 6, ('e', 'w'): 6, ('w', 'est'): 6, ('est', '</w>'): 9, ('w', 'i'): 3, ('i', 'd'): 3, ('d', 'est'): 3}\n",
      "New merge: ('est', '</w>')\n",
      "Vocabulary: {'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w est</w>': 6, 'w i d est</w>': 3}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Iteration 4"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequences : {('l', 'o'): 7, ('o', 'w'): 7, ('w', '</w>'): 5, ('w', 'e'): 2, ('e', 'r'): 2, ('r', '</w>'): 2, ('n', 'e'): 6, ('e', 'w'): 6, ('w', 'est</w>'): 6, ('w', 'i'): 3, ('i', 'd'): 3, ('d', 'est</w>'): 3}\n",
      "New merge: ('l', 'o')\n",
      "Vocabulary: {'lo w </w>': 5, 'lo w e r </w>': 2, 'n e w est</w>': 6, 'w i d est</w>': 3}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Iteration 5"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequences : {('lo', 'w'): 7, ('w', '</w>'): 5, ('w', 'e'): 2, ('e', 'r'): 2, ('r', '</w>'): 2, ('n', 'e'): 6, ('e', 'w'): 6, ('w', 'est</w>'): 6, ('w', 'i'): 3, ('i', 'd'): 3, ('d', 'est</w>'): 3}\n",
      "New merge: ('lo', 'w')\n",
      "Vocabulary: {'low </w>': 5, 'low e r </w>': 2, 'n e w est</w>': 6, 'w i d est</w>': 3}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Iteration 6"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequences : {('low', '</w>'): 5, ('low', 'e'): 2, ('e', 'r'): 2, ('r', '</w>'): 2, ('n', 'e'): 6, ('e', 'w'): 6, ('w', 'est</w>'): 6, ('w', 'i'): 3, ('i', 'd'): 3, ('d', 'est</w>'): 3}\n",
      "New merge: ('n', 'e')\n",
      "Vocabulary: {'low </w>': 5, 'low e r </w>': 2, 'ne w est</w>': 6, 'w i d est</w>': 3}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Iteration 7"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequences : {('low', '</w>'): 5, ('low', 'e'): 2, ('e', 'r'): 2, ('r', '</w>'): 2, ('ne', 'w'): 6, ('w', 'est</w>'): 6, ('w', 'i'): 3, ('i', 'd'): 3, ('d', 'est</w>'): 3}\n",
      "New merge: ('ne', 'w')\n",
      "Vocabulary: {'low </w>': 5, 'low e r </w>': 2, 'new est</w>': 6, 'w i d est</w>': 3}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Iteration 8"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequences : {('low', '</w>'): 5, ('low', 'e'): 2, ('e', 'r'): 2, ('r', '</w>'): 2, ('new', 'est</w>'): 6, ('w', 'i'): 3, ('i', 'd'): 3, ('d', 'est</w>'): 3}\n",
      "New merge: ('new', 'est</w>')\n",
      "Vocabulary: {'low </w>': 5, 'low e r </w>': 2, 'newest</w>': 6, 'w i d est</w>': 3}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Iteration 9"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequences : {('low', '</w>'): 5, ('low', 'e'): 2, ('e', 'r'): 2, ('r', '</w>'): 2, ('w', 'i'): 3, ('i', 'd'): 3, ('d', 'est</w>'): 3}\n",
      "New merge: ('low', '</w>')\n",
      "Vocabulary: {'low</w>': 5, 'low e r </w>': 2, 'newest</w>': 6, 'w i d est</w>': 3}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Iteration 10"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequences : {('low', 'e'): 2, ('e', 'r'): 2, ('r', '</w>'): 2, ('w', 'i'): 3, ('i', 'd'): 3, ('d', 'est</w>'): 3}\n",
      "New merge: ('w', 'i')\n",
      "Vocabulary: {'low</w>': 5, 'low e r </w>': 2, 'newest</w>': 6, 'wi d est</w>': 3}\n"
     ]
    }
   ],
   "source": [
    "def get_stats(vocab):\n",
    "    # 유니그램의 pair들의 빈도수를 카운트\n",
    "    pairs = collections.defaultdict(int)\n",
    "    for word, freq in vocab.items():\n",
    "        symbols = word.split()\n",
    "        for i in range(len(symbols)-1):\n",
    "            pairs[symbols[i],symbols[i+1]] += freq\n",
    "    print('Frequences :', dict(pairs))\n",
    "    return pairs\n",
    "\n",
    "def merge_vocab(pair, v_in):\n",
    "    v_out = {}\n",
    "    bigram = re.escape(' '.join(pair))\n",
    "    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
    "    for word in v_in:\n",
    "        w_out = p.sub(''.join(pair), word)\n",
    "        v_out[w_out] = v_in[word]\n",
    "    return v_out\n",
    "\n",
    "bpe_codes = {}\n",
    "bpe_codes_reverse = {}\n",
    "\n",
    "for i in range(num_merges):\n",
    "    display(Markdown(\"### Iteration {}\".format(i + 1)))\n",
    "    pairs = get_stats(vocab)\n",
    "    best = max(pairs, key=pairs.get)\n",
    "    vocab = merge_vocab(best, vocab)\n",
    "\n",
    "    bpe_codes[best] = i\n",
    "    bpe_codes_reverse[best[0] + best[1]] = best\n",
    "\n",
    "    print(\"New merge: {}\".format(best))\n",
    "    print(\"Vocabulary: {}\".format(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232a8bf6",
   "metadata": {},
   "source": [
    "## 2. Pretrained Tokenizer (WordPiece)\n",
    "\n",
    "트랜스포머스는 사전 훈련된 언어모델을 활용하기 위하여 언어모델 구축에 사용된 토크나이저를 로딩하는 **AutoTokenizer** 클래스를 \n",
    "제공합니다.  \n",
    "이 클래스의 **.from_pretrained()** 메소드를 이용하여 모델ID나 로컬 파일 경로로 호출하면 됩니다.  \n",
    "대표적인 Sub-word Tokenizer 방식으로 WordPiece 계열의 **BERT Tokenizer**를 이용하여 실습해 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e38a996c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "102dd649d8304d18ac31d8052e2f24c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f58f440978124e5082b9419356fded89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc8bd3c160304bbbbbe5d6878221a7cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "371cbe3fe91b4b44802976e7277fb495",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['time', 'flies', 'like', 'an', 'arrow', '!']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# [실습] 다음 코드를 완성하세요!!\n",
    "# 'distilbert-base-uncased' 모델에 사용된 토크나이저를 불러옵니다.\n",
    "model_ckpt = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "\n",
    "tokenizer.tokenize(\"Time flies like an arrow!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0f15c83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30522\n",
      "512\n",
      "['input_ids', 'attention_mask']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.vocab_size)\n",
    "print(tokenizer.model_max_length)\n",
    "print(tokenizer.model_input_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e0cd1f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['token', '##izing', 'a', 'text', 'is', 'splitting', 'it', 'into', 'words', 'or', 'sub', '##words', '.']\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Tokenizing a text is splitting it into words or subwords.\"\n",
    "\n",
    "# [실습] 다음 코드를 완성하세요!!\n",
    "# tokenizer를 이용하여 sentence를 토크나이즈합니다.\n",
    "tokenized_sentence = tokenizer.tokenize(sentence)\n",
    "print(tokenized_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1a6a0be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 19204, 6026, 1037, 3793, 2003, 14541, 2009, 2046, 2616, 2030, 4942, 22104, 1012, 102]\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(sentence)\n",
    "\n",
    "encoded_sentence = inputs[\"input_ids\"]\n",
    "print(encoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a8b2597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] tokenizing a text is splitting it into words or subwords. [SEP]\n"
     ]
    }
   ],
   "source": [
    "# [실습] 다음 코드를 완성하세요!!\n",
    "# tokenizer를 이용하여 encoded_sentence를 문장으로 복원합니다.\n",
    "decoded_sentence = tokenizer.decode(encoded_sentence)\n",
    "print(decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b23df760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[101, 2051, 10029, 2066, 2019, 8612, 1012, 102, 0, 0, 0, 0, 0], [101, 2023, 6251, 2003, 2012, 2560, 2936, 2084, 1996, 6251, 1037, 1012, 102]]\n"
     ]
    }
   ],
   "source": [
    "sentence_a = \"Time flies like an arrow.\"\n",
    "sentence_b = \"This sentence is at least longer than the sentence A.\"\n",
    "\n",
    "padded_sentence = tokenizer([sentence_a, sentence_b], padding=True)\n",
    "print(padded_sentence[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7fa504de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
       " [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_sentence[\"attention_mask\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468a7e05",
   "metadata": {},
   "source": [
    "## 3. Tokenizer Training\n",
    "\n",
    "특정 언어 모델이 당신이 원하는 언어를 제대로 지원하지 못하거나, 현재 보유한 코퍼스가 언어 모델과 특성이 다른 경우, 해당 코퍼스에 적합하게 적응된 토크나이저를 이용하여 모델을 새롭게 학습하기를 원할 수도 있습니다. 이를 위해서는 그 데이터셋에 대한 새로운 토크나이저를 학습해야 합니다. 예로, 파이썬 코드를 토큰화 하기 위해 BPE 토크나이저(GPT2-XL)를 재훈련해 보도록 하겠습니다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9c5ed902",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "723c568a745643c79968e6c8c436e2c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a76c64b37edb4dd193a54b796d721416",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/689 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11c76db8568f4aabb75d855c837d31d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5928466bd6c49f9ae0eaa5a0e51fbe2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2d4d12b92e34eb5a5ee37c7ca278ccf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_ckpt = \"gpt2-xl\"\n",
    "\n",
    "old_tokenizer = AutoTokenizer.from_pretrained(model_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e98898a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 50257\n"
     ]
    }
   ],
   "source": [
    "print(f\"Vocabulary Size: {len(old_tokenizer)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6db7540",
   "metadata": {},
   "source": [
    "Python Code 이용하여 GPT2-XL 토크나이저를 테스트해 보겠습니다.  \n",
    "'Ġ', 'Ċ' 와 같은 문자가 있는 것은 GPT-2 토크나이저가 유니코드 문자가 아닌 바이트 단위로 동작하기 때문입니다.\n",
    "('Ġ': Space, 'Ċ': Line Feed,...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2122b192",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['def',\n",
       " 'Ġadd',\n",
       " '_',\n",
       " 'n',\n",
       " 'umbers',\n",
       " '(',\n",
       " 'a',\n",
       " ',',\n",
       " 'Ġb',\n",
       " '):',\n",
       " 'Ċ',\n",
       " 'Ġ',\n",
       " 'Ġ',\n",
       " 'Ġ',\n",
       " 'Ġ\"\"\"',\n",
       " 'Add',\n",
       " 'Ġthe',\n",
       " 'Ġtwo',\n",
       " 'Ġnumbers',\n",
       " 'Ġ`',\n",
       " 'a',\n",
       " '`',\n",
       " 'Ġand',\n",
       " 'Ġ`',\n",
       " 'b',\n",
       " '`',\n",
       " '.\"',\n",
       " '\"\"',\n",
       " 'Ċ',\n",
       " 'Ġ',\n",
       " 'Ġ',\n",
       " 'Ġ',\n",
       " 'Ġreturn',\n",
       " 'Ġa',\n",
       " 'Ġ+',\n",
       " 'Ġb']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = '''def add_numbers(a, b):\n",
    "    \"\"\"Add the two numbers `a` and `b`.\"\"\"\n",
    "    return a + b'''\n",
    "\n",
    "tokens = old_tokenizer.tokenize(example)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2d58c3cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyword `async` does not exist in Vocabulary!\n",
      "Keyword `await` does not exist in Vocabulary!\n",
      "Keyword `elif` does not exist in Vocabulary!\n",
      "Keyword `finally` does not exist in Vocabulary!\n",
      "Keyword `nonlocal` does not exist in Vocabulary!\n",
      "Keyword `yield` does not exist in Vocabulary!\n"
     ]
    }
   ],
   "source": [
    "import keyword\n",
    "\n",
    "for keyw in keyword.kwlist:\n",
    "    if keyw not in old_tokenizer.vocab:\n",
    "        print(f'Keyword `{keyw}` does not exist in Vocabulary!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f938d5",
   "metadata": {},
   "source": [
    "파이썬 코드 데이터(code_search_net/python)을 이용하여 토크나이저를 새로 학습하겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0acd391a",
   "metadata": {},
   "source": [
    "데이터셋 **\"code search_net\"** 는 CodeSearchNet Challenge를 위해 구축된 것으로, GitHub 상의 오픈소스 코드를 포함하고 있으며 파이썬을 포함 몇개 프로그래밍 언어로 구분되어 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fc0c4e5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0a788b5b97e4f25ad6dc540e5fb0743",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/412178 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c060c6c0fe2d41e1ab346ab8ec784f4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/22176 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20bb60fadf4249deb5920e97ac6d70f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/23107 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "raw_datasets = load_dataset(\"//swschoolavdazfiles002.file.core.windows.net/aias-language/Dataset/code_search_net\", \"python\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f10c2cf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['repository_name', 'func_path_in_repository', 'func_name', 'whole_func_string', 'language', 'func_code_string', 'func_code_tokens', 'func_documentation_string', 'func_documentation_tokens', 'split_name', 'func_code_url'],\n",
       "    num_rows: 412178\n",
       "})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "29b8f3fc-cdb5-425e-8746-e18a795006d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doapi.request\n"
     ]
    }
   ],
   "source": [
    "print(raw_datasets[\"train\"][123455][\"func_name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "24e44212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def last_rate_limit(self):\n",
      "        \"\"\"\n",
      "        A `dict` of the rate limit information returned in the most recent\n",
      "        response, or `None` if no requests have been made yet.  The `dict`\n",
      "        consists of all headers whose names begin with ``\"RateLimit\"`` (case\n",
      "        insensitive).\n",
      "\n",
      "        The DigitalOcean API specifies the following rate limit headers:\n",
      "\n",
      "        :var string RateLimit-Limit: the number of requests that can be made\n",
      "            per hour\n",
      "        :var string RateLimit-Remaining: the number of requests remaining until\n",
      "            the limit is reached\n",
      "        :var string RateLimit-Reset: the Unix timestamp for the time when the\n",
      "            oldest request will expire from rate limit consideration\n",
      "        \"\"\"\n",
      "        if self.last_response is None:\n",
      "            return None\n",
      "        else:\n",
      "            return {k:v for k,v in iteritems(self.last_response.headers)\n",
      "                        if k.lower().startswith('ratelimit')}\n"
     ]
    }
   ],
   "source": [
    "print(raw_datasets[\"train\"][123456][\"whole_func_string\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d685b215-b9b6-49e2-9b61-8ac2e742fbf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "412178"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(raw_datasets[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7c1e65f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_corpus():\n",
    "    dataset = raw_datasets[\"train\"]\n",
    "    for start_idx in range(0, len(dataset), 1000):\n",
    "        samples = dataset[start_idx : start_idx + 1000]\n",
    "        yield samples[\"whole_func_string\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9a6e6db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_corpus = get_training_corpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a2dcdd",
   "metadata": {},
   "source": [
    "기존 토크나이저와 동일한 특성을 가진 새로운 토크나이저를 학습하기 위해 ``AutoTokenizer.train_new_from_iterator()``를 사용하겠습니다.\n",
    "\n",
    "- 목표하는 Vocabulary Size 설정\n",
    "- 토크나이저 학습을 위해 입력 문자열을 공급하는 Iterator 설정\n",
    "- train_new_from_iterator() 메소드 호출\n",
    "\n",
    "토크나이저 학습에 약 2~3분 정도 소요됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "53485219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [실습] 다음 코드를 완성하세요!!\n",
    "# training_corpus를 이용하여 토크나이저를 학습합니다.\n",
    "new_tokenizer = old_tokenizer.train_new_from_iterator(training_corpus, 52000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b40b1c3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['def',\n",
       " 'Ġadd',\n",
       " '_',\n",
       " 'numbers',\n",
       " '(',\n",
       " 'a',\n",
       " ',',\n",
       " 'Ġb',\n",
       " '):',\n",
       " 'ĊĠĠĠ',\n",
       " 'Ġ\"\"\"',\n",
       " 'Add',\n",
       " 'Ġthe',\n",
       " 'Ġtwo',\n",
       " 'Ġnumbers',\n",
       " 'Ġ`',\n",
       " 'a',\n",
       " '`',\n",
       " 'Ġand',\n",
       " 'Ġ`',\n",
       " 'b',\n",
       " '`.\"\"\"',\n",
       " 'ĊĠĠĠ',\n",
       " 'Ġreturn',\n",
       " 'Ġa',\n",
       " 'Ġ+',\n",
       " 'Ġb']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = new_tokenizer.tokenize(example)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1c747073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n",
      "36\n"
     ]
    }
   ],
   "source": [
    "print(len(tokens))\n",
    "print(len(old_tokenizer.tokenize(example)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3c2cbb81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyword `nonlocal` does not exist in Vocabulary!\n"
     ]
    }
   ],
   "source": [
    "import keyword\n",
    "\n",
    "for keyw in keyword.kwlist:\n",
    "    if keyw not in new_tokenizer.vocab:\n",
    "        print(f'Keyword `{keyw}` does not exist in Vocabulary!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1626f2c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 52000\n"
     ]
    }
   ],
   "source": [
    "print(f\"Vocabulary Size: {len(new_tokenizer)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88a1b02",
   "metadata": {},
   "source": [
    "- Ref. https://huggingface.co/learn/nlp-course/chapter6"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
