{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b61c33c4",
   "metadata": {},
   "source": [
    "# ResNet Modeling\n",
    "\n",
    "## Dataset: cifar 10\n",
    "## Model: ResNet v2\n",
    "- **ResNet v1** :  [Deep Residual Learning for Image Recognition, 2015](https://arxiv.org/pdf/1512.03385.pdf)\n",
    "- **ResNet v2** : [Identity Mapping in Deep Residual Networks, 2016](https://arxiv.org/pdf/1603.05027)\n",
    "\n",
    "## Experiments\n",
    "- **ResNet v2** model로 실습, hyperparameter 변경 실습\n",
    "  * epoch \n",
    "  * ResNet block 수 n  \n",
    "  * subtract_pixel_mean = True/False 변경 \n",
    "- inference에서 test image 사용한 결과 보기\n",
    "- 끝 셀까지 실행해 주세요. 학습후 모델 저장 후 로드하여 확인까지 완료해 주세요. \n",
    "  * 저장된 모델은 다른 실습에서 활용할 예정입니다.\n",
    "\n",
    "If necessary, install scipy\n",
    "```{bash}\n",
    "!pip3 install scipy\n",
    "```\n",
    "\n",
    "\n",
    "Reference: https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Keras/blob/master/chapter2-deep-networks/resnet-cifar10-2.2.1.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0836f80e-e422-4182-b3de-b14e20f17497",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pydot --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ca5b053-10e7-4516-b183-3be097d7bd3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu May 29 12:12:17 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 553.62                 Driver Version: 553.62         CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                     TCC/WDDM  | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A10-24Q               WDDM  |   00000002:00:00.0 Off |                    0 |\n",
      "| N/A    0C    P8             N/A /  N/A  |    4043MiB /  24512MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A      2452    C+G   ...on\\136.0.3240.64\\msedgewebview2.exe      N/A      |\n",
      "|    0   N/A  N/A      2860    C+G   ..._x64__8wekyb3d8bbwe\\WebViewHost.exe      N/A      |\n",
      "|    0   N/A  N/A      4020    C+G   ...ekyb3d8bbwe\\PhoneExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A      6856    C+G   ...cal\\Microsoft\\OneDrive\\OneDrive.exe      N/A      |\n",
      "|    0   N/A  N/A      8656      C   ...er-i5\\.conda\\envs\\keras3\\python.exe      N/A      |\n",
      "|    0   N/A  N/A     10976    C+G   ...crosoft\\Edge\\Application\\msedge.exe      N/A      |\n",
      "|    0   N/A  N/A     12020    C+G   ...oogle\\Chrome\\Application\\chrome.exe      N/A      |\n",
      "|    0   N/A  N/A     12532    C+G   C:\\Windows\\System32\\ShellHost.exe           N/A      |\n",
      "|    0   N/A  N/A     12656    C+G   C:\\Windows\\explorer.exe                     N/A      |\n",
      "|    0   N/A  N/A     13288    C+G   ...__8wekyb3d8bbwe\\Notepad\\Notepad.exe      N/A      |\n",
      "|    0   N/A  N/A     13948    C+G   ...nt.CBS_cw5n1h2txyewy\\SearchHost.exe      N/A      |\n",
      "|    0   N/A  N/A     13976    C+G   ...2txyewy\\StartMenuExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A     16012    C+G   ...CBS_cw5n1h2txyewy\\TextInputHost.exe      N/A      |\n",
      "|    0   N/A  N/A     18664    C+G   ...__8wekyb3d8bbwe\\WindowsTerminal.exe      N/A      |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10aac8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf 2.9\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.layers import Dense, Conv2D, BatchNormalization, Activation\n",
    "from keras.layers import AveragePooling2D, Input, Flatten\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.regularizers import l2\n",
    "from keras.models import Model\n",
    "from keras.datasets import cifar10\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "from datetime import datetime "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "267ad0b1-2118-46d4-b326-0050fed10aab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu May 29 12:12:25 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 553.62                 Driver Version: 553.62         CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                     TCC/WDDM  | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A10-24Q               WDDM  |   00000002:00:00.0 Off |                    0 |\n",
      "| N/A    0C    P8             N/A /  N/A  |    4031MiB /  24512MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A      2452    C+G   ...on\\136.0.3240.64\\msedgewebview2.exe      N/A      |\n",
      "|    0   N/A  N/A      2860    C+G   ..._x64__8wekyb3d8bbwe\\WebViewHost.exe      N/A      |\n",
      "|    0   N/A  N/A      4020    C+G   ...ekyb3d8bbwe\\PhoneExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A      6856    C+G   ...cal\\Microsoft\\OneDrive\\OneDrive.exe      N/A      |\n",
      "|    0   N/A  N/A      8656      C   ...er-i5\\.conda\\envs\\keras3\\python.exe      N/A      |\n",
      "|    0   N/A  N/A     10976    C+G   ...crosoft\\Edge\\Application\\msedge.exe      N/A      |\n",
      "|    0   N/A  N/A     12020    C+G   ...oogle\\Chrome\\Application\\chrome.exe      N/A      |\n",
      "|    0   N/A  N/A     12532    C+G   C:\\Windows\\System32\\ShellHost.exe           N/A      |\n",
      "|    0   N/A  N/A     12656    C+G   C:\\Windows\\explorer.exe                     N/A      |\n",
      "|    0   N/A  N/A     13288    C+G   ...__8wekyb3d8bbwe\\Notepad\\Notepad.exe      N/A      |\n",
      "|    0   N/A  N/A     13948    C+G   ...nt.CBS_cw5n1h2txyewy\\SearchHost.exe      N/A      |\n",
      "|    0   N/A  N/A     13976    C+G   ...2txyewy\\StartMenuExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A     16012    C+G   ...CBS_cw5n1h2txyewy\\TextInputHost.exe      N/A      |\n",
      "|    0   N/A  N/A     18664    C+G   ...__8wekyb3d8bbwe\\WindowsTerminal.exe      N/A      |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466d5506-624b-4dde-9f9c-a6da36246ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = 'torch'\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3c3b1e3-2f47-4090-9277-f3c8751b9c4e",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'keras' has no attribute 'version'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mkeras\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m keras_major \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mversion\u001b[49m()\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m])\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'keras' has no attribute 'version'"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "\n",
    "keras_major = int(keras.version().split('.')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "657a874e-61f1-44e0-a993-c5d9d1a4c8c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras_major = int(keras.__version__.split('.')[0])\n",
    "keras_major"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36371b3-f1bf-45ba-a994-a9f6a6790976",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Conv2D, BatchNormalization, Activation\n",
    "from keras.layers import AveragePooling2D, Input, Flatten\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "# migration to keras 3.0\n",
    "#from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.src.legacy.preprocessing.image import ImageDataGenerator\n",
    "from keras.regularizers import l2\n",
    "from keras.models import Model\n",
    "from keras.datasets import cifar10\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "from datetime import datetime "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98e9579-4b75-4884-83a3-c9fd88701a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = None\n",
    "if keras_major == 3:\n",
    "    import torch\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "# device = \"cpu\"\n",
    "#device = None\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7d91f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for display image and plot\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9596bdaa",
   "metadata": {},
   "source": [
    "## 0. Configuration & Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "045965f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10 # cifar10 classes : fixed\n",
    "data_augmentation = True\n",
    "\n",
    "# Subtracting pixel mean can improve accuracy\n",
    "# Do this if True\n",
    "subtract_pixel_mean = True\n",
    "\n",
    "batch_size = 32  # 128  # original paper trained all networks with batch_size=128\n",
    "epochs = 120     # 100, 120(~55m, 91%), 150, 200(1:30m), 300(2:30m, 91%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d3509f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: ResNet-v2-20\n"
     ]
    }
   ],
   "source": [
    "# Model version\n",
    "version = 2      # fixed, 본 실습에서는 ResNet-v2 모델 사용합니다.\n",
    "\n",
    "# depth : layer 수\n",
    "# v1 n : number of residual blocks                        \n",
    "#     { 3,  5,  7,  9, ...}. cf. training time\n",
    "# v1 depth: the corresponding number of layers(depth): \n",
    "#           {20, 32, 44, 56, ... }\n",
    "# v2 n : number of residual blocks                        \n",
    "#     { 2,  3,  4,  5,  6,  7,  9, ...}. cf. training time\n",
    "# v2 depth: the corresponding number of layers(depth): \n",
    "#     {20, 29, 38, 47, 56, 65, 83, ... }\n",
    "\n",
    "n = 2 # 변경 가능하지만, 증가할 수록 학습 시간이 오래 걸립니다.\n",
    "# Computed depth from supplied model parameter n\n",
    "# 1 input Conv, last dense layer : 2 extra layers\n",
    "if version == 1:\n",
    "    # 3 stages x 2 (conv layers / ResNetBlock)  x n ((number of ResNetBlocks)/stage)\n",
    "    depth = n * 6 + 2 \n",
    "elif version == 2:\n",
    "    # 3 stage x 3 (conv layers/ResNetBlock) x n ((number of ResNetBlocks)/stage)\n",
    "    depth = n * 9 + 2 \n",
    "    \n",
    "print(f'model: ResNet-v{version}-{depth}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b45501",
   "metadata": {},
   "source": [
    "## 1. Dataset Preparation\n",
    "- Load cifar10 dataset\n",
    "- Normalize input(x) data\n",
    "- Output encoding to one-hot vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ee0ae4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (50000, 32, 32, 3)\n",
      "50000 train samples\n",
      "10000 test samples\n",
      "y_train shape: (50000, 1)\n"
     ]
    }
   ],
   "source": [
    "# Model name, depth and version\n",
    "model_type = 'ResNet%dv%d' % (depth, version)\n",
    "\n",
    "# Load the CIFAR10 data.\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# Input image dimensions.\n",
    "input_shape = x_train.shape[1:]\n",
    "\n",
    "# Normalize data.\n",
    "x_train = x_train.astype('float32') / 255\n",
    "x_test = x_test.astype('float32') / 255\n",
    "\n",
    "# If subtract pixel mean is enabled\n",
    "if subtract_pixel_mean:\n",
    "    x_train_mean = np.mean(x_train, axis=0)\n",
    "    x_train -= x_train_mean\n",
    "    x_test -= x_train_mean\n",
    "\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "print('y_train shape:', y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "110580a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert class vectors to binary class matrices.\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1abcd878",
   "metadata": {},
   "source": [
    "## 2. Modeling: ResNet\n",
    "- Using functional APIs: Add \n",
    "  * H(x) = F(x) + x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df03a653",
   "metadata": {},
   "source": [
    "### resnet_layer : a conv-layer w/ BN, activation\n",
    "- ResNet v1, v2 모델링 시에 하나의 conv layer 구성을 표현하는 데 사용, \n",
    "  conv, normalization, activation 순서에 따라 2가지 type이 있음\n",
    "  1. conv 먼저 수행 **후 normalization**하는 <span style=\"color:blue\">**post-normalization, post-activation**</span> 구조(conv_first=True)\n",
    "     * layer 구조: conv2d-layer + normalization(BN) + activation(relu)\n",
    "     * input에 대해 수행하는 첫번째 conv layer\n",
    "     * 사용예: image input 입력 받은 첫번째 conv-layer, ResNet v1 block \n",
    "  2. conv 수행 **전 normalization**하는 <span style=\"color:blue\">**pre-normalization, pre-activation**</span> 구조(conv_first=False)\n",
    "     * layer 구조: normalization(BN) + activation(relu) + conv2d-layer\n",
    "     * **v2: normalization(BN) + activation(BN) + conv-layer**  \n",
    "     * 사용시 주의점: 이후 마지막 layer에서 normalization+activation 추가 필요\n",
    "     * 사용예: ResNet v2 block (3-(conv-)layer bottleneck building block)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4253b798",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet_layer(inputs,\n",
    "                 num_filters=16,\n",
    "                 kernel_size=3,\n",
    "                 strides=1,\n",
    "                 activation='relu',\n",
    "                 batch_normalization=True,\n",
    "                 conv_first=True):\n",
    "    \"\"\"2D Convolution-Batch Normalization-Activation stack builder\n",
    "\n",
    "    # Arguments\n",
    "        inputs (tensor): input tensor from input image or previous layer\n",
    "        num_filters (int): Conv2D number of filters\n",
    "        kernel_size (int): Conv2D square kernel dimensions\n",
    "        strides (int): Conv2D square stride dimensions\n",
    "        activation (string): activation name\n",
    "        batch_normalization (bool): whether to include batch normalization\n",
    "        conv_first (bool): conv-bn-activation (True: v1) or\n",
    "            bn-activation-conv (False: v2)\n",
    "\n",
    "    # Returns\n",
    "        x (tensor): tensor as input to the next layer\n",
    "    \"\"\"\n",
    "    conv = Conv2D(num_filters,\n",
    "                  kernel_size=kernel_size,\n",
    "                  strides=strides,\n",
    "                  padding='same',\n",
    "                  kernel_initializer='he_normal',\n",
    "                  kernel_regularizer=l2(1e-4))\n",
    "\n",
    "    x = inputs\n",
    "    if conv_first: # input conv or v1\n",
    "        x = conv(x)\n",
    "        if batch_normalization:\n",
    "            x = BatchNormalization()(x)\n",
    "        if activation is not None:\n",
    "            x = Activation(activation)(x)\n",
    "    else: # v2, pre-normalization, pre-activation \n",
    "        if batch_normalization:\n",
    "            x = BatchNormalization()(x)\n",
    "        if activation is not None:\n",
    "            x = Activation(activation)(x)\n",
    "        x = conv(x) \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe24e159",
   "metadata": {},
   "source": [
    "\n",
    "## Modeling: ResNet v1\n",
    "- input block(first conv.) : Conv+BN+ReLU\n",
    "- middle blocks: 3 stages ResNet blocks (32x32 - 16x16 - 8x8)\n",
    "  * stage 0(32x32): 16 channels(no. of filters), n ResNet Blocks\n",
    "  * stage 1(16x16): 16x2 channels, n ResNet Blocks\n",
    "  * stage 2(8x8): 16x4 channels, n ResNet Blocks\n",
    "- output block(last classification layer): average pooling 2d (8)- flatten - dense(softmax, 10 classes)\n",
    "- depth = 2 + 6*n: number of weighted layers\n",
    "  * n: ResBlocks/stage\n",
    "  * input block(conv) + last dense layer : 2\n",
    "  * middle blocks: 6*n = 3 stages x 2 (conv layers/ResNet Block) * n (ResBlocks/stage)\n",
    "  \n",
    "### Residual block F(x) and projection shortcut H(x)\n",
    "1) idenity shortcut: **H(x) = F(x) + x**\n",
    "  - F(x) = f( resnet_layer, x)\n",
    "    * F(x) : 2 or 3 resnet_layers  \n",
    "        \n",
    "2) projection shortcut: **H(x) = F(x) + Conv2d(x, stride=2)**\n",
    "  - subsampling: Conv2d(stride=2)\n",
    "  - resnet_layer = Conv2d - BN - Activation(relu) \n",
    "  - resolution을 1/2로 줄일 때, stride=2(filter수는 두배씩 증가시킴) \n",
    "  - conv2d(3x3), filter size = 64 (at 32 x 32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8c216571",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet_v1(input_shape, depth, num_classes=10):\n",
    "    \"\"\"ResNet Version 1 Model builder [a]\n",
    "\n",
    "    Stacks of 2 x (3 x 3) Conv2D-BN-ReLU\n",
    "    Last ReLU is after the shortcut connection.\n",
    "    At the beginning of each stage, the feature map size is halved (downsampled)\n",
    "    by a convolutional layer with strides=2, while the number of filters is\n",
    "    doubled. Within each stage, the layers have the same number filters and the\n",
    "    same number of filters.\n",
    "    Features maps sizes:\n",
    "    stage 0: 32x32, 16\n",
    "    stage 1: 16x16, 32\n",
    "    stage 2:  8x8,  64\n",
    "    The Number of parameters is approx the same as Table 6 of [a]:\n",
    "    ResNet20 0.27M\n",
    "    ResNet32 0.46M\n",
    "    ResNet44 0.66M\n",
    "    ResNet56 0.85M\n",
    "    ResNet110 1.7M\n",
    "\n",
    "    # Arguments\n",
    "        input_shape (tensor): shape of input image tensor\n",
    "        depth (int): number of core convolutional layers\n",
    "        num_classes (int): number of classes (CIFAR10 has 10)\n",
    "\n",
    "    # Returns\n",
    "        model (Model): Keras model instance\n",
    "    \"\"\"\n",
    "    if (depth - 2) % 6 != 0:\n",
    "        raise ValueError('depth should be 6n+2 (eg 20, 32, 44 in [a])')\n",
    "    # Start model definition.\n",
    "    num_filters = 16\n",
    "    num_res_blocks = int((depth - 2) / 6)\n",
    "\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = resnet_layer(inputs=inputs)\n",
    "    # Instantiate the stack of residual units\n",
    "    for stack in range(3):\n",
    "        for res_block in range(num_res_blocks):\n",
    "            strides = 1\n",
    "            if stack > 0 and res_block == 0:  # first layer but not first stack\n",
    "                strides = 2  # downsample\n",
    "            y = resnet_layer(inputs=x,\n",
    "                             num_filters=num_filters,\n",
    "                             strides=strides)\n",
    "            y = resnet_layer(inputs=y,\n",
    "                             num_filters=num_filters,\n",
    "                             activation=None)\n",
    "            if stack > 0 and res_block == 0:  # first layer but not first stack\n",
    "                # linear projection residual shortcut connection to match\n",
    "                # changed dims\n",
    "                x = resnet_layer(inputs=x,\n",
    "                                 num_filters=num_filters,\n",
    "                                 kernel_size=1,\n",
    "                                 strides=strides,\n",
    "                                 activation=None,\n",
    "                                 batch_normalization=False)\n",
    "            x = keras.layers.add([x, y]) # x = x + y\n",
    "            x = Activation('relu')(x)\n",
    "        num_filters *= 2\n",
    "\n",
    "    # Add classifier on top.\n",
    "    # v1 does not use BN after last shortcut connection-ReLU\n",
    "    x = AveragePooling2D(pool_size=8)(x)\n",
    "    y = Flatten()(x)\n",
    "    outputs = Dense(num_classes,\n",
    "                    activation='softmax',\n",
    "                    kernel_initializer='glorot_uniform')(y) \n",
    "\n",
    "    # Instantiate model.\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59202a2",
   "metadata": {},
   "source": [
    "### Modeling: ResNet v2\n",
    "- **Pre-activation** 방식 ResNet Block 사용\n",
    "- input block: Conv+BN+ReLU (post-normalization:resnet_layer(conv_first=True))\n",
    " * Conv 한번 수행\n",
    "- middle blocks: 3 stages로 구성 (32x32 (ch) - 16x16 - 8x8)\n",
    " * 각 stage별 n ResNet Blocks, 각 ResNet Block은 3 resnet_layer, 각 resnet_layer는 pre-normalization(resnet_layer(conv_first=False))\n",
    " * stage 0(HxW=32x32): 32x32 $\\rightarrow$ 32x32, 16 channels(no. of input filters) $\\rightarrow$ 64 channels(no for output filters), n ResNet Blocks\n",
    "   - 첫번째 conv layer의 차이 주의  \n",
    "   - 첫번째에는 conv(no normalization, no activation): 이어진 resnet_layer에서 normalization, activation 수행\n",
    "   - 첫번째 ResNet Block은 channel 수 증가\n",
    " * stage 1(16x16): 32x32 $\\rightarrow$ 16x16, 64 channels $\\rightarrow$ 128 channls,  n ResNet Blocks\n",
    "   - 첫번째 ResNet Block은 해상도 1/2, Conv2d(stride=2)\n",
    "   - 나머지 ResNet Block은 동일 해상도, Conv2d(stride=1)\n",
    " * stage 2(8x8): 16x16 $\\rightarrow$ 8x8, 128 channels $\\rightarrow$ 256 channels, n ResNet Blocks\n",
    "   - 첫번째 ResNet Block은 해상도 1/2, Conv2d(stride=2)\n",
    "   - 나머지 ResNet Block은 동일 해상도, Conv2d(stride=1)\n",
    "- output block (last classification layer) \n",
    " * **Normalization-Activation** : pre-normalization, pre-activation 사용하였으므로, 추가 사용\n",
    " * (Global) Average Pooling 2d (pool_size=8,256d)- Flatten(**'feature'**) - Dense(softmax, 10 classes)\n",
    "   - Flatten layer의 이름을 'feature' layer로 이름을 주고, feature level 분석에 활용\n",
    "- layer 수(depth) 계산: \n",
    "  depth = 2 + 9*n (n: ResBlocks/stage)\n",
    " * number of weighted layers in the middle blocks = 3 convs/resnet block * n resnet block/stage * 3 stages \n",
    " * input block(conv) + last dense layer : 2\n",
    " \n",
    "### Residual block F(x) and Projection shortcut H(x)\n",
    "1) idenity mapping : H(x) = **F(x)** + x\n",
    " - F(x) = f( resnet_layer, x) : ResNet Block\n",
    " - resnet block : 3 convs, **pre-normalization, pre-activation**\n",
    "   * resnet_layer (Conv_First=False): pre-normalization, pre-activation\n",
    "   *  (**BN** - **Activation** - Conv2d) - (BN - Activation - Conv2d) - (BN - Activation - Conv2d)\n",
    "   * Activation: ReLU\n",
    " \n",
    "2) projection shortcut: H(x) = F(x) + **Conv2d(x, stride=2)**\n",
    "  - resolution 또는 channel 변경시 사용 (add위해 dimension 맞춰줌)\n",
    "  - subsampling: Conv2d(stride=2)\n",
    "  - resolution을  1/2로 줄일 때 stride=2(filter수 는 두배로 증가)\n",
    "  - no normalization, no activation\n",
    "  - conv2d(3x3) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7327c1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet_v2(input_shape, depth, num_classes=10):\n",
    "    \"\"\"ResNet Version 2 Model builder [b]\n",
    "\n",
    "    Stacks of (1 x 1)-(3 x 3)-(1 x 1) BN-ReLU-Conv2D or \n",
    "    also known as bottleneck layer.\n",
    "    First shortcut connection per layer is 1 x 1 Conv2D.\n",
    "    Second and onwards shortcut connection is identity.\n",
    "    At the beginning of each stage, \n",
    "    the feature map size is halved (downsampled)\n",
    "    by a convolutional layer with strides=2, \n",
    "    while the number of filter maps is\n",
    "    doubled. Within each stage, the layers have \n",
    "    the same number filters and the same filter map sizes.\n",
    "    Features maps sizes:\n",
    "    conv1  : 32x32,  16\n",
    "    stage 0: 32x32,  64\n",
    "    stage 1: 16x16, 128\n",
    "    stage 2:  8x8,  256\n",
    "\n",
    "    Arguments:\n",
    "        input_shape (tensor): shape of input image tensor\n",
    "        depth (int): number of core convolutional layers\n",
    "        num_classes (int): number of classes (CIFAR10 has 10)\n",
    "\n",
    "    Returns:\n",
    "        model (Model): Keras model instance\n",
    "    \"\"\"\n",
    "    if (depth - 2) % 9 != 0:\n",
    "        raise ValueError('depth should be 9n+2 (eg 110 in [b])')\n",
    "    # start model definition.\n",
    "    num_filters_in = 16\n",
    "    num_res_blocks = int((depth - 2) / 9)\n",
    "\n",
    "    inputs = Input(shape=input_shape)\n",
    "    # first conv2d \n",
    "    # on input before splitting into 2 paths\n",
    "    # - Conv2D + BN + ReLU\n",
    "    x = resnet_layer(inputs=inputs,\n",
    "                     num_filters=num_filters_in,\n",
    "                     conv_first=True)\n",
    "\n",
    "    # instantiate the stack of residual units\n",
    "    for stage in range(3):\n",
    "        for res_block in range(num_res_blocks):\n",
    "            activation = 'relu'\n",
    "            batch_normalization = True\n",
    "            strides = 1\n",
    "            if stage == 0:\n",
    "                # 첫번째 stage에서 filter수 크게\n",
    "                num_filters_out = num_filters_in * 4\n",
    "                # when first layer and first stage,\n",
    "                # conv layer 추가, \n",
    "                # 뒤에 이어진 resnet_layer에서 pre-normalization 사용에서 normalization, activation 수행\n",
    "                if res_block == 0:  \n",
    "                    activation = None\n",
    "                    batch_normalization = False\n",
    "            else:\n",
    "                # 2, 3번째 stage에서 filter수 \n",
    "                num_filters_out = num_filters_in * 2\n",
    "                # when first layer but not first stage,\n",
    "                # downsampling with stride=2\n",
    "                if res_block == 0:\n",
    "                    # downsample\n",
    "                    strides = 2 \n",
    "\n",
    "            # bottleneck residual unit\n",
    "            y = resnet_layer(inputs=x,\n",
    "                             num_filters=num_filters_in,\n",
    "                             kernel_size=1, # 1x1 kernel\n",
    "                             strides=strides,\n",
    "                             activation=activation,\n",
    "                             batch_normalization=batch_normalization,\n",
    "                             conv_first=False)\n",
    "            y = resnet_layer(inputs=y,\n",
    "                             num_filters=num_filters_in,\n",
    "                             conv_first=False) # 3x3 kernel\n",
    "            y = resnet_layer(inputs=y,\n",
    "                             num_filters=num_filters_out,\n",
    "                             kernel_size=1, # 1x1 kernel\n",
    "                             conv_first=False)\n",
    "            if res_block == 0:\n",
    "                # when linear projection residual shortcut connection,\n",
    "                # - match changed dims(filters, kernel_size=1, stride)\n",
    "                #   * stride = 2 or 1(only if to change no. of channels )\n",
    "                # - conv_first=True, activation=None, BN=False\n",
    "                x = resnet_layer(inputs=x,\n",
    "                                 num_filters=num_filters_out,\n",
    "                                 kernel_size=1,\n",
    "                                 strides=strides,\n",
    "                                 activation=None,\n",
    "                                 batch_normalization=False)\n",
    "            x = keras.layers.add([x, y]) # x = x + y\n",
    "\n",
    "        num_filters_in = num_filters_out\n",
    "\n",
    "    # add classifier on top.\n",
    "    # v2 has BN-ReLU before Pooling\n",
    "    # 채워 보세요\n",
    "    x = BatchNormalization()(x) \n",
    "    x = Activation('relu')(x)\n",
    "    x = AveragePooling2D(pool_size=8)(x)\n",
    "    y = Flatten(name='feature')(x) # feature vector layer 이름 남김\n",
    "    outputs = Dense(num_classes,\n",
    "                    activation='softmax',\n",
    "                    kernel_initializer='glorot_uniform')(y) \n",
    "\n",
    "    # instantiate model.\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5bc9da4",
   "metadata": {},
   "source": [
    "## 3. Training\n",
    "### 3.1 **Optimizer**: **Adam** + lr scheduler\n",
    "#### Learning scheduler 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a147a4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_schedule(epoch):\n",
    "    \"\"\"Learning Rate Schedule\n",
    "\n",
    "    Learning rate is scheduled to be reduced after 80, 120, 160, 180 epochs.\n",
    "    Called automatically every epoch as part of callbacks during training.\n",
    "\n",
    "    # Arguments\n",
    "        epoch (int): The number of epochs\n",
    "\n",
    "    # Returns\n",
    "        lr (float32): learning rate\n",
    "    \"\"\"\n",
    "    lr = 1e-3\n",
    "    if epoch > 180:\n",
    "        lr *= 0.5e-3\n",
    "    elif epoch > 160:\n",
    "        lr *= 1e-3\n",
    "    elif epoch > 120:\n",
    "        lr *= 1e-2\n",
    "    elif epoch > 80:\n",
    "        lr *= 1e-1\n",
    "    \n",
    "    return lr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6739ca08",
   "metadata": {},
   "source": [
    "- **Optimizer**: **Adam** + lr scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "61a7a2cb-fbde-45b9-a9d8-a11c39bc6b3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 32, 32, 3)]  0           []                               \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 32, 32, 16)   448         ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 32, 32, 16)  64          ['conv2d[0][0]']                 \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " activation (Activation)        (None, 32, 32, 16)   0           ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 32, 32, 16)   272         ['activation[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 32, 32, 16)  64          ['conv2d_1[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_1 (Activation)      (None, 32, 32, 16)   0           ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 32, 32, 16)   2320        ['activation_1[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 32, 32, 16)  64          ['conv2d_2[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_2 (Activation)      (None, 32, 32, 16)   0           ['batch_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)              (None, 32, 32, 64)   1088        ['activation[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)              (None, 32, 32, 64)   1088        ['activation_2[0][0]']           \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 32, 32, 64)   0           ['conv2d_4[0][0]',               \n",
      "                                                                  'conv2d_3[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 32, 32, 64)  256         ['add[0][0]']                    \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_3 (Activation)      (None, 32, 32, 64)   0           ['batch_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)              (None, 32, 32, 16)   1040        ['activation_3[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_4 (BatchNo  (None, 32, 32, 16)  64          ['conv2d_5[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_4 (Activation)      (None, 32, 32, 16)   0           ['batch_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)              (None, 32, 32, 16)   2320        ['activation_4[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_5 (BatchNo  (None, 32, 32, 16)  64          ['conv2d_6[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_5 (Activation)      (None, 32, 32, 16)   0           ['batch_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)              (None, 32, 32, 64)   1088        ['activation_5[0][0]']           \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 32, 32, 64)   0           ['add[0][0]',                    \n",
      "                                                                  'conv2d_7[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_6 (BatchNo  (None, 32, 32, 64)  256         ['add_1[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_6 (Activation)      (None, 32, 32, 64)   0           ['batch_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)              (None, 16, 16, 64)   4160        ['activation_6[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_7 (BatchNo  (None, 16, 16, 64)  256         ['conv2d_8[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_7 (Activation)      (None, 16, 16, 64)   0           ['batch_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_9 (Conv2D)              (None, 16, 16, 64)   36928       ['activation_7[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_8 (BatchNo  (None, 16, 16, 64)  256         ['conv2d_9[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_8 (Activation)      (None, 16, 16, 64)   0           ['batch_normalization_8[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_11 (Conv2D)             (None, 16, 16, 128)  8320        ['add_1[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_10 (Conv2D)             (None, 16, 16, 128)  8320        ['activation_8[0][0]']           \n",
      "                                                                                                  \n",
      " add_2 (Add)                    (None, 16, 16, 128)  0           ['conv2d_11[0][0]',              \n",
      "                                                                  'conv2d_10[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_9 (BatchNo  (None, 16, 16, 128)  512        ['add_2[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_9 (Activation)      (None, 16, 16, 128)  0           ['batch_normalization_9[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_12 (Conv2D)             (None, 16, 16, 64)   8256        ['activation_9[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_10 (BatchN  (None, 16, 16, 64)  256         ['conv2d_12[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_10 (Activation)     (None, 16, 16, 64)   0           ['batch_normalization_10[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_13 (Conv2D)             (None, 16, 16, 64)   36928       ['activation_10[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_11 (BatchN  (None, 16, 16, 64)  256         ['conv2d_13[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_11 (Activation)     (None, 16, 16, 64)   0           ['batch_normalization_11[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_14 (Conv2D)             (None, 16, 16, 128)  8320        ['activation_11[0][0]']          \n",
      "                                                                                                  \n",
      " add_3 (Add)                    (None, 16, 16, 128)  0           ['add_2[0][0]',                  \n",
      "                                                                  'conv2d_14[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_12 (BatchN  (None, 16, 16, 128)  512        ['add_3[0][0]']                  \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_12 (Activation)     (None, 16, 16, 128)  0           ['batch_normalization_12[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_15 (Conv2D)             (None, 8, 8, 128)    16512       ['activation_12[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_13 (BatchN  (None, 8, 8, 128)   512         ['conv2d_15[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_13 (Activation)     (None, 8, 8, 128)    0           ['batch_normalization_13[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_16 (Conv2D)             (None, 8, 8, 128)    147584      ['activation_13[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_14 (BatchN  (None, 8, 8, 128)   512         ['conv2d_16[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_14 (Activation)     (None, 8, 8, 128)    0           ['batch_normalization_14[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_18 (Conv2D)             (None, 8, 8, 256)    33024       ['add_3[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_17 (Conv2D)             (None, 8, 8, 256)    33024       ['activation_14[0][0]']          \n",
      "                                                                                                  \n",
      " add_4 (Add)                    (None, 8, 8, 256)    0           ['conv2d_18[0][0]',              \n",
      "                                                                  'conv2d_17[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_15 (BatchN  (None, 8, 8, 256)   1024        ['add_4[0][0]']                  \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_15 (Activation)     (None, 8, 8, 256)    0           ['batch_normalization_15[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_19 (Conv2D)             (None, 8, 8, 128)    32896       ['activation_15[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_16 (BatchN  (None, 8, 8, 128)   512         ['conv2d_19[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_16 (Activation)     (None, 8, 8, 128)    0           ['batch_normalization_16[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_20 (Conv2D)             (None, 8, 8, 128)    147584      ['activation_16[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_17 (BatchN  (None, 8, 8, 128)   512         ['conv2d_20[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_17 (Activation)     (None, 8, 8, 128)    0           ['batch_normalization_17[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_21 (Conv2D)             (None, 8, 8, 256)    33024       ['activation_17[0][0]']          \n",
      "                                                                                                  \n",
      " add_5 (Add)                    (None, 8, 8, 256)    0           ['add_4[0][0]',                  \n",
      "                                                                  'conv2d_21[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_18 (BatchN  (None, 8, 8, 256)   1024        ['add_5[0][0]']                  \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_18 (Activation)     (None, 8, 8, 256)    0           ['batch_normalization_18[0][0]'] \n",
      "                                                                                                  \n",
      " average_pooling2d (AveragePool  (None, 1, 1, 256)   0           ['activation_18[0][0]']          \n",
      " ing2D)                                                                                           \n",
      "                                                                                                  \n",
      " feature (Flatten)              (None, 256)          0           ['average_pooling2d[0][0]']      \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 10)           2570        ['feature[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 574,090\n",
      "Trainable params: 570,602\n",
      "Non-trainable params: 3,488\n",
      "__________________________________________________________________________________________________\n",
      "ResNet20v2\n"
     ]
    }
   ],
   "source": [
    "if version == 2:\n",
    "    model = resnet_v2(input_shape=input_shape, depth=depth)\n",
    "else: \n",
    "    model = resnet_v1(input_shape=input_shape, depth=depth)\n",
    "    \n",
    "if keras_major == 3 and device is not None:\n",
    "    model = model.to(device)\n",
    "    print(f'to {device}')\n",
    "    \n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(learning_rate=lr_schedule(0)),\n",
    "              metrics=['accuracy'])\n",
    "model.summary()\n",
    "print(model_type)\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "\n",
    "callbacks = [lr_scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dc3096af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'pydot' has no attribute 'InvocationException'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32m~\\.conda\\envs\\tf29py39\\lib\\site-packages\\pydot\\core.py:1851\u001b[0m, in \u001b[0;36mDot.create\u001b[1;34m(self, prog, format, encoding)\u001b[0m\n\u001b[0;32m   1850\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1851\u001b[0m     stdout_data, stderr_data, process \u001b[38;5;241m=\u001b[39m \u001b[43mcall_graphviz\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1852\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogram\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprog\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1853\u001b[0m \u001b[43m        \u001b[49m\u001b[43marguments\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43marguments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1854\u001b[0m \u001b[43m        \u001b[49m\u001b[43mworking_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtmp_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1855\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1856\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf29py39\\lib\\site-packages\\pydot\\core.py:211\u001b[0m, in \u001b[0;36mcall_graphviz\u001b[1;34m(program, arguments, working_dir, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m program_with_args \u001b[38;5;241m=\u001b[39m [program] \u001b[38;5;241m+\u001b[39m arguments\n\u001b[1;32m--> 211\u001b[0m process \u001b[38;5;241m=\u001b[39m subprocess\u001b[38;5;241m.\u001b[39mPopen(\n\u001b[0;32m    212\u001b[0m     program_with_args,\n\u001b[0;32m    213\u001b[0m     env\u001b[38;5;241m=\u001b[39menv,\n\u001b[0;32m    214\u001b[0m     cwd\u001b[38;5;241m=\u001b[39mworking_dir,\n\u001b[0;32m    215\u001b[0m     shell\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    216\u001b[0m     stderr\u001b[38;5;241m=\u001b[39msubprocess\u001b[38;5;241m.\u001b[39mPIPE,\n\u001b[0;32m    217\u001b[0m     stdout\u001b[38;5;241m=\u001b[39msubprocess\u001b[38;5;241m.\u001b[39mPIPE,\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    219\u001b[0m )\n\u001b[0;32m    220\u001b[0m stdout_data, stderr_data \u001b[38;5;241m=\u001b[39m process\u001b[38;5;241m.\u001b[39mcommunicate()\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf29py39\\lib\\subprocess.py:951\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[1;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask)\u001b[0m\n\u001b[0;32m    948\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mTextIOWrapper(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr,\n\u001b[0;32m    949\u001b[0m                     encoding\u001b[38;5;241m=\u001b[39mencoding, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m--> 951\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreexec_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    952\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mpass_fds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    953\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstartupinfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreationflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshell\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    954\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mp2cread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp2cwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    955\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mc2pread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc2pwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    956\u001b[0m \u001b[43m                        \u001b[49m\u001b[43merrread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    957\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mrestore_signals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    958\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mgid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mumask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    959\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstart_new_session\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    960\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m    961\u001b[0m     \u001b[38;5;66;03m# Cleanup if the child failed starting.\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf29py39\\lib\\subprocess.py:1420\u001b[0m, in \u001b[0;36mPopen._execute_child\u001b[1;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, unused_restore_signals, unused_gid, unused_gids, unused_uid, unused_umask, unused_start_new_session)\u001b[0m\n\u001b[0;32m   1419\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1420\u001b[0m     hp, ht, pid, tid \u001b[38;5;241m=\u001b[39m \u001b[43m_winapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCreateProcess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1421\u001b[0m \u001b[43m                             \u001b[49m\u001b[38;5;66;43;03m# no special security\u001b[39;49;00m\n\u001b[0;32m   1422\u001b[0m \u001b[43m                             \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1423\u001b[0m \u001b[43m                             \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1424\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mcreationflags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1425\u001b[0m \u001b[43m                             \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1426\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1427\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mstartupinfo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1428\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   1429\u001b[0m     \u001b[38;5;66;03m# Child is launched. Close the parent's copy of those pipe\u001b[39;00m\n\u001b[0;32m   1430\u001b[0m     \u001b[38;5;66;03m# handles that only the child should have open.  You need\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1433\u001b[0m     \u001b[38;5;66;03m# pipe will not close when the child process exits and the\u001b[39;00m\n\u001b[0;32m   1434\u001b[0m     \u001b[38;5;66;03m# ReadFile will hang.\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] 지정된 파일을 찾을 수 없습니다",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32m~\\.conda\\envs\\tf29py39\\lib\\site-packages\\keras\\utils\\vis_utils.py:51\u001b[0m, in \u001b[0;36mcheck_pydot\u001b[1;34m()\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     49\u001b[0m   \u001b[38;5;66;03m# Attempt to create an image of a blank graph\u001b[39;00m\n\u001b[0;32m     50\u001b[0m   \u001b[38;5;66;03m# to check the pydot/graphviz installation.\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m   \u001b[43mpydot\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDot\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpydot\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDot\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     52\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf29py39\\lib\\site-packages\\pydot\\core.py:1860\u001b[0m, in \u001b[0;36mDot.create\u001b[1;34m(self, prog, format, encoding)\u001b[0m\n\u001b[0;32m   1859\u001b[0m     args[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprog\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m not found in path.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m-> 1860\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m   1861\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] \"dot\" not found in path.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m model_arch_png \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_cifar10_\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m model_type\n\u001b[1;32m----> 2\u001b[0m \u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mto_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_arch_png\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_shapes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf29py39\\lib\\site-packages\\keras\\utils\\vis_utils.py:421\u001b[0m, in \u001b[0;36mplot_model\u001b[1;34m(model, to_file, show_shapes, show_dtype, show_layer_names, rankdir, expand_nested, dpi, layer_range, show_layer_activations)\u001b[0m\n\u001b[0;32m    416\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m model\u001b[38;5;241m.\u001b[39mbuilt:\n\u001b[0;32m    417\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThis model has not yet been built. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    418\u001b[0m                    \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBuild the model first by calling `build()` or by calling \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    419\u001b[0m                    \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthe model on a batch of data.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 421\u001b[0m dot \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_to_dot\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    422\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    423\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshow_shapes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_shapes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    424\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshow_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    425\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshow_layer_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_layer_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    426\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrankdir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrankdir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    427\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexpand_nested\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpand_nested\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    428\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdpi\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdpi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    429\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_range\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_range\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    430\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshow_layer_activations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_layer_activations\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    431\u001b[0m to_file \u001b[38;5;241m=\u001b[39m io_utils\u001b[38;5;241m.\u001b[39mpath_to_string(to_file)\n\u001b[0;32m    432\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dot \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf29py39\\lib\\site-packages\\keras\\utils\\vis_utils.py:151\u001b[0m, in \u001b[0;36mmodel_to_dot\u001b[1;34m(model, show_shapes, show_dtype, show_layer_names, rankdir, expand_nested, dpi, subgraph, layer_range, show_layer_activations)\u001b[0m\n\u001b[0;32m    148\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m sequential\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m functional\n\u001b[1;32m--> 151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mcheck_pydot\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    152\u001b[0m   message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    153\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYou must install pydot (`pip install pydot`) \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    154\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mand install graphviz \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    155\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m(see instructions at https://graphviz.gitlab.io/download/) \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    156\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfor plot_model/model_to_dot to work.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    157\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIPython.core.magics.namespace\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mmodules:\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;66;03m# We don't raise an exception here in order to avoid crashing notebook\u001b[39;00m\n\u001b[0;32m    159\u001b[0m     \u001b[38;5;66;03m# tests where graphviz is not available.\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf29py39\\lib\\site-packages\\keras\\utils\\vis_utils.py:53\u001b[0m, in \u001b[0;36mcheck_pydot\u001b[1;34m()\u001b[0m\n\u001b[0;32m     51\u001b[0m   pydot\u001b[38;5;241m.\u001b[39mDot\u001b[38;5;241m.\u001b[39mcreate(pydot\u001b[38;5;241m.\u001b[39mDot())\n\u001b[0;32m     52\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 53\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mOSError\u001b[39;00m, \u001b[43mpydot\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mInvocationException\u001b[49m):\n\u001b[0;32m     54\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'pydot' has no attribute 'InvocationException'"
     ]
    }
   ],
   "source": [
    "model_arch_png = 'model_cifar10_%s.png' % model_type\n",
    "keras.utils.plot_model(model, to_file=model_arch_png, show_shapes=True )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa9e93c",
   "metadata": {},
   "source": [
    "### 3.2 Train the designed model\n",
    "- Data Augmentation\n",
    "- Traing time: ~1:30min/200 epoch, 91%(test dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "85883015-10df-445f-bb84-113534487ecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using real-time data augmentation.\n",
      "steps_per_epoch= 1563 , len(x_train)/batch_size)= 1562.5\n",
      "Epoch 1/120\n",
      "1563/1563 [==============================] - 319s 202ms/step - loss: 1.7604 - accuracy: 0.4872 - val_loss: 1.5812 - val_accuracy: 0.5536 - lr: 0.0010\n",
      "Epoch 2/120\n",
      "1563/1563 [==============================] - 323s 207ms/step - loss: 1.3496 - accuracy: 0.6182 - val_loss: 1.6293 - val_accuracy: 0.5430 - lr: 0.0010\n",
      "Epoch 3/120\n",
      "1563/1563 [==============================] - 341s 218ms/step - loss: 1.1734 - accuracy: 0.6773 - val_loss: 1.6548 - val_accuracy: 0.5496 - lr: 0.0010\n",
      "Epoch 4/120\n",
      "1563/1563 [==============================] - 326s 209ms/step - loss: 1.0614 - accuracy: 0.7145 - val_loss: 1.1006 - val_accuracy: 0.7083 - lr: 0.0010\n",
      "Epoch 5/120\n",
      "1563/1563 [==============================] - 346s 221ms/step - loss: 0.9886 - accuracy: 0.7400 - val_loss: 1.0327 - val_accuracy: 0.7289 - lr: 0.0010\n",
      "Epoch 6/120\n",
      "1563/1563 [==============================] - 341s 218ms/step - loss: 0.9353 - accuracy: 0.7554 - val_loss: 1.0661 - val_accuracy: 0.7025 - lr: 0.0010\n",
      "Epoch 7/120\n",
      "1563/1563 [==============================] - 360s 230ms/step - loss: 0.8942 - accuracy: 0.7708 - val_loss: 0.9844 - val_accuracy: 0.7459 - lr: 0.0010\n",
      "Epoch 8/120\n",
      "1563/1563 [==============================] - 356s 228ms/step - loss: 0.8654 - accuracy: 0.7791 - val_loss: 1.1495 - val_accuracy: 0.7035 - lr: 0.0010\n",
      "Epoch 9/120\n",
      "1563/1563 [==============================] - 356s 228ms/step - loss: 0.8355 - accuracy: 0.7905 - val_loss: 1.1195 - val_accuracy: 0.7052 - lr: 0.0010\n",
      "Epoch 10/120\n",
      " 646/1563 [===========>..................] - ETA: 3:16 - loss: 0.8171 - accuracy: 0.7979"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 47\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# fit the model on the batches generated by datagen.flow().\u001b[39;00m\n\u001b[0;32m     46\u001b[0m start_time \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()    \n\u001b[1;32m---> 47\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdatagen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[43m          \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# progress bar\u001b[39;49;00m\n\u001b[0;32m     49\u001b[0m \u001b[43m          \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[43m          \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[43m          \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[43m          \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m end_time \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf29py39\\lib\\site-packages\\keras\\utils\\traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf29py39\\lib\\site-packages\\keras\\engine\\training.py:1409\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1402\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1403\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   1404\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   1405\u001b[0m     step_num\u001b[38;5;241m=\u001b[39mstep,\n\u001b[0;32m   1406\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m   1407\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m   1408\u001b[0m   callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1409\u001b[0m   tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1410\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1411\u001b[0m     context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf29py39\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf29py39\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf29py39\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateless_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf29py39\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2453\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2450\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m   2451\u001b[0m   (graph_function,\n\u001b[0;32m   2452\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2453\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2454\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf29py39\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1860\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1856\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1857\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1858\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1859\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1860\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1861\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1862\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1863\u001b[0m     args,\n\u001b[0;32m   1864\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1865\u001b[0m     executing_eagerly)\n\u001b[0;32m   1866\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf29py39\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:497\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    495\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    496\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 497\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    503\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    504\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    505\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    506\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    509\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    510\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf29py39\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Run training, with or without data augmentation.\n",
    "if not data_augmentation:\n",
    "    print('Not using data augmentation.')\n",
    "    start_time = datetime.now()    \n",
    "    history = model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_data=(x_test, y_test),\n",
    "              shuffle=False, #  True if necessary \n",
    "              callbacks=callbacks)\n",
    "    end_time = datetime.now()\n",
    "\n",
    "else: # Data Augmentation 사용\n",
    "    print('Using real-time data augmentation.')\n",
    "    # this will do preprocessing and realtime data augmentation:\n",
    "    datagen = ImageDataGenerator(\n",
    "        # set input mean to 0 over the dataset\n",
    "        featurewise_center=False,\n",
    "        # set each sample mean to 0\n",
    "        samplewise_center=False,\n",
    "        # divide inputs by std of dataset\n",
    "        featurewise_std_normalization=False,\n",
    "        # divide each input by its std\n",
    "        samplewise_std_normalization=False,\n",
    "        # apply ZCA whitening\n",
    "        zca_whitening=False,\n",
    "        # randomly rotate images in the range (deg 0 to 180)\n",
    "        rotation_range=0,\n",
    "        # randomly shift images horizontally\n",
    "        width_shift_range=0.1,\n",
    "        # randomly shift images vertically\n",
    "        height_shift_range=0.1,\n",
    "        # randomly flip images\n",
    "        horizontal_flip=True,\n",
    "        # randomly flip images\n",
    "        vertical_flip=False)\n",
    "\n",
    "    # compute quantities required for featurewise normalization\n",
    "    # (std, mean, and principal components if ZCA whitening is applied).\n",
    "    datagen.fit(x_train)\n",
    "    \n",
    "    # epoch당 step 수 지정해서 입력하는 방법, 안쓰면 아래의 식으로 자동 계산\n",
    "    steps_per_epoch = math.ceil(len(x_train) / batch_size)\n",
    "    print('steps_per_epoch=',steps_per_epoch, ', len(x_train)/batch_size)=', len(x_train) / batch_size) \n",
    "    # fit the model on the batches generated by datagen.flow().\n",
    "    start_time = datetime.now()    \n",
    "    history = model.fit(x=datagen.flow(x_train, y_train, batch_size=batch_size),\n",
    "              verbose=1, # progress bar\n",
    "              epochs=epochs,\n",
    "              validation_data=(x_test, y_test),\n",
    "              steps_per_epoch=steps_per_epoch,\n",
    "              callbacks=callbacks)\n",
    "    end_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2a207f",
   "metadata": {},
   "outputs": [],
   "source": [
    "elapsed_time = end_time - start_time\n",
    "print(f'Training time:{elapsed_time}') # ~ 55 min"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9e71a7",
   "metadata": {},
   "source": [
    "## Plot the train history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e62f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot history.\n",
    "def plot_history(h):\n",
    "    epochs_ = len(h.history['loss'])\n",
    "    ep = np.arange(epochs_)\n",
    "    fig = plt.figure(figsize=(17,8))\n",
    "    ax1 = fig.add_subplot(1,2,1)\n",
    "    ax2 = fig.add_subplot(1,2,2)\n",
    "\n",
    "    ax1.plot(ep, h.history['loss'], label='loss')\n",
    "    ax1.plot(ep, h.history['val_loss'], label='val_loss')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss ')\n",
    "    ax1.legend(bbox_to_anchor=(1,1))\n",
    "    ax1.grid(True)\n",
    "\n",
    "    ax2.plot(ep, h.history['accuracy'], label='accuracy')\n",
    "    ax2.plot(ep, h.history['val_accuracy'], label='val_accuracy')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax2.legend(bbox_to_anchor=(1,1))\n",
    "    ax2.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19b7519",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec6873c",
   "metadata": {},
   "source": [
    "## 4. Evaluation \n",
    "### Quantitative Evaluation\n",
    "- Accuracy on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee9b25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score trained model.\n",
    "scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39c607c",
   "metadata": {},
   "source": [
    "### Generate Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80426a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions for the test dataset.\n",
    "predictions = model.predict(x_test)\n",
    "\n",
    "# For each sample image in the test dataset, select the class label with the highest probability.\n",
    "predicted_labels = [np.argmax(i) for i in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f0b0af-f589-4539-afa0-4d8658db93c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_integer_labels[:10], predicted_labels[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61cccaca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert one-hot encoded labels to integers.\n",
    "y_test_integer_labels = tf.argmax(y_test, axis=1)\n",
    "\n",
    "# Generate a confusion matrix for the test dataset.\n",
    "cm = tf.math.confusion_matrix(labels=y_test_integer_labels, predictions=predicted_labels)\n",
    "\n",
    "# Plot the confusion matrix as a heatmap.\n",
    "plt.figure(figsize=[12, 6])\n",
    "import seaborn as sn\n",
    "sn.heatmap(cm, annot=True, fmt='d', annot_kws={\"size\": 12})\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Truth')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a43a81",
   "metadata": {},
   "source": [
    "## Inference test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc0a8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar10_categories=['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bbd7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run inference on new data\n",
    "# save an image from cifar10 dataset and load it\n",
    "image_id = 0 # cifar10 test image id in number\n",
    "image_file = f'./cifar10_test{image_id}.png'\n",
    "keras.utils.save_img(image_file, x_test[image_id])\n",
    "img = keras.utils.load_img(image_file, target_size=(32,32))\n",
    "\n",
    "# image display\n",
    "plt.imshow(img)\n",
    "img_array = keras.utils.img_to_array(img)\n",
    "\n",
    "# add a dimension to make an array ( a form of a list of images) for model.predict input\n",
    "# [image1]\n",
    "img_array = np.expand_dims(img_array, axis=0)\n",
    "\n",
    "# preprocessing the input image  \n",
    "img_array = img_array.astype('float32') / 255\n",
    "\n",
    "# If subtract pixel mean is enabled\n",
    "if subtract_pixel_mean:\n",
    "    img_array -= x_train_mean  # x_train_mean = np.mean(x_train, axis=0)\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "#score = predictions[0]\n",
    "\n",
    "prediction = np.argmax(predictions[0]) # predicted class id : id for max scores\n",
    "\n",
    "# categories[prediction]\n",
    "print(f\"ground truth_label = {cifar10_categories[np.argmax(y_test[image_id])]}, predicted_label = {cifar10_categories[prediction]}\")\n",
    "print(f\"Similarity = {np.dot(predictions[0],y_test[image_id])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4387b83b",
   "metadata": {},
   "source": [
    "## Qualitative Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f4b9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(x_dataset, y_label, model):\n",
    "\n",
    "    class_names = ['airplane',\n",
    "                   'automobile',\n",
    "                   'bird',\n",
    "                   'cat',\n",
    "                   'deer',\n",
    "                   'dog',\n",
    "                   'frog',\n",
    "                   'horse',\n",
    "                   'ship',\n",
    "                   'truck' ]\n",
    "    num_rows = 3\n",
    "    num_cols = 6\n",
    "    \n",
    "    # Retrieve a number of images from the dataset.\n",
    "    data_batch = x_dataset[0:num_rows*num_cols]\n",
    "\n",
    "    # Get predictions from model.  \n",
    "    predictions = model.predict(data_batch)\n",
    "\n",
    "    plt.figure(figsize=(20, 8))\n",
    "    num_matches = 0\n",
    "        \n",
    "    if subtract_pixel_mean:\n",
    "        data_batch += x_train_mean  # add to range [0,1] for display , x_train_mean = np.mean(x_train, axis=0)\n",
    "\n",
    "    for idx in range(num_rows*num_cols):\n",
    "        ax = plt.subplot(num_rows, num_cols, idx + 1)\n",
    "        plt.axis(\"off\")\n",
    "        plt.imshow(data_batch[idx])\n",
    "\n",
    "        pred_idx = tf.argmax(predictions[idx]).numpy()\n",
    "        truth_idx = np.nonzero(y_label[idx])\n",
    "            \n",
    "        title = str(class_names[truth_idx[0][0]]) + \" : \" + str(class_names[pred_idx])\n",
    "        title_obj = plt.title(title, fontdict={'fontsize':13})\n",
    "            \n",
    "        if pred_idx == truth_idx:\n",
    "            num_matches += 1\n",
    "            plt.setp(title_obj, color='g')\n",
    "        else:\n",
    "            plt.setp(title_obj, color='r')\n",
    "                \n",
    "        acc = num_matches/(idx+1)\n",
    "    print(\"Prediction accuracy: \", int(100*acc)/100)\n",
    "    \n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e0937c",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(x_test, y_test, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deebc42d-a5cf-4491-9992-097708323b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/tensorflow/tensorflow/issues/59694\n",
    "import absl.logging\n",
    "absl.logging.set_verbosity(absl.logging.ERROR) # to ignore tracing functions warning messages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d30d477-536a-4e7e-b53f-2f24d7c2857f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to load the model later\n",
    "model_file = 'cifar10_%s' % model_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23737361",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(model_file) # tf format\n",
    "#model.save('%s.h5' % model_file) # h5 format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb93fad9-0286-4102-8058-731d9689ec21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the saved model can be loaded \n",
    "reloaded_model = keras.models.load_model(model_file)\n",
    "reloaded_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603f3023",
   "metadata": {},
   "outputs": [],
   "source": [
    "!dir cifar10_ResNet20v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890bc586",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = reloaded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34d546b-fc68-4e0a-bf3a-3d98c365de56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2f10e4-09cd-4ba0-8572-3f7c86ef5b14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf29py39",
   "language": "python",
   "name": "tf29py39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
