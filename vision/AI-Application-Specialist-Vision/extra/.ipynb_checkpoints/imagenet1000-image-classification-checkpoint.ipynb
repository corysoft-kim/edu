{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f94a9560",
   "metadata": {},
   "source": [
    "# Image Classification \n",
    "\n",
    "### to use the pretrained models using ImageNet 1000 for inference \n",
    "### Dataset: ImageNet 1000\n",
    "\n",
    "### - Use pretrained models in the Keras Applications for applications: prediction, fine-tuning\n",
    "### - models: ResNet50, VGG16, VGG19\n",
    "### - Reference: https://keras.io/api/applications/ \n",
    " - imagenet 1000 class list: \n",
    "     [keras-imagenet_class_index.json](https://github.com/raghakot/keras-vis/blob/master/resources/imagenet_class_index.json),\n",
    "     [Class ID-Class Name Table](https://deeplearning.cms.waikato.ac.nz/user-guide/class-maps/IMAGENET/),                                  [clsidx_to_lables.txt](https://gist.github.com/aaronpolhamus/964a4411c0906315deb9f4a3723aac57)\n",
    " \n",
    " - imagenet-1k Dataset card: https://huggingface.co/datasets/imagenet-1k\n",
    " - image-net.org: https://www.image-net.org/update-mar-11-2021.php"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589f7b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras import applications \n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "from tensorflow.keras.applications.vgg19 import VGG19\n",
    "from tensorflow.keras.applications.vgg19 import preprocess_input as vgg19_preprocess_input\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input as vgg16_preprocess_input\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67317cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in order to download models from huggingface in the ML Platform, it is necessary to set the following proxy and ssl \n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "# suwon\n",
    "import os\n",
    "os.environ['REQUESTS_CA_BUNDLE'] = '/etc/ssl/certs/ca-certificates.crt'\n",
    "os.environ['HTTP_PROXY'] ='http://75.17.107.42:8080'\n",
    "os.environ['HTTPS_PROXY'] ='http://75.17.107.42:8080'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea41b4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test images 포함한 폴더\n",
    "img_path = '../images/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6eddec4-cf52-4e17-9760-608c74aee0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls $img_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e933f0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load image from url\n",
    "import urllib\n",
    "from io import BytesIO\n",
    "import PIL\n",
    "\n",
    "def load_image_url(URL):\n",
    "    with urllib.request.urlopen(URL) as url:\n",
    "        img = keras.preprocessing.image.load_img(BytesIO(url.read()), target_size=(224, 224))\n",
    "\n",
    "    return img\n",
    "    # return image.img_to_array(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1108cfc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet50(weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1343cbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load from an image in the local directory\n",
    "img_file = img_path + 'elephant.jpg'\n",
    "img = image.load_img(img_file, target_size=(224, 224)) # Loads an image into PIL format\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "\n",
    "x = image.img_to_array(img) # PIL image into numpy array 224x224x3 (float32)\n",
    "x = np.expand_dims(x, axis=0) # batch form by adding axis: 224x224x3 into (1x224x224x3), 3차원을 4차원으 tensor로 확장\n",
    "x = preprocess_input(x) # substract mean values in [R, G, B]\n",
    "\n",
    "preds = model.predict(x)\n",
    "# decode the results into a list of tuples (class, description, probability)\n",
    "# (one such list for each sample in the batch)\n",
    "print('Predicted:', decode_predictions(preds, top=3)[0])\n",
    "# Predicted: [(u'n02504013', u'Indian_elephant', 0.82658225), (u'n01871265', u'tusker', 0.1122357), (u'n02504458', u'African_elephant', 0.061040461)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f14e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_url1 = 'https://d1bg8rd1h4dvdb.cloudfront.net/img/storypick/monamipet/2019/01/1811_pet_dog_pomeranian_m_01.jpg'\n",
    "img_url2 = 'https://github.com/pytorch/hub/raw/master/images/dog.jpg'\n",
    "img1 = load_image_url(img_url1)\n",
    "img2 = load_image_url(img_url2)\n",
    "plt.subplot(121)\n",
    "plt.imshow(img1)\n",
    "plt.axis('off')\n",
    "plt.subplot(122)\n",
    "plt.imshow(img2)\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c81a8b4-eee7-46b1-959d-17ba4fb495f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = image.img_to_array(img1)\n",
    "x = np.expand_dims(x, axis=0) # batch 단위 처리\n",
    "x = preprocess_input(x)\n",
    "\n",
    "preds = model.predict(x)\n",
    "# decode the results into a list of tuples (class, description, probability)\n",
    "# (one such list for each sample in the batch)\n",
    "print('Predicted:', decode_predictions(preds, top=3)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c12f28-bb8b-425e-8f10-bf6938d9473d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = image.img_to_array(img2)\n",
    "x = np.expand_dims(x, axis=0) # batch 단위 처리\n",
    "x = preprocess_input(x)\n",
    "\n",
    "preds = model.predict(x)\n",
    "# decode the results into a list of tuples (class, description, probability)\n",
    "# (one such list for each sample in the batch)\n",
    "print('Predicted:', decode_predictions(preds, top=3)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98136b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da6fdc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.utils.plot_model(model, to_file=\"resnet50.png\", show_shapes=True )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcabf780",
   "metadata": {},
   "source": [
    "### Use VGG16, VGG19\n",
    "- You may use to extract features with VGG16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3be4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16_model = VGG16(weights='imagenet') \n",
    "vgg19_model = VGG19(weights='imagenet') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eced8bc-83a4-41c6-a120-4f0914aacc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_file = img_path + 'elephant.jpg'\n",
    "img = image.load_img(img_file, target_size=(224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00660959-2dbd-4d2a-89b3-d53ea84dbdbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = vgg16_preprocess_input(x)\n",
    "\n",
    "preds = vgg16_model.predict(x)\n",
    "print('Predicted:', decode_predictions(preds, top=5)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c0f04f-2d63-4619-be9a-e4f3321611c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = vgg19_preprocess_input(x)\n",
    "\n",
    "preds = vgg19_model.predict(x)\n",
    "print('Predicted:', decode_predictions(preds, top=5)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4e58ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86cf2bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.utils.plot_model(model, to_file=\"vgg16.png\", show_shapes=True )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2250f7e",
   "metadata": {},
   "source": [
    "### MobileNet v1, v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e9b8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mobilenetv1 = applications.MobileNet(weights='imagenet')\n",
    "mobilenetv2 = applications.MobileNetV2(weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec482591-36ec-4452-8d17-ab7a965c10ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = image.img_to_array(img1)\n",
    "x = np.expand_dims(x, axis=0) # batch 단위 처리\n",
    "x = applications.mobilenet.preprocess_input(x)\n",
    "\n",
    "preds = mobilenetv1.predict(x)\n",
    "print('Predicted:', decode_predictions(preds, top=3)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef883f3-7d76-4690-85a3-3ee74d8c810d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = image.img_to_array(img1)\n",
    "x = np.expand_dims(x, axis=0) # batch 단위 처리\n",
    "x = applications.mobilenet_v2.preprocess_input(x)\n",
    "\n",
    "preds = mobilenetv2.predict(x)\n",
    "print('Predicted:', decode_predictions(preds, top=3)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505e020f-099f-4bd1-85d9-2d33f0c51691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model visualization\n",
    "mobilenetv1.summary()\n",
    "keras.utils.plot_model(mobilenetv1, to_file=\"mobilenetv1.png\", show_shapes=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6ff16e-6bd5-4f40-96b4-1c4031d3dddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model visualization\n",
    "mobilenetv2.summary()\n",
    "keras.utils.plot_model(mobilenetv2, to_file=\"mobilenetv2.png\", show_shapes=True )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_keras2",
   "language": "python",
   "name": "venv_keras2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
