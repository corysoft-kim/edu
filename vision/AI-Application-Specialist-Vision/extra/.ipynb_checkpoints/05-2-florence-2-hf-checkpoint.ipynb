{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfe0f787-f391-428e-a455-2bb6993bc86d",
   "metadata": {},
   "source": [
    "## Florence-2\n",
    "- 사용 모델: Florence-2-large \n",
    "- Reference: [florence-2-large jupyter notebook](https://huggingface.co/microsoft/Florence-2-large/blob/main/sample_inference.ipynb)\n",
    "- Paper: [Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks, 2023-Nov-10](https://arxiv.org/pdf/2311.06242)\n",
    "- Requirement: upgrade to torch 2, torchvision\n",
    "  * torch==2.0.1+cu117, torchvision==0.15.2+cu117"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c947a7d-160d-40e0-97d0-a562eb54ad59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "# disable waring from torchvision/datapoints/__init__.py\n",
    "torchvision.disable_beta_transforms_warning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8578826-8a63-41b6-a5f3-c1facf472226",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, AutoModelForCausalLM  \n",
    "from PIL import Image\n",
    "import requests\n",
    "import copy\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d063b2e5-16df-4f83-a6d5-9a65a5b5e028",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22cc8b54-1fee-4420-b8e4-8d87085934ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# torch.float16 did not work\n",
    "# torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "torch_dtype = torch.float32\n",
    "\n",
    "print('torch version: ', torch.__version__)\n",
    "print('torchvision version: ', torchvision.__version__)\n",
    "print(device)\n",
    "print('the torch model will use ', torch_dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa148ef-6878-4757-b0be-68eeab83e02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in order to download models from huggingface, it is necessary to set the following proxy and ssl \n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "# suwon\n",
    "import os\n",
    "os.environ['REQUESTS_CA_BUNDLE'] = '/etc/ssl/certs/ca-certificates.crt'\n",
    "os.environ['HTTP_PROXY'] ='http://75.17.107.42:8080'\n",
    "os.environ['HTTPS_PROXY'] ='http://75.17.107.42:8080'\n",
    "\n",
    "# to disable tokenizers warning\n",
    "os.environ['TOKENIZERS_PARALLELISM'] ='false'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a037399b-59f6-4571-b111-70f845ab4d1b",
   "metadata": {},
   "source": [
    "### Loading the florence-2 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c86d719-c0cd-44ed-8cb9-995eb82080dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = 'microsoft/Florence-2-large'\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, trust_remote_code=True, torch_dtype=torch_dtype).to(device)\n",
    "processor = AutoProcessor.from_pretrained(model_id, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8eb98c-22f0-4d3d-931d-060a5988e2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to see the models\n",
    "# print(model)\n",
    "# print(processor) # see the tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81bda7f8-2941-4398-9ecd-96046e4c75db",
   "metadata": {},
   "source": [
    "### To define the prediction function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350337f8-2561-4949-a284-0e19d40f95f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/docs/transformers/generation_strategies\n",
    "\n",
    "def run_example(image_input, task_prompt, text_input=None):\n",
    "    if text_input is None:\n",
    "        prompt = task_prompt\n",
    "    else:\n",
    "        prompt = task_prompt + text_input\n",
    "    inputs = processor(text=prompt, images=image_input, return_tensors=\"pt\").to('cuda', torch_dtype)  #  torch.float16\n",
    "    generated_ids = model.generate(\n",
    "      input_ids=inputs[\"input_ids\"].cuda(),\n",
    "      pixel_values=inputs[\"pixel_values\"].cuda(),\n",
    "      max_new_tokens=1024,\n",
    "      early_stopping=False,\n",
    "      do_sample=False,\n",
    "      num_beams=3,\n",
    "    )\n",
    "    # print(\"generated_ids=\", generated_ids) # 중간 결과 보기 \n",
    "    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]\n",
    "    # print(\"generated_text=\", generated_text) # 중간 결과 보기 \n",
    "    parsed_answer = processor.post_process_generation(\n",
    "        generated_text, \n",
    "        task=task_prompt, \n",
    "        image_size=(image_input.width, image_input.height)\n",
    "    )\n",
    "\n",
    "    return parsed_answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a823a4-5970-4783-b1cd-63995bc8ac91",
   "metadata": {},
   "source": [
    "### To load a test image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63951eb6-7839-4c24-bf32-b27e014ee153",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/car.jpg?download=true\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa40c04-0baa-45a2-841b-a4e101c24d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381c26ed-3eea-42aa-8015-745f678ca749",
   "metadata": {},
   "outputs": [],
   "source": [
    "image.size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2b794d-331e-4499-b9ec-225fdaed31e4",
   "metadata": {},
   "source": [
    "## 사용법\n",
    "- pre-defined task 사용하는 방법과 여러개의 pre-defined task를 연결하여 사용하는 방법이 있음\n",
    "## 1.To run pre-defined tasks without addtional inputs\n",
    "   |pre-defined tasks| task promt |\n",
    "   |:---:|:---|\n",
    "   |captioning| ``` '<CAPTION>', '<DETAILED_CAPTION>', '<MORE_DETAILED_CAPTION>' ``` |\n",
    "   |object detection | ``` '<OD>' ```\n",
    "   | dense region caption| ``` '<DENSE_REGION_CAPTION>' ``` |\n",
    "   | region proposal| ``` '<REGION_PROPOSAL>' ``` |\n",
    "    \n",
    "## 2. To run pre-defined tasks with addtional inputs\n",
    "   |pre-defined tasks| task promt |\n",
    "   |:---:|:---|\n",
    "   |phrase grounding| ``` '<CAPTION_TO_PHRASE_GROUNDING>' ``` |\n",
    "   |referring expression segmentation| ``` '<REFERRING_EXPRESSION_SEGMENTATION>' ``` |\n",
    "   |region to segmentation| ``` '<REGION_TO_SEGMENTATION>' ``` |\n",
    "   |open vocabulary detection| ``` '<OPEN_VOCABULARY_DETECTION>' ``` |\n",
    "   |region to texts| ```'<REGION_TO_CATEGORY>', '<REGION_TO_DESCRIPTION>' ``` \n",
    "   |OCR| ``` '<OCR>', '<OCR_WITH_REGION>' ```  |\n",
    " \n",
    " ## 3. To run cascaed tasks\n",
    " - Caption + Phrase Grounding\n",
    " - Detailed Caption + Phrase Grounding\n",
    " - More Detailed Caption + Phrase Grounding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ccbd50-5308-49db-a66d-4100110315d7",
   "metadata": {},
   "source": [
    "## 1. Run pre-defined tasks without addtional inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67586f4c-7fa7-40dd-871e-ebdeb4d9b3cc",
   "metadata": {},
   "source": [
    "### Image Captioning (Scene Understanding) : 영상 내용을 text(English)로 표현\n",
    "- input: image\n",
    "- output: text description\n",
    "- task_prompt로 3가지 level 제공: (brief) CAPTION(단문장, 두드러진 object나 activity 등 표현), DETAILED_CAPTION(여러 문장), MORE_DETAILED_CAPTION(여러 문장, object, attributes, action등 풍부하게 묘사)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a6a030-2d0c-4820-9d4d-af657f97e983",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_prompt = '<CAPTION>'\n",
    "run_example(image, task_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e106124d-b516-4d41-b2fe-db5b0bc97ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_prompt = '<DETAILED_CAPTION>'\n",
    "run_example(image, task_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c2e79c-c9e8-4e61-9c3e-b16504626fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_prompt = '<MORE_DETAILED_CAPTION>'\n",
    "run_example(image, task_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9769742-5bb1-4584-b7d4-dd0a36c2dfe7",
   "metadata": {},
   "source": [
    "## Object Detection\n",
    "- input: image\n",
    "- output: 영상내 시각적인 물체 위치 및 label 검출"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002056f5-8f7a-4515-82c8-43389675ee14",
   "metadata": {},
   "source": [
    "OD results format: {'<OD>': { 'bboxes': [[x1, y1, x2, y2], ...], 'labels': ['label1', 'label2', ...] } }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b722b3e-985b-4afa-ba92-3238f008389d",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_prompt = '<OD>'\n",
    "results = run_example(image, task_prompt)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f677831f-544a-444d-b4cb-ecfd7cb31c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt  \n",
    "import matplotlib.patches as patches  \n",
    "\n",
    "# bounding box 그려주기 \n",
    "# data['bboxes], data['labels'] 가 있어야 함\n",
    "def plot_bbox(image, data):\n",
    "   # Create a figure and axes  \n",
    "    fig, ax = plt.subplots()  \n",
    "      \n",
    "    # Display the image  \n",
    "    ax.imshow(image)  \n",
    "      \n",
    "    # Plot each bounding box  \n",
    "    for bbox, label in zip(data['bboxes'], data['labels']):  \n",
    "        # Unpack the bounding box coordinates  \n",
    "        x1, y1, x2, y2 = bbox  \n",
    "        # Create a Rectangle patch  \n",
    "        rect = patches.Rectangle((x1, y1), x2-x1, y2-y1, linewidth=1, edgecolor='r', facecolor='none')  \n",
    "        # Add the rectangle to the Axes  \n",
    "        ax.add_patch(rect)  \n",
    "        # Annotate the label  \n",
    "        plt.text(x1, y1, label, color='white', fontsize=8, bbox=dict(facecolor='red', alpha=0.5))  \n",
    "      \n",
    "    # Remove the axis ticks and labels  \n",
    "    ax.axis('off')  \n",
    "      \n",
    "    # Show the plot  \n",
    "    plt.show()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732f9158-d877-41ce-add5-a202daefb033",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bbox(image, results['<OD>'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66f2fcd-440e-45be-bffc-92ce7f73a0c1",
   "metadata": {},
   "source": [
    "## Dense region caption\n",
    "- 영상내에서 region에 대한 phrase로 표현"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777ccab4-e373-4743-b936-0e68d6e95886",
   "metadata": {},
   "source": [
    "Dense region caption results format: {'<DENSE_REGION_CAPTION>': {'bboxes': [[x1, y1, x2, y2], ...], 'labels': ['label1', 'label2', ...]}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843bccc4-b9ea-4bf8-9c4e-4388619e2060",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_prompt = '<DENSE_REGION_CAPTION>'\n",
    "results = run_example(image, task_prompt)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03dd11d2-d5cc-451c-aef9-4e57650034c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bbox(image, results['<DENSE_REGION_CAPTION>'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506d3bbc-a53b-453d-9b0a-c0e94eba394a",
   "metadata": {},
   "source": [
    "## Region proposal\n",
    "- object region proposal: 물체 후보 영상 검출"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5ad2db-2c92-4f3b-8806-f7c60b419dbd",
   "metadata": {},
   "source": [
    "Region proposal results format: {'<REGION_PROPOSAL>' : {'bboxes': [[x1, y1, x2, y2], ...], 'labels': ['', '', ...]}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c2230a-35a6-4a87-80c6-67d4361f53d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_prompt = '<REGION_PROPOSAL>'\n",
    "results = run_example(image, task_prompt)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadcf8ae-ccd9-458b-9335-6dd0a19ea1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bbox(image, results['<REGION_PROPOSAL>'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe94884-c2ec-4381-af31-f979ae8d07aa",
   "metadata": {},
   "source": [
    "## Run pre-defined tasks that requires additional inputs\n",
    "- task_prompt외 text_input이 필요한 경우\n",
    "- visual grounding: 해당 문자 표현의 영역을 검출\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f71a518-e55c-4683-bc0b-6ebe74015d8e",
   "metadata": {},
   "source": [
    "### Phrase Grounding\n",
    "- (visual) phrase grounding: 해당 문자 어구 표현의 영역을 검출\n",
    "- input: image, task_prompt(task 지정), text_input(문자 표현)\n",
    "- output: BB's"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122d861a-8380-472b-aefc-066b5a89a61d",
   "metadata": {},
   "source": [
    "Phrase grounding results format: {'<CAPTION_TO_PHRASE_GROUNDING>': {'bboxes': [[x1, y1, x2, y2], ...], 'labels': ['', '', ...]}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2f82d9-7ba3-42a1-9885-5ba466007b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_prompt = '<CAPTION_TO_PHRASE_GROUNDING>'\n",
    "results = run_example(image, task_prompt, text_input=\"A green car parked in front of a yellow building.\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7d6747-cb53-474c-9d55-fd44062f9992",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bbox(image, results['<CAPTION_TO_PHRASE_GROUNDING>'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655eb9ad-5c24-47f4-bfb7-23fe1a17097d",
   "metadata": {},
   "source": [
    "## Referring expression segmentation\n",
    "- 지정하는 표현에 대한 image에서의 영역을 segmenation mask로 검출\n",
    "- input: image, task_prompt, text_input(segmentatioin하고자 하는 지정하는 text 표현)\n",
    "- output: polygon mask "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7433d737-9e1f-48bf-a933-58f288008314",
   "metadata": {},
   "source": [
    "Referring expression segmentation results format: {'<REFERRING_EXPRESSION_SEGMENTATION>': {'Polygons': [[[polygon]], ...], 'labels': ['', '', ...]}}, one object is represented by a list of polygons. each polygon is [x1, y1, x2, y2, ..., xn, yn]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9552a66-ee97-4dd5-9ea7-e411c0a8e131",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_prompt = '<REFERRING_EXPRESSION_SEGMENTATION>'\n",
    "results = run_example(image, task_prompt, text_input=\"a green car\")\n",
    "# print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2635e7f-a7f2-4b31-86fb-8d0d257c5025",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw, ImageFont \n",
    "import random\n",
    "import numpy as np\n",
    "colormap = ['blue','orange','green','purple','brown','pink','gray','olive','cyan','red',\n",
    "            'lime','indigo','violet','aqua','magenta','coral','gold','tan','skyblue']\n",
    "\n",
    "# segmentation의 output인 polygon을 영상 위에 그리기\n",
    "def draw_polygons(image, prediction, fill_mask=False):  \n",
    "    \"\"\"  \n",
    "    Draws segmentation masks with polygons on an image.  \n",
    "  \n",
    "    Parameters:  \n",
    "    - image_path: Path to the image file.  \n",
    "    - prediction: Dictionary containing 'polygons' and 'labels' keys.  \n",
    "                  'polygons' is a list of lists, each containing vertices of a polygon.  \n",
    "                  'labels' is a list of labels corresponding to each polygon.  \n",
    "    - fill_mask: Boolean indicating whether to fill the polygons with color.  \n",
    "    \"\"\"  \n",
    "    # Load the image  \n",
    "   \n",
    "    draw = ImageDraw.Draw(image)  \n",
    "      \n",
    "   \n",
    "    # Set up scale factor if needed (use 1 if not scaling)  \n",
    "    scale = 1  \n",
    "      \n",
    "    # Iterate over polygons and labels  \n",
    "    for polygons, label in zip(prediction['polygons'], prediction['labels']):  \n",
    "        color = random.choice(colormap)  \n",
    "        fill_color = random.choice(colormap) if fill_mask else None  \n",
    "          \n",
    "        for _polygon in polygons:  \n",
    "            _polygon = np.array(_polygon).reshape(-1, 2)  \n",
    "            if len(_polygon) < 3:  \n",
    "                print('Invalid polygon:', _polygon)  \n",
    "                continue  \n",
    "              \n",
    "            _polygon = (_polygon * scale).reshape(-1).tolist()  \n",
    "              \n",
    "            # Draw the polygon  \n",
    "            if fill_mask:  \n",
    "                draw.polygon(_polygon, outline=color, fill=fill_color)  \n",
    "            else:  \n",
    "                draw.polygon(_polygon, outline=color)  \n",
    "              \n",
    "            # Draw the label text  \n",
    "            draw.text((_polygon[0] + 8, _polygon[1] + 2), label, fill=color)  \n",
    "  \n",
    "    # Save or display the image  \n",
    "    #image.show()  # Display the image  \n",
    "    display(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e99a262-72e0-4c31-a145-778fac957f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_image = copy.deepcopy(image)\n",
    "draw_polygons(output_image, results['<REFERRING_EXPRESSION_SEGMENTATION>'], fill_mask=True)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c239327-c9ee-4905-8436-1f23daa2a1c0",
   "metadata": {},
   "source": [
    "## Region to segmentation\n",
    "- region내 segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e9da8d-9573-4ea9-8648-2479754ba466",
   "metadata": {},
   "source": [
    "with additional region as inputs, format is '<loc_x1><loc_y1><loc_x2><loc_y2>', [x1, y1, x2, y2] is the quantized corrdinates in [0, 999]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4489d96e-1bb6-474e-9d8d-245e44c7dcd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_prompt = '<REGION_TO_SEGMENTATION>'\n",
    "results = run_example(image, task_prompt, text_input=\"<loc_702><loc_575><loc_866><loc_772>\")\n",
    "# print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3b03d4-bb41-4c3b-b65f-00880c4f8534",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_region = {}\n",
    "input_region['bboxes'] = [[0.702*image.width, 0.575*image.height, 0.866*image.width, 0.772*image.height ]]\n",
    "input_region['labels']= ['']\n",
    "plot_bbox(image, input_region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6889b6-c575-4cac-8a63-6428dc785052",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_image = copy.deepcopy(image)\n",
    "draw_polygons(output_image, results['<REGION_TO_SEGMENTATION>'], fill_mask=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96952d9-7994-490d-8235-f398ca0b72cf",
   "metadata": {},
   "source": [
    "## Open vocabulary detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28e8777-a6bb-4258-9552-0a6fa7a5b211",
   "metadata": {},
   "source": [
    "open vocabulary detection can detect both objects and ocr texts.\n",
    "\n",
    "results format:\n",
    "\n",
    "{ '<OPEN_VOCABULARY_DETECTION>': {'bboxes': [[x1, y1, x2, y2], [x1, y1, x2, y2], ...]], 'bboxes_labels': ['label_1', 'label_2', ..], 'polygons': [[[x1, y1, x2, y2, ..., xn, yn], [x1, y1, ..., xn, yn]], ...], 'polygons_labels': ['label_1', 'label_2', ...] }}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0cf9670-357c-4da5-91d1-31110cf4d12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_prompt = '<OPEN_VOCABULARY_DETECTION>'\n",
    "results = run_example(image, task_prompt, text_input=\"a green car\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a55163-ea91-4ce1-b726-f53cd2909cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_od_format(data):  \n",
    "    \"\"\"  \n",
    "    Converts a dictionary with 'bboxes' and 'bboxes_labels' into a dictionary with separate 'bboxes' and 'labels' keys.  \n",
    "  \n",
    "    Parameters:  \n",
    "    - data: The input dictionary with 'bboxes', 'bboxes_labels', 'polygons', and 'polygons_labels' keys.  \n",
    "  \n",
    "    Returns:  \n",
    "    - A dictionary with 'bboxes' and 'labels' keys formatted for object detection results.  \n",
    "    \"\"\"  \n",
    "    # Extract bounding boxes and labels  \n",
    "    bboxes = data.get('bboxes', [])  \n",
    "    labels = data.get('bboxes_labels', [])  \n",
    "      \n",
    "    # Construct the output format  \n",
    "    od_results = {  \n",
    "        'bboxes': bboxes,  \n",
    "        'labels': labels  \n",
    "    }  \n",
    "      \n",
    "    return od_results  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae5c8f2-e5eb-42db-8f89-b3384d8a8e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox_results  = convert_to_od_format(results['<OPEN_VOCABULARY_DETECTION>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c7a1c5-aeaa-49f0-8a8a-f30415ca5f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bbox(image, bbox_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d074aaa2-431a-48ba-93b1-b11b0d4d9cbf",
   "metadata": {},
   "source": [
    "### region to texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265dc9f4-a6a0-44d3-9e6d-a7b8e0785ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_prompt = '<REGION_TO_CATEGORY>'\n",
    "results = run_example(image, task_prompt, text_input=\"<loc_52><loc_332><loc_932><loc_774>\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f42fdb-f674-47b6-b112-6a7b757d7293",
   "metadata": {},
   "source": [
    "### ocr related tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85bd98d6-6241-4dfb-903a-8a4451d8a887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you can directly access the file, you may use this\n",
    "#url = \"http://ecx.images-amazon.com/images/I/51UUzBDAMsL.jpg?download=true\"\n",
    "#image = Image.open(requests.get(url, stream=True).raw).convert('RGB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179fe4a4-df38-4636-955b-80871ec6cc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to use local file access\n",
    "image_path = '/group-volume/sr_edu/AI-Application-Specialist-Vision-Dataset/hf-assets/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cbd706-2f96-4b9a-bd3e-29ac61267c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls $image_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8258c4fa-af9d-4dec-9f96-a6ff3ae2d1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(image_path+'/51UUzBDAMsL.jpg')\n",
    "plt.imshow(image)\n",
    "plt.axis('off')\n",
    "plt.title(f'image size:{image.size}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbba0f9-0651-4418-b9aa-82bf52aa25b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_prompt = '<OCR>'\n",
    "run_example(image, task_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685ac7a8-f069-476d-96c4-c81f5ae998de",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_prompt = '<OCR_WITH_REGION>'\n",
    "results = run_example(image, task_prompt)\n",
    "print(results)\n",
    "# ocr results format\n",
    "# {'OCR_WITH_REGION': {'quad_boxes': [[x1, y1, x2, y2, x3, y3, x4, y4], ...], 'labels': ['text1', ...]}}\n",
    "# quad_boxes: (검출된 문자 영역을 표현하는) 4개의 점(point), top-left를 시작으로 clockwise 방향, 예) 회전된 문자 열 등 표현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2856c74-e758-44d8-9bdc-a7f3acdf80d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_ocr_bboxes(image, prediction, scale=1):\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    bboxes, labels = prediction['quad_boxes'], prediction['labels']\n",
    "    for box, label in zip(bboxes, labels):\n",
    "        color = random.choice(colormap)\n",
    "        new_box = (np.array(box) * scale).tolist()\n",
    "        draw.polygon(new_box, width=3, outline=color)\n",
    "        ''' \n",
    "        draw.text((new_box[0]+8, new_box[1]+2),\n",
    "                    \"{}\".format(label),\n",
    "                    align=\"right\",\n",
    "        \n",
    "                    fill=color)\n",
    "        '''\n",
    "    display(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35d16a5-4520-47fa-934e-398945f6383d",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_image = copy.deepcopy(image)\n",
    "w, h = output_image.size\n",
    "scale = 800 / max(w, h)\n",
    "new_output_image = output_image.resize((int(w * scale), int(h * scale)))\n",
    "draw_ocr_bboxes(new_output_image, results['<OCR_WITH_REGION>'], scale=scale)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1795f584-f657-4618-a5e9-b1e8fbed6356",
   "metadata": {},
   "source": [
    "## Cascaded tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe8dc9a-9cf5-47d7-a36b-57f37ed96abb",
   "metadata": {},
   "source": [
    "### Caption + Phrase Grounding\n",
    "- input image에 대해 image description(captioning)을 하고, \n",
    "이 표현에 사용된 phrase에 대해 visual grounding함으로써 captioning에 대한 근거도 알수 있음\n",
    "- 2 개의 step으로 구성됨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1bc3098-8e10-4ccc-85b7-bc7ad50ff81f",
   "metadata": {},
   "source": [
    "Caption + Phrase Grounding\n",
    "results format:\n",
    "\n",
    "{ '<CAPTION': pure_text, {'<CAPTION_TO_PHRASE_GROUNDING>': {'bboxes': [[x1, y1, x2, y2], ...], 'labels': ['', '', ...]}} }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63272498-291b-4959-9288-f10532ef6726",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/car.jpg?download=true\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4f93e3-158c-4894-aa43-0b7426a12917",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1. image captioning \n",
    "task_prompt = '<CAPTION>'\n",
    "results = run_example(image, task_prompt)\n",
    "text_input = results[task_prompt]\n",
    "# step 2. caption을 입력으로해서, visual grounding\n",
    "task_prompt = '<CAPTION_TO_PHRASE_GROUNDING>'\n",
    "results = run_example(image, task_prompt, text_input)\n",
    "results['<CAPTION>'] = text_input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb342cac-61b0-44b0-89c7-bf812bfc279f",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3436599a-2d7b-41e7-af82-ac4b07af046b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bbox(image, results['<CAPTION_TO_PHRASE_GROUNDING>'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a7acd8-104a-4bdb-b745-0373c322829e",
   "metadata": {},
   "source": [
    "### Detailed Caption + Phrase Grounding\n",
    "- 위에서 한 방법과 유사하나, detailed_caption을 사용함\n",
    "즉, image에 대한 상세한 설명과 설명내에 포함된 표현에 대한 visual grounding함\n",
    "- 2 step으로 구성됨: detailed_caption, caption_to_phrase_grounding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec89fc0-c29b-41f3-a722-af7f233b8885",
   "metadata": {},
   "source": [
    "results format:\n",
    "\n",
    "{ '<DETAILED_CAPTION': pure_text, {'<CAPTION_TO_PHRASE_GROUNDING>': {'bboxes': [[x1, y1, x2, y2], ...], 'labels': ['', '', ...]}} }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458a946a-58f4-4c24-a9a7-a975886965e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_prompt = '<DETAILED_CAPTION>'\n",
    "results = run_example(task_prompt)\n",
    "text_input = results[task_prompt]\n",
    "task_prompt = '<CAPTION_TO_PHRASE_GROUNDING>'\n",
    "results = run_example(task_prompt, text_input)\n",
    "results['<DETAILED_CAPTION>'] = text_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e187f890-1d16-4a46-b305-c458d6a3cb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51648b69-6bac-47c9-8f21-2c308ef6c6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bbox(image, results['<CAPTION_TO_PHRASE_GROUNDING>'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e89e6b-d7cf-47d6-b848-9f1d831c39ba",
   "metadata": {},
   "source": [
    "### More Detailed Caption + Phrase Grounding\n",
    "- 'MORE_DETAILED_CAPTION'을 사용하여 영상을 문자로 표현하고, 이 표현에 사용한 phrase에 대한 visual grounding "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6fa54a-d092-4652-9c48-5785dcc9ee41",
   "metadata": {},
   "source": [
    "results format:\n",
    "\n",
    "{ '<MORE_DETAILED_CAPTION': pure_text, {'<CAPTION_TO_PHRASE_GROUNDING>': {'bboxes': [[x1, y1, x2, y2], ...], 'labels': ['', '', ...]}} }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f98652-84ea-4e17-891f-2a306279c71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_prompt = '<MORE_DETAILED_CAPTION>'\n",
    "results = run_example(image, task_prompt)\n",
    "text_input = results[task_prompt]\n",
    "task_prompt = '<CAPTION_TO_PHRASE_GROUNDING>'\n",
    "results = run_example(image, task_prompt, text_input)\n",
    "results['<MORE_DETAILED_CAPTION>'] = text_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb97038-4e39-4925-9a05-d6b1c087ba1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3ab183-135e-43c6-907e-fae5899be56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bbox(image, results['<CAPTION_TO_PHRASE_GROUNDING>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a8271a-44a7-4f42-b5ed-0b04d8107148",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7986d45f-bfcb-4e98-aa1c-a06270567ec3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
