{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48be7ce9",
   "metadata": {},
   "source": [
    "## DDPM Forward process (Noising process) - Image Case \n",
    "- 임의의 입력 이미지에 대해 DDPM forward process를 거친 영상을 시각화한다.\n",
    "- image 차원 x0[C, H, W] 에 정규 분포 noise가 각 각 pixel의 RGB별로 추가됨을 이해하자.\n",
    "- linear beta로 noise schedule했을 때 영상의 변화를 관찰해 보자.\n",
    "- 학습 데이터에서 추출하는 어떠한 image에 대해서도 N(0,1) 표준 정규 분포의 noise image로 변환됨을 이해해보자.  \n",
    "\n",
    "\n",
    "- Reference: [annotated diffusion](https://huggingface.co/blog/annotated-diffusion)\n",
    "- Requirements: pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cee36a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image as DisplayImage\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232fb479-9fcf-4782-b7a6-2ef7d579ad8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Azure Cloud\n",
    "n_drive = '\\\\\\\\swschoolavdazfiles002.file.core.windows.net\\\\aias-vision'\n",
    "dataset_path = n_drive + '\\\\' + 'AI-Application-Specialist-Vision-Dataset' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0add11",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#DisplayImage(filename='/group-volume/sr_edu/AI-Application-Specialist-Vision-Dataset/hf-assets/ddpm_paper.png', width=600)\n",
    "DisplayImage(filename=dataset_path+'/hf-assets/ddpm_paper.png', width=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1080a5",
   "metadata": {},
   "source": [
    "## Define the forward diffusion process\n",
    "\n",
    "### Define Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc1bae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_beta_schedule(timesteps): ## DDPM original scheduler\n",
    "    beta_start = 0.0001\n",
    "    beta_end = 0.02\n",
    "    # torch.linspace는 start에서 end를 timesteps 갯수 만큼 등간격으로 나눔(evenly spaced)\n",
    "    # 단, inclusive start, end \n",
    "    # https://pytorch.org/docs/stable/generated/torch.linspace.html\n",
    "    return torch.linspace(beta_start, beta_end, timesteps) \n",
    "\n",
    "def cosine_beta_schedule(timesteps, s=0.008):\n",
    "    \"\"\"\n",
    "    cosine schedule as proposed in https://arxiv.org/abs/2102.09672\n",
    "    \"\"\"\n",
    "    steps = timesteps + 1\n",
    "    x = torch.linspace(0, timesteps, steps)\n",
    "    alphas_cumprod = torch.cos(((x / timesteps) + s) / (1 + s) * torch.pi * 0.5) ** 2\n",
    "    alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n",
    "    betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n",
    "    return torch.clip(betas, 0.0001, 0.9999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227fd6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "timesteps = 1000\n",
    "\n",
    "# define beta schedule\n",
    "betas = linear_beta_schedule(timesteps=timesteps)\n",
    "\n",
    "# define alphas \n",
    "alphas = 1. - betas\n",
    "alphas_cumprod = torch.cumprod(alphas, axis=0) # alpha_bars\n",
    "\n",
    "sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod)\n",
    "sqrt_one_minus_alphas_cumprod = torch.sqrt(1. - alphas_cumprod)\n",
    "\n",
    "# array a에서 t번째 index B개 추출(gather)\n",
    "# input: a(array), t(B개)\n",
    "# return: out[B,1,1,1] (B: batch_size == t.shape[0])\n",
    "#        image: x_shape = [C, H, W] 와 계산위해 shape 맞춰줌\n",
    "def extract(a, t, x_shape):\n",
    "    batch_size = t.shape[0]\n",
    "    out = a.gather(-1, t.cpu()) \n",
    "    return out.reshape(batch_size, *((1,) * (len(x_shape) - 1))).to(t.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedc33a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "betas.shape, alphas.shape, alphas_cumprod.shape, betas[-1:], alphas[:5], alphas_cumprod[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc29d44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = torch.linspace(1, timesteps, timesteps)\n",
    "plt.plot(tt, sqrt_alphas_cumprod, label=r'$\\sqrt {\\bar {\\alpha}} $')\n",
    "plt.plot(tt, sqrt_one_minus_alphas_cumprod, label=r'$\\sqrt {1- \\bar {\\alpha} }$')\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f99075b",
   "metadata": {},
   "source": [
    "### Load an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297fd70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1356e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n",
    "image = Image.open(requests.get(url, stream=True).raw) # PIL image of shape HWC\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8c8988",
   "metadata": {},
   "source": [
    "### Pre-processing \n",
    "- using torchvision transforms\n",
    "- transform a PIL image to a normalized PyTorch tensor in [-1, 1] \n",
    "  * input: image (PIL image, HWC format)\n",
    "  * output: tensor ([-1, 1], CHW format)\n",
    "- unsqueeze(0): add axis=0 (for batch processing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fad6502",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import Compose, ToTensor, Lambda, ToPILImage, CenterCrop, Resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3284b19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 128\n",
    "\n",
    "# PIL image (HWC) to normalized torch image [-1,1]\n",
    "image2tensor = Compose([\n",
    "    Resize(image_size),\n",
    "    CenterCrop(image_size),\n",
    "    ToTensor(), # turn into torch Tensor of shape CHW, divide by 255, in the range [0,1]\n",
    "    Lambda(lambda t: (t * 2) - 1),    \n",
    "])\n",
    "\n",
    "x_start = image2tensor(image).unsqueeze(0)\n",
    "image.size, image.mode, x_start.shape, x_start.min(), x_start.max() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0acd69b5",
   "metadata": {},
   "source": [
    "### Post-processing \n",
    "- to transform a PyTorch tensor normalized in [-1,1] to a PIL Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47fd7689",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor2image = Compose([\n",
    "     Lambda(lambda t: (t + 1) / 2),\n",
    "     Lambda(lambda t: t.permute(1, 2, 0)), # CHW to HWC\n",
    "     Lambda(lambda t: torch.clamp(t * 255., min=0, max=255)),\n",
    "     Lambda(lambda t: t.numpy().astype(np.uint8)),\n",
    "     ToPILImage(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd59efef",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor2image(x_start.squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294a277d",
   "metadata": {},
   "source": [
    "### Apply forward diffusion process \n",
    "#### to get a noisy image at time=t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682252df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward diffusion q (using the nice property)\n",
    "# input: x_start(x0: [C, H, W]), t: [B]\n",
    "# output: xt[B(t), C, H, W]\n",
    "def q_sample(x_start, t, noise=None):\n",
    "    if noise is None:\n",
    "        noise = torch.randn_like(x_start)\n",
    "\n",
    "    sqrt_alphas_cumprod_t = extract(sqrt_alphas_cumprod, t, x_start.shape)\n",
    "    sqrt_one_minus_alphas_cumprod_t = extract(\n",
    "        sqrt_one_minus_alphas_cumprod, t, x_start.shape\n",
    "    )\n",
    "\n",
    "    return sqrt_alphas_cumprod_t * x_start + sqrt_one_minus_alphas_cumprod_t * noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be5d736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to get a PIL Image from x_t (tensor)\n",
    "# x_start = x0\n",
    "def get_noisy_image(x_start, t):\n",
    "    # add noise\n",
    "    x_noisy = q_sample(x_start, t=t)\n",
    "\n",
    "    # turn back into PIL image\n",
    "    noisy_image = tensor2image(x_noisy.squeeze())\n",
    "\n",
    "    return noisy_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f693de82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [0,T-1]에서 임의의 time step t 에서  noise image 그려보자\n",
    "t = torch.tensor([40])\n",
    "\n",
    "get_noisy_image(x_start, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626dc0d2",
   "metadata": {},
   "source": [
    "### To visualize this for various time steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa72e2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use seed for reproducability\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# source: https://pytorch.org/vision/0.15/auto_examples/plot_transforms.html \n",
    "def plot_images(imgs, with_orig=False, row_title=None, **imshow_kwargs):\n",
    "    if not isinstance(imgs[0], list):\n",
    "        # Make a 2d grid even if there's just 1 row\n",
    "        imgs = [imgs]\n",
    "\n",
    "    num_rows = len(imgs)\n",
    "    num_cols = len(imgs[0]) + with_orig\n",
    "    fig, axs = plt.subplots(figsize=(200,200), nrows=num_rows, ncols=num_cols, squeeze=False)\n",
    "    for row_idx, row in enumerate(imgs):\n",
    "        row = [image] + row if with_orig else row\n",
    "        for col_idx, img in enumerate(row):\n",
    "            ax = axs[row_idx, col_idx]\n",
    "            ax.imshow(np.asarray(img), **imshow_kwargs)\n",
    "            ax.set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
    "\n",
    "    if with_orig:\n",
    "        axs[0, 0].set(title='Original image')\n",
    "        axs[0, 0].title.set_size(8)\n",
    "    if row_title is not None:\n",
    "        for row_idx in range(num_rows):\n",
    "            axs[row_idx, 0].set(ylabel=row_title[row_idx])\n",
    "\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa8a36f",
   "metadata": {},
   "source": [
    "## forward diffusion process\n",
    "- $x_t(x_0)$ ~ $ q(x_t|x_0) = N(\\mu_t , \\sigma_t ^2)$, where $\\mu_t = \\sqrt{\\overline \\alpha_t} x_0 , \\sigma_t = \\sqrt{1-\\overline \\alpha_t}$\n",
    "- 정규 분포를 사용하여 데이터 샘플링: $x_t =\\sqrt{\\overline \\alpha_t} x_0 + \\sqrt{1-\\overline \\alpha_t}\\epsilon$ \n",
    "  * where $\\epsilon$ ~ $N(0 , 1^2)$\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed12924",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_images([get_noisy_image(x_start, torch.tensor([t])) for t in [0, 50, 100, 150, 199]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a347dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_images([get_noisy_image(x_start, torch.tensor([t])) \n",
    "             for t in [0, 100, 200, 300, 400, 500, 600, 700, 800, 999]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de949ce-effc-4e51-986a-d3b472087620",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
