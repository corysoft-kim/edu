{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8090a9ae-3e91-443c-b5ac-dd0b839d1545",
   "metadata": {},
   "source": [
    "# ResNet v2 model 학습 및 평가 실습\n",
    "- dataset: cifar10\n",
    "- AI tool: **pytorch**\n",
    "- Reference\n",
    "  * [cifar10-resnet-keras](https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Keras/blob/master/chapter2-deep-networks/resnet-cifar10-2.2.1.py)\n",
    "  * [cifar10-tutorial](https://docs.pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html)\n",
    "  * [cifar10-resnet-pytorch](https://www.kaggle.com/code/kannapat/cifar10-with-vgg-and-resnet-in-pytorch)\n",
    "  * [torchvision.models.resnet](https://docs.pytorch.org/vision/0.9/_modules/torchvision/models/resnet.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98813ad9-d0b2-49d0-b763-299df0d2a8d7",
   "metadata": {},
   "source": [
    "### print out model summary \n",
    "- install torchinfo to use sumamry of model\n",
    "```python\n",
    "\n",
    "try:\n",
    "  import torchinfo\n",
    "except:\n",
    "  !pip install torchinfo\n",
    "  import torchinfo\n",
    "\n",
    "from torchinfo import summary\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05b8ce6-62b5-493f-abe8-80c368d6919b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, random_split\n",
    "\n",
    "# Import torchvison\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import ToPILImage\n",
    "from torchvision.transforms import v2\n",
    "from torchvision.transforms import Normalize, Resize, ToTensor, Compose\n",
    "from torchvision.io import decode_image\n",
    "# Import dataset to load cifar10\n",
    "from torchvision import datasets\n",
    "\n",
    "from torchinfo import summary\n",
    "# PIL Image\n",
    "from PIL import Image\n",
    "# Import for model evaluation\n",
    "# for plot\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sn\n",
    "\n",
    "# for confusion matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "from datetime import datetime\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "import random\n",
    "import os\n",
    "\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af98bdf-50db-47ae-bdfc-5f1f969678a9",
   "metadata": {},
   "source": [
    "## 0. Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5269ac72-1cb3-40a9-a204-bc2d9930c2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device:{device}\")\n",
    "\n",
    "cache_dir = 'D:\\\\HF_cache'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba9410b-ddcc-4761-a5f4-1b7f0eb2dc38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurations\n",
    "num_classes = 10 # cifar10 classes : fixed\n",
    "batch_size = 128 # 32, 64, 128, 256 # orig paper trained all networks with batch_size=128\n",
    "epochs = 120"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb01fea-27b9-4352-a7fd-ca069c9cc785",
   "metadata": {},
   "source": [
    "### Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd69e37-c92f-4981-8831-2eb782209a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model version\n",
    "version = 2 # fixed\n",
    "n = 2 # number of residual blocks per stage\n",
    "# detpth = n * 9 + 2 # ResNet-v2 depth calculation\n",
    "depth = n * 9 + 2\n",
    "\n",
    "# model name, depth and version\n",
    "model_type = 'ResNet%dv%d' % (depth, version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690d51d1-9f7b-47e6-b2aa-66186e63eb7f",
   "metadata": {},
   "source": [
    "## 1. Dataset Preparation\n",
    "- torch의 torch.utils.data.dataset, torch.utils.data.dataloader 사용\n",
    "  * dataset: data를 일정한 포맷으로 정리해서 넣어 두고 필요할 때 하나씩 꺼낼 수 있도록 정의하고 있음 (x, y)\n",
    "  * dataloader: batch 단위로 꺼내 주고, shuffle 또는 병렬로 꺼내주는 기능\n",
    "- 참조 [data_tutorial](https://docs.pytorch.org/tutorials/beginner/basics/data_tutorial.html), \n",
    "[한국어](https://tutorials.pytorch.kr/beginner/basics/data_tutorial.html)\n",
    "### Loading the Data\n",
    "- dataset load시에 사용할 data augmentation 방법 지정\n",
    "- Data Augmentation and preprocessing\n",
    "  * Simple normalization ([0,255] --> [-1,1]) + alpha\n",
    "  * 학습 데이터의 mean과 std를 구하여 normalize할 수도 있습니다.\n",
    "      * mean = cifar10_mean\n",
    "      * std = cifar10_std\n",
    "      \n",
    "### 1.1 GPU 전체 데이터셋을 올림.\n",
    "- 속도가 빠름, GPU memory 사용\n",
    "- 데이터셋이 GPU memory에 올라갈 정도로 작은 경우만 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56036350-2fb4-4291-89d7-291d360f15b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset data augmentation for training and test dataset\n",
    "data_augmentation = True # default=True\n",
    "\n",
    "# dataset을 모두 gpu에 올려서 사용하면 true, batch 단위로 gpu에 copy하면 False\n",
    "# dataset, dataloader, augmentation 방법에 차이가 있음\n",
    "gpu_to_all = True # default=True\n",
    "\n",
    "# zero_init_residual: ResNet 모델에서 Bottleneck arch 의 세번째 conv의 init를 zero로 initalization(True(default))\n",
    "zero_init_residual = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3e2a0e-1aea-4f6b-a91c-b28bf55ca867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU에 전체 데이터를 올림\n",
    "def load_all_to_gpu(dataset):\n",
    "    data_loader = DataLoader(dataset, batch_size=len(dataset))\n",
    "    images, labels = next(iter(data_loader))\n",
    "    return images.to(device), labels.to(device)\n",
    "    \n",
    "# Custom Dataset 클래스 정의 (transform 적용용)\n",
    "class TransformedTensorDataset(Dataset):\n",
    "    def __init__(self, images, labels, cifar10_class_names, transform=None):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        self.classes = cifar10_class_names  #cifar10_dataset.classes ## class names[str]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db801ad0-0c42-4368-81bf-38e12edd4f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomHorizontalShiftWithZeroPad(v2.Transform):\n",
    "    def __init__(self, shift_range):\n",
    "        super().__init__()\n",
    "        self.shift_range = shift_range # 이미지 너비 대비 비율\n",
    "\n",
    "    def _get_params(self, flat_inputs):\n",
    "        # 이동할 픽셀 수를 계산\n",
    "        img_width = flat_inputs[0].shape[-1]\n",
    "        max_shift_pixels = int(img_width * self.shift_range)\n",
    "        \n",
    "        # -max_shift_pixels부터 max_shift_pixels까지의 정수 랜덤 값\n",
    "        shift_pixels = torch.randint(-max_shift_pixels, max_shift_pixels + 1, (1,)).item()\n",
    "        return shift_pixels\n",
    "\n",
    "    def _transform(self, inpt, params):\n",
    "        shift_pixels = params\n",
    "        img_height, img_width = inpt.shape[-2], inpt.shape[-1]\n",
    "        \n",
    "        # 원본 이미지보다 큰 제로 텐서 생성\n",
    "        padded_width = img_width + abs(shift_pixels) * 2\n",
    "        padded_height = img_height\n",
    "        \n",
    "        # 패딩된 텐서를 GPU에 생성\n",
    "        zero_padded_tensor = torch.zeros(inpt.shape[:-2] + (padded_height, padded_width),\n",
    "                                         dtype=inpt.dtype, device=inpt.device)\n",
    "        \n",
    "        # 원본 이미지를 랜덤하게 패딩된 텐서에 복사\n",
    "        x_start = abs(shift_pixels) + shift_pixels\n",
    "        zero_padded_tensor[..., :, x_start:x_start+img_width] = inpt\n",
    "        \n",
    "        # 중심에서 원본 크기만큼 크롭\n",
    "        x_crop_start = (padded_width - img_width) // 2\n",
    "        shifted_inpt = zero_padded_tensor[..., :, x_crop_start:x_crop_start+img_width]\n",
    "        \n",
    "        return shifted_inpt\n",
    "\n",
    "class RandomVerticalShiftWithZeroPad(v2.Transform):\n",
    "    def __init__(self, shift_range):\n",
    "        super().__init__()\n",
    "        # 이미지 높이 대비 이동 비율\n",
    "        self.shift_range = shift_range \n",
    "\n",
    "    def _get_params(self, flat_inputs):\n",
    "        # 이미지 높이를 기반으로 이동할 픽셀 수를 계산\n",
    "        img_height = flat_inputs[0].shape[-2]\n",
    "        max_shift_pixels = int(img_height * self.shift_range)\n",
    "        \n",
    "        # -max_shift_pixels부터 max_shift_pixels까지의 정수 랜덤 값\n",
    "        shift_pixels = torch.randint(-max_shift_pixels, max_shift_pixels + 1, (1,)).item()\n",
    "        return shift_pixels\n",
    "\n",
    "    def _transform(self, inpt, params):\n",
    "        shift_pixels = params\n",
    "        img_height, img_width = inpt.shape[-2], inpt.shape[-1]\n",
    "        \n",
    "        # 원본 이미지보다 큰 제로 텐서 생성\n",
    "        padded_width = img_width\n",
    "        padded_height = img_height + abs(shift_pixels) * 2\n",
    "        \n",
    "        # 패딩된 텐서를 GPU에 생성\n",
    "        zero_padded_tensor = torch.zeros(inpt.shape[:-2] + (padded_height, padded_width),\n",
    "                                         dtype=inpt.dtype, device=inpt.device)\n",
    "        \n",
    "        # 원본 이미지를 랜덤하게 패딩된 텐서에 복사\n",
    "        y_start = abs(shift_pixels) + shift_pixels\n",
    "        zero_padded_tensor[..., y_start:y_start+img_height, :] = inpt\n",
    "        \n",
    "        # 중심에서 원본 크기만큼 크롭\n",
    "        y_crop_start = (padded_height - img_height) // 2\n",
    "        shifted_inpt = zero_padded_tensor[..., y_crop_start:y_crop_start+img_height, :]\n",
    "        \n",
    "        return shifted_inpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b338e5e0-5b9a-4c70-a006-d85d4ff91f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### batch_size, shuffle=True for train_dataloader\n",
    "def create_dataloader_all_to_gpu(batch_size_, val_split_ratio = 0.2):\n",
    "    # 1. CIFAR-10 다운로드 및 변환 (ToTensor로만 변환, Normalize는 선택)\n",
    "    tensor_transform_a = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # 정규화 : y = (x-m)/std\n",
    "    ])\n",
    "    \n",
    "    '''\n",
    "        datasets.CIFAR10(root, train, download, transform)\n",
    "        root='./data', # where to download data to (cache_dir)\n",
    "        train=True, # get training data(True) or test data(False)\n",
    "        download=True, # download data if it doesn't exist on disk\n",
    "        transform=tensor_transform, # images come as PIL format, we want to turn into Torch tensors\n",
    "        target_transform=None # you can transform labels as well\n",
    "    '''\n",
    "    # 1. CIFAR-10 다운로드 및 변환 (ToTensor로만 변환, Normalize는 선택)\n",
    "    cifar10_train_dataset = datasets.CIFAR10(root=cache_dir, train=True, download=True, transform=tensor_transform_a)\n",
    "    cifar10_test_dataset = datasets.CIFAR10(root=cache_dir, train=False, download=True, transform=tensor_transform_a)\n",
    "\n",
    "    # split train to train, val\n",
    "    val_ratio = val_split_ratio\n",
    "    if val_split_ratio <=0: \n",
    "        val_ratio = 0.1\n",
    "        \n",
    "    val_dataset_count = int(len(cifar10_train_dataset) * val_ratio) \n",
    "    train_dataset_count = len(cifar10_train_dataset) - val_dataset_count\n",
    "    train_dataset, val_dataset = random_split(dataset=cifar10_train_dataset, \n",
    "                                              lengths=[train_dataset_count, val_dataset_count],\n",
    "                                              generator=torch.Generator().manual_seed(1234))\n",
    "    print (f'train dataset: {len(train_dataset)}')\n",
    "    print (f'val dataset: {len(val_dataset)}')\n",
    "\n",
    "    test_dataset = cifar10_test_dataset\n",
    "    # 2. 전체 데이터를 한 번에 불러와서 GPU로 옮김\n",
    "    train_images, train_labels = load_all_to_gpu(train_dataset)\n",
    "    val_images, val_labels = load_all_to_gpu(val_dataset)\n",
    "    test_images, test_labels = load_all_to_gpu(test_dataset)\n",
    "    \n",
    "    # Transform 정의\n",
    "    if data_augmentation:\n",
    "        train_transform_a = v2.Compose([\n",
    "            v2.RandomHorizontalFlip(),\n",
    "            RandomHorizontalShiftWithZeroPad(shift_range=0.1), # 이미지 너비의 최대 10%만큼 이동\n",
    "            RandomVerticalShiftWithZeroPad(shift_range=0.1), # 이미지 높이의 최대 10%만큼 이동\n",
    "        ])\n",
    "    else:\n",
    "        train_transform_a = v2.Compose([\n",
    "            v2.RandomHorizontalFlip()\n",
    "        ])\n",
    "\n",
    "    print(cifar10_train_dataset.classes)\n",
    "    print(val_dataset)\n",
    "    # 3. Transform이 적용된 GPU에 있는 TensorDataset 만들기\n",
    "    train_dataset_a = TransformedTensorDataset(train_images, train_labels, cifar10_train_dataset.classes, transform=train_transform_a)\n",
    "    val_dataset_a = TransformedTensorDataset(val_images, val_labels, cifar10_train_dataset.classes, transform=None)\n",
    "    test_dataset_a = TransformedTensorDataset(test_images, test_labels, cifar10_test_dataset.classes, transform=None)\n",
    "    \n",
    "    # 4. DataLoader 구성 (shuffle 등은 가능)\n",
    "    train_dataloader_a = DataLoader(train_dataset_a, batch_size=batch_size_, shuffle=True) #pin_memory=True works only for cpu\n",
    "    val_dataloader_a = DataLoader(val_dataset_a, batch_size=batch_size_)\n",
    "    test_dataloader_a = DataLoader(test_dataset_a, batch_size=batch_size_)\n",
    "    return (train_dataloader_a, val_dataloader_a, test_dataloader_a, \n",
    "            train_dataset_a, val_dataset_a, test_dataset_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0a21c3-d309-4e5c-af8b-6de2864e3d6d",
   "metadata": {},
   "source": [
    "### 1.2 Standard pytorch pattern\n",
    "- dataloader에서 cpu 통해 dataset 매번 mini-batch 단위로 copy해서 gpu에 올려서 사용\n",
    "- 많은 데이터에도 대응 가능\n",
    "- 느림"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f5cf14-3fdc-4706-ba6b-5aa86b232124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size, max_num_workers\n",
    "def create_dataloader_batch(batch_size_, val_split_ratio=0.1, max_num_workers=2):\n",
    "        \n",
    "    # Simple normalize to [-1, 1]\n",
    "    mean, std = [0.5, 0.5, 0.5], [0.5, 0.5, 0.5]\n",
    "\n",
    "    if data_augmentation:\n",
    "        train_transform_s = transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(), # .cuda()\n",
    "            transforms.RandomAffine(degrees=0, translate=(0.1,0.1)), # horizontal, vertial shift 10%\n",
    "            transforms.ToTensor(), # Turn the image into a torch.Tensor\n",
    "            transforms.Normalize(mean, std),\n",
    "        ])\n",
    "    else:\n",
    "        train_transform_s = transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(), # Turn the image into a torch.Tensor\n",
    "            transforms.Normalize(mean, std),\n",
    "        ])\n",
    "    \n",
    "    test_transform_s = transforms.Compose([\n",
    "        # Turn the image into a torch.Tensor\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std),\n",
    "    ])\n",
    "        \n",
    "    # Load training dataset and preprocess them into torch tensors\n",
    "    # Usage:\n",
    "    # tensor_image, label = train_data[index] : # 0: image in torch.Tensor, 1: label (integer)\n",
    "    cifar10_train_dataset_s = datasets.CIFAR10(\n",
    "        root=cache_dir, # where to download data to?\n",
    "        train=True, # get training data\n",
    "        download=True, # download data if it doesn't exist on disk\n",
    "        transform=train_transform_s, # images come as PIL format, we want to turn into Torch tensors\n",
    "        target_transform=None # you can transform labels as well\n",
    "    )\n",
    "    \n",
    "    # Setup testing data\n",
    "    cifar10_test_dataset_s = datasets.CIFAR10(\n",
    "        root=cache_dir,\n",
    "        train=False, # get test data\n",
    "        download=True,\n",
    "        transform=test_transform_s\n",
    "    )\n",
    "    \n",
    "    # split train to train, val\n",
    "    val_ratio = val_split_ratio\n",
    "    if val_split_ratio <=0: \n",
    "        val_ratio = 0.1\n",
    "        \n",
    "    val_dataset_count = int(len(cifar10_train_dataset_s) * val_ratio)\n",
    "    train_dataset_count = len(cifar10_train_dataset_s) - val_dataset_count\n",
    "    train_dataset_s, val_dataset_s = random_split(dataset=cifar10_train_dataset_s, \n",
    "                                              lengths=[train_dataset_count, val_dataset_count],\n",
    "                                              generator=torch.Generator().manual_seed(1234))\n",
    "    print (f'train dataset: {len(train_dataset_s)}')\n",
    "    print (f'val dataset: {len(val_dataset_s)}')\n",
    "\n",
    "    test_dataset_s = cifar10_test_dataset_s \n",
    "           \n",
    "    # 2. 전체 데이터를 한 번에 불러와서 GPU로 옮김\n",
    "    # load data into dataloader for training\n",
    "    num_workers = os.cpu_count()\n",
    "    if num_workers > max_num_workers:\n",
    "        num_workers = max_num_workers\n",
    "        \n",
    "    train_dataloader_s = DataLoader(dataset=train_dataset_s,\n",
    "                                  batch_size=batch_size_,\n",
    "                                  num_workers=num_workers,\n",
    "                                  shuffle=True, \n",
    "                                  pin_memory=True) #pin_memory=True works only for cpu\n",
    "    \n",
    "    val_dataloader_s = DataLoader(dataset=val_dataset_s,\n",
    "                                 batch_size=batch_size_,\n",
    "                                 num_workers=num_workers,\n",
    "                                 shuffle=False, \n",
    "                                 pin_memory=True)\n",
    "    \n",
    "    test_dataloader_s = DataLoader(dataset=test_dataset_s,\n",
    "                                 batch_size=batch_size_,\n",
    "                                 num_workers=num_workers,\n",
    "                                 shuffle=False, \n",
    "                                 pin_memory=True)\n",
    "    \n",
    "    return (train_dataloader_s, val_dataloader_s, test_dataloader_s, \n",
    "            train_dataset_s, val_dataloader_s, test_dataset_s, \n",
    "            num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c10a894-39e2-45ea-90e0-46869fb6141b",
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_dataloader_a, val_dataloader_a, test_dataloader_a, train_dataset_a, val_dataset_a, test_dataset_a) =  create_dataloader_all_to_gpu(batch_size_=batch_size, val_split_ratio=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b618bb9f-08e1-4b8b-9f9d-6f3e4072108b",
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_dataloader_s, val_dataloader_s, test_dataloader_s, train_dataset_s, val_dataset_s, test_dataset_s, num_workers) = create_dataloader_batch(batch_size_=batch_size, val_split_ratio=0.1, max_num_workers=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99a84d5-01f4-45f9-a932-a4f0d5eee49d",
   "metadata": {},
   "source": [
    "### load된 정보 일부 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca75e06-86e6-4b06-8c52-83f318ca92cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (f'num_train_dataset = {len(train_dataloader_a)}, steps/epoch = {len(train_dataset_a)/batch_size}')\n",
    "print (f'num_test_dataset = {len(test_dataloader_a)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec900fb6-6b4f-4ba9-9468-00ae27d904f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader 사용 예시\n",
    "for x, y in train_dataloader_a:\n",
    "    print(x.shape, x.device, x.min().item(), x.max().item())  # Normalize 확인\n",
    "    print(x.device, y.device)  # GPU\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c579552d-208c-4bd9-95fe-03f5dd054f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensor x가 gpu에 있을 때, value access 차이: x (device 정보 포함), x.item()과 x.cpu() (tensor), x.cpu().item()(float) \n",
    "x.min(), x.min().item(), x.min().cpu().item(), type(x.min().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6b20ac-ad33-4d90-beaa-7e1c4eeed287",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (f'num_train_dataset = {len(train_dataloader_s)}, steps/epoch = {len(train_dataset_s)/batch_size}')\n",
    "print (f'num_test_dataset = {len(test_dataloader_s)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36cc132-9e20-46d5-8b55-7c38c0f11cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_dataset_a.images.shape, train_dataset_a.images.device)\n",
    "print(train_dataset_a.labels.shape,  train_dataset_a.labels.device) \n",
    "print(test_dataset_a.images.shape, test_dataset_a.images.device, test_dataset_a.labels.shape, test_dataset_a.labels.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d14f255-f3c6-4295-9556-8788fea70847",
   "metadata": {},
   "source": [
    "## 1.2 mini-batch 단위로 dataloading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daebf78a-859a-4a45-9b0c-492d84f21fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if gpu_to_all == True: \n",
    "    train_dataset = train_dataset_a\n",
    "    test_dataset = test_dataset_a \n",
    "else:\n",
    "    train_dataset = train_dataset_s\n",
    "    test_dataset = test_dataset_s \n",
    "    print(num_workers, batch_size, len(train_dataset_s), len(train_dataset_s)/batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176aba6a-ad44-46f4-a036-43417872aed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# see classes\n",
    "class_names = test_dataset.classes\n",
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c697bd-e98a-4dca-bacc-5da0936b0597",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b44acf3-c65c-4bd6-a7ca-dd3559a899a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8947d12b-69ad-416e-8815-cafcb0b21d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('train image:', train_dataset_s[0][0].shape, train_dataset_s[0][0].device) # image\n",
    "print('train label:', train_dataset_s[0][1]) # label, scalar(int) on cpu\n",
    "print('test image:', test_dataset_s[0][0].shape, test_dataset_s[0][0].device)\n",
    "print('test label:', test_dataset_s[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1104cb7-2f50-4741-b038-fca413ab4a29",
   "metadata": {},
   "source": [
    "### load한 dataset에서 sampling해서 보기 \n",
    "-  look into some of the image data from the dataset we've downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69f7a2e-fa4e-4115-8651-8f32e4ab66e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show a (normalized ) torch tensor image\n",
    "# tensor_range=0: in [-1,1]\n",
    "# tensor_range=1: in [0,1]\n",
    "def imdisplay_tensor(img, tensor_range=0):\n",
    "    if tensor_range == 0:\n",
    "        img = img/2 + 0.5 # unnormalize\n",
    "        npimg = img.clamp(min=0, max=1).numpy()\n",
    "    else:\n",
    "        npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0))) # (C, H, W) ---> (H, W, C)\n",
    "    plt.show()\n",
    "#or plt.imshow(image_tensor.permute(1,2,0).clamp(min=0, max=1))\n",
    "\n",
    "def imdisplay(im, tensor_range=0):\n",
    "    if isinstance(im, torch.Tensor):\n",
    "        imdisplay_tensor(im, tensor_range)\n",
    "    elif isinstance(im, numpy.ndarray):\n",
    "        plt.imshow(np.transpose(npimg, (1, 2, 0))) # (C, H, W) ---> (H, W, C)\n",
    "        plt.show()\n",
    "    elif isinstance(im, PIL.Image):\n",
    "        plt.imshow(im)\n",
    "        plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192323f1-a9d0-44c0-a217-9335ad926f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display \n",
    "\n",
    "def display_cifar10_sample(dataset, rand_idx):\n",
    "    plt.figure(figsize=(7,7))\n",
    "    \n",
    "    for i, idx in enumerate(rand_idx):\n",
    "    \n",
    "        img, label = dataset[idx] # torch.Tensor, int(label index)\n",
    "\n",
    "        img = img.cpu() # gpu to cpu \n",
    "        img = img/2 + 0.5 # unnormalize from [-1, 1] to [0, 1]\n",
    "    \n",
    "        img_class = class_names[label]\n",
    "    \n",
    "        plt.subplot(4,4,i+1)\n",
    "        plt.imshow(img.permute(1,2,0).clamp(min=0, max=1))\n",
    "        plt.title(f\"Class : {img_class}\",fontsize=10)\n",
    "        plt.axis(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9a0cc3-7de5-4357-b6c9-6ab23f04d3bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rand_idx = random.sample(range(len(test_dataset)),k=16)\n",
    "display_cifar10_sample(test_dataset, rand_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aad55e0-6438-46e2-9078-635c9006d954",
   "metadata": {},
   "outputs": [],
   "source": [
    "## display the augmented images in the train dataset \n",
    "\n",
    "def display_cifar10_transformed_sample(x_dataloader):\n",
    "    plt.figure(figsize=(7,7))\n",
    "    n = 16 \n",
    "    i = 0\n",
    "    n_imgs = n // 4\n",
    "    for batch_idx, (inputs, targets) in enumerate(x_dataloader):\n",
    "        imgs = inputs \n",
    "        for k, img in enumerate(inputs):\n",
    "            img = img.cpu() # gpu to cpu \n",
    "            img = img/2 + 0.5 # unnormalize from [-1, 1] to [0, 1]\n",
    "            label = targets[k].cpu() \n",
    "            img_class = class_names[label]\n",
    "    \n",
    "            plt.subplot(n_imgs, n_imgs,i+1)\n",
    "            plt.imshow(img.permute(1,2,0).clamp(min=0, max=1))\n",
    "            plt.title(f\"Class : {img_class}\",fontsize=10)\n",
    "            plt.axis(False)\n",
    "            i+= 1\n",
    "            if i >= n:\n",
    "                break\n",
    "        if i>= n:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20858a68-edcd-4f68-b893-a82de08890d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_cifar10_transformed_sample(train_dataloader_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b4abe3-d0ab-46bb-a704-8ad1cf0a8c05",
   "metadata": {},
   "source": [
    "## 2.Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f262b4-3267-4ae3-84b3-6e36b33775ea",
   "metadata": {},
   "source": [
    "### ResNet v2 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75b3aa6-a9b0-42e9-aad2-5340f01f8583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResNet Layer\n",
    "class ResNetLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, activation=True, batch_norm=True, conv_first=True):\n",
    "        super(ResNetLayer, self).__init__()\n",
    "        layers = []\n",
    "\n",
    "        padding = 0 if kernel_size == 1 else 1 # kernel_size = 3 : padding = 1\n",
    "        if conv_first:\n",
    "            layers.append(nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding=padding, bias=False)) # bias=False\n",
    "            if batch_norm:\n",
    "                layers.append(nn.BatchNorm2d(out_channels))\n",
    "            if activation:\n",
    "                layers.append(nn.ReLU(inplace=True)) ### inplace=False\n",
    "        else:\n",
    "            if batch_norm:\n",
    "                layers.append(nn.BatchNorm2d(in_channels))\n",
    "            if activation:\n",
    "                layers.append(nn.ReLU(inplace=True)) ### inplace=False\n",
    "            layers.append(nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding=padding, bias=False))\n",
    "\n",
    "        self.block = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "# Bottleneck Residual Unit\n",
    "class BottleneckResidualUnit(nn.Module):\n",
    "    def __init__(self, in_channels, mid_channels, out_channels, stride=1, activation=True, batch_norm=True):\n",
    "        super(BottleneckResidualUnit, self).__init__()\n",
    "\n",
    "        # shortcut connection \n",
    "        self.shortcut = None\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False)\n",
    "\n",
    "        # Bottleneck layers\n",
    "        self.conv1 = ResNetLayer(in_channels, mid_channels, kernel_size=1, stride=stride,\n",
    "                                 activation=activation, batch_norm=batch_norm, conv_first=False)\n",
    "        self.conv2 = ResNetLayer(mid_channels, mid_channels, kernel_size=3, stride=1,\n",
    "                                 activation=True, batch_norm=True, conv_first=False)\n",
    "        self.conv3 = ResNetLayer(mid_channels, out_channels, kernel_size=1, stride=1,\n",
    "                                 activation=True, batch_norm=True, conv_first=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.conv1(x)\n",
    "        y = self.conv2(y)\n",
    "        y = self.conv3(y)\n",
    "        if self.shortcut is not None:\n",
    "            y += self.shortcut(x)\n",
    "        else:\n",
    "            y += x\n",
    "        return y\n",
    "\n",
    "# ResNet v2 model \n",
    "class ResNetV2(nn.Module):\n",
    "    def __init__(self, depth, num_classes=10, loss_fn=nn.CrossEntropyLoss(), zero_init_residual=True, debug_mode=False):\n",
    "        super(ResNetV2, self).__init__()\n",
    "        self.debug = debug_mode\n",
    "        \n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        \n",
    "        self.num_filters_in = 16\n",
    "        num_res_blocks = (depth - 2) // 9\n",
    "\n",
    "        # first convolution\n",
    "        self.conv1 = ResNetLayer(3, self.num_filters_in, conv_first=True)\n",
    "\n",
    "        # stage 0\n",
    "        self.stage0, num_filters_in = self._make_stage(0, num_res_blocks, self.num_filters_in)\n",
    "\n",
    "        # stage 1\n",
    "        self.stage1, num_filters_in = self._make_stage(1, num_res_blocks, num_filters_in)\n",
    "    \n",
    "        # stage 2\n",
    "        self.stage2, num_filters_in = self._make_stage(2, num_res_blocks, num_filters_in)\n",
    "\n",
    "        # final layers\n",
    "        self.bn = nn.BatchNorm2d(num_filters_in)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.gap = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(num_filters_in, num_classes)\n",
    "\n",
    "        # parameter initialization \n",
    "        self.zero_init_residual = zero_init_residual\n",
    "        self.init_weights()\n",
    "\n",
    "        # check model \n",
    "        if self.debug: \n",
    "            self.test_print()\n",
    "            \n",
    "    def _make_stage(self, stage, num_res_blocks, num_filters_in=0):\n",
    "        layers = []\n",
    "        stride = 1\n",
    "        if num_filters_in == 0:\n",
    "            num_filters_in = self.num_filters_in\n",
    "\n",
    "        for res_block in range(num_res_blocks):\n",
    "            # 첫번째 블록에서만 다운샘플링 stride=2\n",
    "            if stage > 0 and res_block == 0:\n",
    "                stride = 2 # downsampling\n",
    "            else:\n",
    "                stride = 1\n",
    "\n",
    "            if stage == 0:\n",
    "                num_filters_out = num_filters_in * 4\n",
    "                # 첫번째 stage, 첫번째 블록은 활성화, 배치정규화 안 함\n",
    "                if res_block == 0:\n",
    "                    activation = False\n",
    "                    batch_norm = False\n",
    "                else:\n",
    "                    activation = True\n",
    "                    batch_norm = True\n",
    "            else:\n",
    "                num_filters_out = num_filters_in * 2\n",
    "                activation = True\n",
    "                batch_norm = True\n",
    "\n",
    "            if res_block == 0:\n",
    "                num_filters_mid = num_filters_in\n",
    "                layers.append(BottleneckResidualUnit(\n",
    "                    num_filters_in, num_filters_mid, num_filters_out,\n",
    "                    stride=stride,\n",
    "                    activation=activation, batch_norm=batch_norm\n",
    "                ))\n",
    "            else: \n",
    "                # mid_channels 계산: stage0에서는 out_channels//4, 나머지 stage에서는 out_channels//2\n",
    "                if stage == 0:\n",
    "                    num_filters_mid = num_filters_out // 4\n",
    "                else:\n",
    "                    num_filters_mid = num_filters_out // 2\n",
    "                layers.append(BottleneckResidualUnit(\n",
    "                    num_filters_out, num_filters_mid, num_filters_out,\n",
    "                    stride=1, \n",
    "                    activation=activation, batch_norm=batch_norm\n",
    "                ))\n",
    "\n",
    "        return nn.Sequential(*layers), num_filters_out\n",
    "\n",
    "    def init_weights(self):\n",
    "        # parameter initialization \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, nonlinearity='relu') \n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            else:\n",
    "                if isinstance(m, nn.Sequential) != True and \\\n",
    "                   isinstance(m, BottleneckResidualUnit) != True and \\\n",
    "                   isinstance(m, ResNetLayer) != True and \\\n",
    "                   isinstance(m, nn.ReLU) != True and \\\n",
    "                   isinstance(m, nn.AdaptiveAvgPool2d):\n",
    "                    if self.debug:\n",
    "                        print (\"uninitialized module: \", m)\n",
    "                \n",
    "        # Zero-initialize the last BN in each residual branch\n",
    "        # so that the residual branch starts with zeros,\n",
    "        # and each residual block hehavies like identity\n",
    "\n",
    "        if self.zero_init_residual:\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, BottleneckResidualUnit):\n",
    "                    # conv3.block[2]은 Conv2d layer을 0 으로 초기화 \n",
    "                    if isinstance(m.conv3, ResNetLayer) and len(m.conv3.block) > 1:\n",
    "                        if self.debug:\n",
    "                            print((len(m.conv3.block)))\n",
    "                        nn.init.constant_(m.conv3.block[2].weight, 0) \n",
    "                        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.stage0(x)\n",
    "        x = self.stage1(x)\n",
    "        x = self.stage2(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.gap(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "    def test_print(self): # , num_res_blocks, num_filters_in):\n",
    "        x = torch.randn(1, 3, 32, 32).to(device)\n",
    "        print('input:', x.shape)\n",
    "        x = self.conv1(x)\n",
    "        print('after conv1:', x.shape)\n",
    "        x = self.stage0(x)\n",
    "        print('after stage0:', x.shape)\n",
    "        x = self.stage1(x)\n",
    "        print('after stage1:', x.shape)\n",
    "        x = self.stage2(x)\n",
    "        print('after stage2:', x.shape)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.gap(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06adad95-f721-4f8c-bdc5-5b55fdfe4db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize a model\n",
    "model = ResNetV2(depth=depth, num_classes=num_classes,zero_init_residual=zero_init_residual).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5bea2d-3cdb-4024-851c-abcb0604a04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize a model\n",
    "model = ResNetV2(depth=depth, num_classes=num_classes).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b2c3f3-f352-4573-9c26-ac91df2fac14",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "summary(model, input_size=[1, 3, 32, 32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd4a5a7-edd6-4241-8002-9532be8eea79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.test_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3ba474-e5ce-491d-a3c8-fa0a15c7428a",
   "metadata": {},
   "source": [
    "## 3.Training\n",
    "### Let's train\n",
    "- optimizer, loss function(loss_fn), scheduler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889ffb3b-9efb-44c7-9162-df93abb26d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "## lr scheduler\n",
    "def lr_schedule(epoch):\n",
    "    lr = 1e-3  # initial learning rate\n",
    "    if epoch > 180:\n",
    "        lr *= 0.5e-3\n",
    "    elif epoch > 160:\n",
    "        lr *= 1e-3\n",
    "    elif epoch > 130:\n",
    "        lr *= 1e-2\n",
    "    elif epoch > 80:\n",
    "        lr *= 1e-1\n",
    "    \n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489eab6c-612a-474b-8daa-5f646c8269b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run training, with or without data adata_augmentation\n",
    "\n",
    "# loss and optimizer\n",
    "# loss_fn = nn.CrossEntropyLoss()\n",
    "print (\"Loss function:\", model.loss_fn, \"== nn.CrossEntropyLoss()\")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr_schedule(0))\n",
    "\n",
    "# learning rate scheduler\n",
    "scheduler = LambdaLR(optimizer, lr_lambda=lambda epoch: lr_schedule(epoch) / lr_schedule(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac0b362-69d3-4ac4-a2fd-08743b3b76dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training function\n",
    "def train_epoch(\n",
    "                model,\n",
    "                train_dataloader,\n",
    "                optimizer,\n",
    "                scheduler,\n",
    "                epoch=-1, \n",
    "                verbose=0):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_dataloader):\n",
    "        inputs, targets = inputs.to(device, non_blocking=True), targets.to(device, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = model.loss_fn(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        if verbose and batch_idx % 100 == 0:\n",
    "            if epoch >=0:\n",
    "                print(f'Epoch: {epoch+1} | Batch: {batch_idx+1}/{len(train_dataloader)} | '\n",
    "                      f'Loss: {loss.item():.4f} | Accuracy: {correct/total:.2f}')\n",
    "            else:\n",
    "                print(f'Batch: {batch_idx+1}/{len(train_dataloader)} | '\n",
    "                      f'Loss: {loss.item():.4f} | Accuracy: {correct/total:.2f}')\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    train_loss = running_loss / len(train_dataloader)\n",
    "    train_acc = correct /total\n",
    "\n",
    "    return train_loss, train_acc\n",
    "\n",
    "# evaluate dataset to caluclate loss, accuracy\n",
    "# return loss, accuracy\n",
    "def evaluate_val(model, val_dataloader):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(val_dataloader):\n",
    "            inputs, targets = inputs.to(device, non_blocking=True), targets.to(device, non_blocking=True)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = model.loss_fn(outputs, targets)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "            \n",
    "        val_loss = running_loss / len(val_dataloader)\n",
    "        val_acc = correct /total\n",
    "\n",
    "    return val_loss, val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dec91eb-46e7-479b-b798-b10df5b56f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calcuate accuracy\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, t in loader:\n",
    "            x = x.to(device=device, non_blocking=True)\n",
    "            t = t.to(device=device, non_blocking=True)\n",
    "            \n",
    "            y = model(x)\n",
    "            _, predictions = y.max(1)\n",
    "            num_correct += (predictions == t).sum()\n",
    "            num_samples += predictions.size(0)\n",
    "        \n",
    "        print(f'Got {num_correct} / {num_samples} with accuracy {float(num_correct)/float(num_samples)*100:.2f}') \n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    return float(num_correct)/float(num_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796dc2ec-aa7a-4f58-a892-48e10327dbcb",
   "metadata": {},
   "source": [
    "### Training Loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be5fe64-f3bf-43fd-bae1-9cfc135cc1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model: torch.nn.Module,\n",
    "          train_dataloader: torch.utils.data.DataLoader,\n",
    "          val_dataloader: torch.utils.data.DataLoader,\n",
    "          # test_dataloader: torch.utils.data.DataLoader,\n",
    "          optimizer: torch.optim.Optimizer,\n",
    "          scheduler:torch.optim.lr_scheduler,\n",
    "          #grad_clip:float=None,\n",
    "          epochs: int = 10):\n",
    "\n",
    "    history = {\"train_loss\": [],\n",
    "                \"train_accuracy\": [],\n",
    "                \"val_loss\": [],\n",
    "                \"val_accuracy\": [],\n",
    "                \"lr\": []\n",
    "              }\n",
    "\n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        start_time1 = datetime.now()\n",
    "        train_loss, train_acc = train_epoch(model, train_dataloader, optimizer=optimizer, scheduler=scheduler, epoch=epoch)\n",
    "    \n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_accuracy'].append(train_acc)\n",
    "        history['lr'].append(scheduler.get_last_lr()[0])\n",
    "        if test_dataloader is not None:\n",
    "            val_loss, val_acc = evaluate_val(model, val_dataloader)\n",
    "            history['val_loss'].append(val_loss)\n",
    "            history['val_accuracy'].append(val_acc)\n",
    "        end_time1 = datetime.now()\n",
    "        \n",
    "        if test_dataloader is None:\n",
    "            print(f'Epoch: {epoch+1}/{epochs} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f} |'\n",
    "                  f'lr: {scheduler.get_last_lr()[0]:.4e}, {end_time1-start_time1} sec/epoch')\n",
    "        else:\n",
    "            print(f'Epoch: {epoch+1}/{epochs} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f} |'\n",
    "                  f'Val. Loss: {val_loss:.4f} | Val. Acc: {val_acc:.2f} | '\n",
    "                  f'lr: {scheduler.get_last_lr()[0]:.4e}, {end_time1-start_time1} sec/epoch')\n",
    "    end_time = datetime.now()\n",
    "    print(f'Training completed in: {end_time - start_time}')\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f2c360-20fd-4686-8b93-5d54e6d5534d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpu_to_all = False\n",
    "if gpu_to_all == True:\n",
    "    train_dataloader =  train_dataloader_a\n",
    "    val_dataloader = val_dataloader_a\n",
    "    test_dataloader = test_dataloader_a\n",
    "    train_dataset = train_dataset_a\n",
    "    val_dataset = val_dataset_a\n",
    "    test_dataset = test_dataset_a\n",
    "else:\n",
    "    train_dataloader =  train_dataloader_s\n",
    "    val_dataloader = val_dataloader_s\n",
    "    test_dataloader = test_dataloader_s\n",
    "    train_dataset = train_dataset_s\n",
    "    val_dataset = val_dataset_s\n",
    "    test_dataset = test_dataset_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2daa75d2-ad74-4e7f-a9ad-f1c6c9e22789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.init_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704ca104-2cde-47ea-a3ca-7feb5f6591c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Try initial run\n",
    "# print information \n",
    "print (f'batch_size={batch_size}')\n",
    "print (f'num_train_dataset = {len(train_dataloader)}, steps/epoch = {len(train_dataset)/batch_size}')\n",
    "print (f'num_test_dataset = {len(test_dataloader)}, steps/epoch = {len(test_dataset)/batch_size}')\n",
    "print ('all data on gpu' if gpu_to_all == True else 'moving by mini-batch from host to gpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ef0f9c-bec5-4f4c-9581-58260d3eb442",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test run\n",
    "# model_history1 = train(model, train_dataloader,  val_dataloader, optimizer, scheduler,  epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea053b4-c904-4cf7-8457-848d324af0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to compare the processing time with and without test_dataloader\n",
    "# 시간이 많이 걸리면 test_dataloader를 None 또는 적은 횟수로 평가한다.\n",
    "# model_history2 = train(model, train_dataloader,  test_dataloader=None, optimizer=optimizer, scheduler=scheduler,  epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700e813b-a237-4691-8b8e-f2119d941a1a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train\n",
    "model_history = train(model, train_dataloader,  val_dataloader, optimizer=optimizer, scheduler=scheduler, \n",
    "                      epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298929af-4663-4e03-88e5-c3b486f76361",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f299ad-11f0-42e3-9267-f56ecefa1428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# type\n",
    "from typing import List , Dict , Tuple\n",
    "\n",
    "# function to plot loss & accuracy curve\n",
    "def plot_history(history: Dict[str, List[float]]):\n",
    "    # Get the loss values of the results dictionary (training and val)\n",
    "    loss = history['train_loss']\n",
    "\n",
    "    # Get the accuracy values of the results dictionary (training and val)\n",
    "    accuracy = history['train_accuracy']\n",
    "\n",
    "    # Figure out how many epochs there were\n",
    "    epochs = range(len(history['train_loss']))\n",
    "\n",
    "    if 'val_loss' in history.keys() and len(epochs) == len(history['val_loss']):\n",
    "        val_loss = history['val_loss']\n",
    "        val_accuracy = history['val_accuracy']\n",
    "        \n",
    "    # Setup a plot\n",
    "    plt.figure(figsize=(15, 7))\n",
    "\n",
    "    # Plot loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, loss, label='train_loss')\n",
    "    if 'val_loss' in history.keys() and len(epochs) == len(history['val_loss']):\n",
    "        plt.plot(epochs, val_loss, label='val_loss')\n",
    "    plt.title('Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, accuracy, label='train_accuracy')\n",
    "    if 'val_loss' in history.keys() and len(epochs) == len(history['val_loss']):\n",
    "        plt.plot(epochs, val_accuracy, label='val_accuracy')\n",
    "    plt.title('Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.grid()\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74867c18-3fe3-448f-8f7d-4e68eec5366a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(model_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3d72d6-71ba-4d75-8a65-6c973e2618a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = range(len(model_history['lr']))\n",
    "plt.plot(epochs, model_history['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6c803d-79f1-4b44-a7b1-09cef5e4c550",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test accuracy\n",
    "test_acc = evaluate(model, test_dataloader)\n",
    "print(f'Final evaluation accuracy for test dataset: {test_acc:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff971ce8-79ea-4328-9fe3-5c6c6d7989dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d007fe-6db0-46d2-8fed-21641744e9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116d7d3d-b216-40db-b4fd-b0e0766ba1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "# 1. Save and load uing model.state_dict()\n",
    "model_filename = f\"cifar10_{model_type}_state_dict.pth\"\n",
    "torch.save(model.state_dict(), model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd296663-b590-469f-9825-e057a99d245c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model for evaluation\n",
    "reloaded_model = ResNetV2(depth, num_classes=num_classes).to(device)\n",
    "reloaded_model.load_state_dict(torch.load(model_filename, weights_only=True, map_location=device))\n",
    "reloaded_model.eval()\n",
    "\n",
    "# test accuracy\n",
    "test_acc = evaluate(reloaded_model, test_dataloader)\n",
    "print(f'Final evaluation accuracy for test dataset: {test_acc:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3131b689-c244-4ed5-9d19-50a21014bfd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.Save and load a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f7546f-b1fb-4567-b3ad-77a8478ce7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_model_filename = f\"cifar10_{model_type}.pth\"\n",
    "torch.save(model, full_model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d37726-144a-493f-a33d-756fb28cb82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_model = torch.load(full_model_filename, weights_only=False, map_location=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fb8dda-3222-4c29-b949-8f4d7a0d75a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(full_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9cf40cd-35bc-4308-bfdf-9305a2a73a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(full_model, input_size=(1, 3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff2a2e4-749d-41f7-8000-aeea7fa58a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#outputs = evaluate(reloaded_model, train_dataloader)\n",
    "outputs = evaluate(model, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad302d72-67e4-462e-a9ad-3fbecafa03a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = evaluate_val(reloaded_model, test_dataloader)\n",
    "test_loss, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed6728d-0c5f-4b08-9fe0-c257604138fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy = evaluate(model, test_dataloader)\n",
    "test_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e956bd35-c68f-480b-894a-ebb6486263bc",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e13a2c8-cccc-48e9-a557-646b7d3d0025",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in test_dataloader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(targets.cpu().numpy())\n",
    "\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sn.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Ground Truth')\n",
    "plt.title('Confunsion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a925a6-3aa0-46e4-a5b4-fa9da81502c9",
   "metadata": {},
   "source": [
    "### Inference Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba2c8d4-9629-4a7a-ab1f-4200b376efd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms.v2.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f743dbc-64ac-492d-9560-5f26653651c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save an image from cifar10 test dataset and load it\n",
    "import PIL\n",
    "\n",
    "image_id = 0\n",
    "image_file = f'./images/cifar10_test{image_id}.png'\n",
    "image_tensor = test_dataset[image_id][0] # 0: tensor image, 1: label\n",
    "#imdisplay(image_tensor.cpu(), 0)\n",
    "label = test_dataset[image_id][1] # 0: tensor image, 1: label\n",
    "image_tensor = (image_tensor / 2 + 0.5).clamp(min=0, max=1).cpu()\n",
    "#imdisplay(image_tensor, 1)\n",
    "plt.imshow(image_tensor.permute(1,2,0))\n",
    "#image_tensor = image_tensor.permute(1,2,0)\n",
    "image_pil = transforms.ToPILImage()(image_tensor)\n",
    "image_pil.save(image_file)\n",
    "#image_pil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194c8d7a-bb70-4706-b6d0-65b0927a8fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the image\n",
    "img = PIL.Image.open(image_file) # img: PIL image [0, 255], HxW, C\n",
    "\n",
    "img_tensor = transforms.ToTensor()(img) # torch tensor image [0, 1], (C, H, W)\n",
    "imdisplay(img_tensor, 1)\n",
    "\n",
    "img_tensor = transforms.Normalize(0.5, 0.5)(img_tensor)\n",
    "inputs = img_tensor.unsqueeze(0).to(device)\n",
    "\n",
    "outputs = model(inputs)\n",
    "_, preds = torch.max(outputs, 1)\n",
    "print (f'gnd = {class_names[label]}, prediction = {class_names[preds[0]]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88e8acd-25af-4779-8367-55565efcf31d",
   "metadata": {},
   "source": [
    "### Qualitative Evaluation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f4b9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(x_dataloader, model):\n",
    "    num_rows = 4\n",
    "    num_cols = 6\n",
    "    \n",
    "    # Retrieve a number of images from the dataset.\n",
    "    with torch.no_grad():\n",
    "        (inputs, targets) = next(iter(x_dataloader))\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "        # Get predictions from model.  \n",
    "        outputs = model(inputs)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total = targets.size(0)\n",
    "        correct = predicted.eq(targets).sum().item()\n",
    "    print(f'batch accuracy = {correct/total * 100:.1f}(N={total})')\n",
    "    \n",
    "    #n = len(targets) \n",
    "    num_cols = 6\n",
    "    num_rows = min(total//num_cols, 4)\n",
    "    \n",
    "    data_batch = inputs.cpu()\n",
    "\n",
    "    plt.figure(figsize=(20, 8))\n",
    "    num_matches = 0\n",
    "        \n",
    "    for idx in range(num_rows*num_cols):\n",
    "        ax = plt.subplot(num_rows, num_cols, idx + 1)\n",
    "        plt.axis(\"off\")\n",
    "        img = transforms.Resize((32,32), interpolation=transforms.InterpolationMode.NEAREST)(data_batch[idx])\n",
    "        img = img/2 + 0.5 # unnormalize\n",
    "        npimg = img.numpy()\n",
    "        plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "\n",
    "        pred_idx = predicted[idx]\n",
    "        truth_idx = targets[idx]\n",
    "            \n",
    "        title = str(class_names[truth_idx]) + \" : \" + str(class_names[pred_idx])\n",
    "        title_obj = plt.title(title, fontdict={'fontsize':13})\n",
    "            \n",
    "        if pred_idx == truth_idx:\n",
    "            num_matches += 1\n",
    "            plt.setp(title_obj, color='g')\n",
    "        else:\n",
    "            plt.setp(title_obj, color='r')\n",
    "                \n",
    "        acc = num_matches/(idx+1)\n",
    "        \n",
    "    print(\"Prediction accuracy (for data in display): \", int(100*acc)/100)\n",
    "    \n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164faf69-591a-4871-9418-36116ba2c199",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(test_dataloader, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db2cdc9-4687-4a23-98c0-460ada0e392e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
