{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e13eb84a-d16d-4f42-9cea-e0b34e639e39",
   "metadata": {},
   "source": [
    "## Vision Transformers (ViT)\n",
    "- Fine-tuning\n",
    "- AI Tools: Huggingface, pytorch\n",
    "- Dataset: cifar10\n",
    "- Reference \n",
    "  * [ViT-hugging Face](https://huggingface.co/docs/transformers/model_doc/vit)\n",
    "  * [ViT-hf-ft-cifar10-pytorch ](https://github.com/supersjgk/Transformers/blob/main/VisionTransformers/Vision_Transformers_Hugging_Face_Fine_Tuning_Cifar10_PyTorch.ipynb) \n",
    "  * [vit-pytorch](https://github.com/lucidrains/vit-pytorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd8acc8-076d-4809-acdb-a4cfa271e929",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.transforms import Normalize, Resize, ToTensor, Compose\n",
    "from torchvision.transforms import ToPILImage\n",
    "\n",
    "from torchinfo import summary\n",
    "\n",
    "from transformers import pipeline\n",
    "from transformers import ViTImageProcessor, ViTForImageClassification\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from datasets import load_dataset\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import requests\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53861c1-43c8-4192-886d-8adf49a8686d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd4431d-d411-4957-b28e-b4ea5fe4f7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "cache_path = 'd:/HF_cache'\n",
    "os.environ['HF_HOME'] = cache_path\n",
    "os.environ['TRANSFORMERS_CACHE'] = cache_path # seems not to work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854ab01a-febe-4997-bdc2-b7fe4f66530d",
   "metadata": {},
   "source": [
    "## 1. ImageNet 1k - Image Classification \n",
    "## [Hugging Face] Inference Methods for ViT model\n",
    " 크게 4가지가 있음\n",
    "1. pipeline.pretrained()\n",
    "2. AutoModel():\n",
    "   - image_processor = AutoImageProcessor.from_pretrained(model_name, use_fast=True)\n",
    "   - model = AutoModelForImageClassification.from_pretrained(model_name, device_map=\"auto\", attn_implementation=\"sdpa\")\n",
    "3. ViTForImageClasssification\n",
    "4. ViTModel, VitImageProcessor(ViTImageProcessorFast)\n",
    "### Downloading pre-trained weights\n",
    "- Model: ViT-B/16\n",
    "  * input image size = 224\n",
    "  * patch size = 16\n",
    "  * Transformer-encoder configuartions\n",
    "  * Classification Head\n",
    "  * dropout = 0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7606533-0faa-4db8-82d6-69242d7b6cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"google/vit-base-patch16-224\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d4ea12-39cd-48bf-8204-b20667deb3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a sample image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60306d65-c065-466d-84b7-b82c7e447231",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_url(url):\n",
    "    return Image.open(requests.get(url, stream=True).raw) # PIL format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92742ad7-d424-47e7-8977-df7f4907cb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load an image\n",
    "url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n",
    "image = load_image_url(url)\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c05c73f-4cc1-460c-8a91-26001fe22d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "image.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaed0384-0775-40dc-b829-0e28d5757933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. inference using pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b96516-1861-4f95-bf49-f9ceec64e17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vit_pipeline = pipeline(\n",
    "    task=\"image-classification\", # task 종류 이름 \n",
    "    model=model_name,\n",
    "    torch_dtype=torch.float, \n",
    "    device = device,\n",
    "    use_fast = True\n",
    ")\n",
    "\n",
    "start_time = datetime.now()\n",
    "result = vit_pipeline(inputs='http://images.cocodataset.org/val2017/000000039769.jpg')\n",
    "end_time = datetime.now()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f'processing time: {str(elapsed_time)} sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8a9a44-76c2-4521-86ae-024d6116ba85",
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c598a8-5339-42f1-a4e8-4e4861727a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. inference using ViT HugggingFace models\n",
    "# Model Card: https://huggingace.co/google/vit-base-patch16-224\n",
    "# use torchvision to process fast\n",
    "# - cf. ViTImageProcessorFast\n",
    "processor = ViTImageProcessor.from_pretrained(model_name, device=device, use_fast=True)\n",
    "\n",
    "mu, sigma = processor.image_mean, processor.image_std\n",
    "size = processor.size\n",
    "print(size, mu, sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a62f037-dd00-4965-9522-4dd4b8af8d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ViTForImageClassification.from_pretrained(model_name, device_map=device)\n",
    "# print(model.classifier) #The google/vit-base-patch16-224 model is originally fine tuned on imagenet-1K with 1000 output classes\n",
    "\n",
    "# print(model.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb87a93-9504-4324-8d53-112a9baab71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdee4a87-2fc0-49f8-9c62-7b14c477cf60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.device\n",
    "#inputs['pixel_values'].device\n",
    "#inputs.keys()\n",
    "#inputs['pixel_values'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c844cf-b5fe-4c05-a1a5-bdd27490f861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using logits\n",
    "start_time = datetime.now()\n",
    "inputs = processor(images=image, return_tensors='pt').to(device)\n",
    "# the model outputs logits\n",
    "outputs = model(**inputs)\n",
    "end_time = datetime.now()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f'processing time: {str(elapsed_time)} sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4375c78-c1c6-44eb-b99f-9b0c6c8834c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logits을 이용한 top1 class \n",
    "logits = outputs.logits\n",
    "\n",
    "# model predicts one of the 1000 ImageNet classes\n",
    "predicted_class_idx = logits.argmax(-1).item()\n",
    "print(\"Predicted class:\", model.config.id2label[predicted_class_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e67e24-972c-41de-ae49-0e6770b8d9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# top-k using softmax or logits\n",
    "prob_output = torch.softmax(outputs.logits[0], dim=0)\n",
    "scores, indices = torch.topk(prob_output, 3)\n",
    "print(\"Predicted class:\", [(model.config.id2label[i.item()], s.item()) for (s, i) in zip(scores, indices)] )\n",
    "prob_output.argmax().item(), torch.topk(prob_output, 3), torch.topk(logits, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc1f3fa-0443-4b75-aaff-f87053834cee",
   "metadata": {},
   "source": [
    "### Loading local image files to classify them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef7967d-45b7-4037-b27f-1a550d8c7561",
   "metadata": {},
   "outputs": [],
   "source": [
    "#image_path = \"../AI-Application-Specialist-Vision/images/\"\n",
    "#image_path = \"./images/\"\n",
    "image_path = \".\\\\images\\\\\"\n",
    "url1 = image_path + 'Granny_smith_and_cross_section.jpg'\n",
    "url2 = image_path + 'Free!_(3987584939).jpg'\n",
    "url = image_path + 'ILSVRC2012_val_00000466.jpg'\n",
    "\n",
    "image1 = Image.open(url1).convert(\"RGB\")\n",
    "image2 = Image.open(url2).convert(\"RGB\")\n",
    "image = Image.open(url).convert(\"RGB\")\n",
    "image1, image2, image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35707ad-ee53-4b62-99b6-c2fe21ef0110",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio = 0.2 \n",
    "image1.resize((int(image1.size[0] * ratio), int(image1.size[1] * ratio)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce2899f-c322-4c7d-b5f2-44de873ba287",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ratio = 0.2 \n",
    "image2.resize((int(image2.size[0] * ratio), int(image2.size[1] * ratio)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdf8001-1473-44f4-9914-92035e78d353",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!dir {$image_path} /B\n",
    "#image_path = \".\\\\images\\\\\"\n",
    "!dir {image_path} /B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b0d820-7277-44de-96c9-8e4ef35a3ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = processor(images=image1, return_tensors='pt').to(device)\n",
    "# the model outputs logits\n",
    "outputs = model(**inputs)\n",
    "display(image1.resize((200, 200))) # you may specify interpolation methods(ex, Image.LANCZOS) for resizing\n",
    "# model predicts one of the 1000 ImageNet classes\n",
    "print(\"Predicted class:\", model.config.id2label[outputs.logits.argmax(-1).item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14e9a2e-5078-4f44-a12d-7c90bcc2dceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = processor(images=image2, return_tensors='pt').to(device)\n",
    "# the model outputs logits\n",
    "outputs = model(**inputs)\n",
    "display(image2.resize((200,200)))\n",
    "# model predicts one of the 1000 ImageNet classes\n",
    "print(\"Predicted class:\", model.config.id2label[outputs.logits.argmax(-1).item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48396e4-e1eb-4ee2-b8cc-222cfe574a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = processor(images=image, return_tensors='pt').to(device)\n",
    "# the model outputs logits\n",
    "outputs = model(**inputs)\n",
    "display(image.resize((200,200)))\n",
    "# model predicts one of the 1000 ImageNet classes\n",
    "print(\"Predicted class:\", model.config.id2label[outputs.logits.argmax(-1).item()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823bd4c7-f352-49b6-8ab2-4e9cf90adaf4",
   "metadata": {},
   "source": [
    "## 2. Fine-tuning ViT using cifar10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af685a2-f69a-4066-a001-f08425654045",
   "metadata": {},
   "source": [
    "## Dataset Preparation\n",
    "### Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a64e81b-06d7-4193-84b9-090be51b9448",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = load_dataset(\"cifar10\", split=[\"train\",\"test\"])\n",
    "# train dataset을 train dataset과 validation dataset으로 나눔\n",
    "splits = train_dataset.train_test_split(test_size=0.1)\n",
    "train_dataset = splits['train']\n",
    "val_dataset = splits['test']\n",
    "train_dataset, val_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bd8a80-d11a-4c89-97a1-e574d914f8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.features, train_dataset.num_rows, train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2d08fa-f94b-4498-8e54-2cbec3cd7722",
   "metadata": {},
   "outputs": [],
   "source": [
    "itos = dict((k,v) for k,v in enumerate(train_dataset.features['label'].names))\n",
    "stoi = dict((v,k) for k,v in enumerate(train_dataset.features['label'].names))\n",
    "itos, stoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47ef27a-f336-4d3a-ab26-0a5b5cccc291",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# sample access \n",
    "img, label = train_dataset[0]['img'], itos[train_dataset[0]['label']]\n",
    "print(label)\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb8750d-a8b6-45e1-b059-8b80f8c80ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(img.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988fea1d-1c64-492d-82d6-eee217e19418",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abe4719-20f0-47c4-9cbc-bc2747d62fcf",
   "metadata": {},
   "source": [
    "### Preprocessing Data or Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfdfa67-8c20-4df4-961c-1e661f72240c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"google/vit-base-patch16-224\"\n",
    "# use torchvision to process fast\n",
    "processor = ViTImageProcessor.from_pretrained(model_name, device=device, use_fast=True)\n",
    "\n",
    "#mu, sigma = processor.image_mean, processor.image_std\n",
    "#size = processor.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5667ec5-2e2f-478a-ba69-12e103534491",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb7143f-fe93-4ee2-bd48-d390cbe10888",
   "metadata": {},
   "source": [
    "### You may add or modify data augmentation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2d23b1-9d44-4868-9203-9cbd0310502a",
   "metadata": {},
   "outputs": [],
   "source": [
    "_transf = Compose([\n",
    "    Resize(size['height']).cuda(),\n",
    "    ToTensor(),\n",
    "    Normalize(mean=mu, std=sigma)\n",
    "])\n",
    "\n",
    "def transf(arg):\n",
    "    arg['pixels'] = [_transf(image.convert('RGB')) for image in arg['img']]\n",
    "    return arg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5de8e93-f1ce-4dc0-bd8c-da5ac8ee0412",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.set_transform(transf)\n",
    "val_dataset.set_transform(transf)\n",
    "test_dataset.set_transform(transf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da69a6bc-08d5-40d3-85ed-3ddf291f1c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab27a8f0-cb65-4611-88a0-792e06a1d56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ex = train_dataset[0]['pixels']\n",
    "print(ex.shape)\n",
    "print(torch.min(ex), torch.max(ex))\n",
    "ex = (ex+1)/2\n",
    "print(torch.min(ex), torch.max(ex))\n",
    "\n",
    "exi = ToPILImage()(ex)\n",
    "plt.imshow(exi)\n",
    "#plt.show()\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd07d14-1a64-4170-9dd9-e89b5866db5c",
   "metadata": {},
   "source": [
    "### Model - Fine-Tuning\n",
    "- weight initialization method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c16c1c9-a5fa-4d57-a5d7-68f64903d946",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ImageNet 1k\n",
    "# model = ViTForImageClassification.from_pretrained(model_name, device_map=device)\n",
    "#The google/vit-base-patch16-224 model is originally fine tuned on imagenet-1K with 1000 output classes\n",
    "\n",
    "# Fine-tune vit model using CIFAR10 dataset\n",
    "# 10개 output class로 모델 생성 \n",
    "ft_model = ViTForImageClassification.from_pretrained(model_name, num_labels=10,  ignore_mismatched_sizes=True, \n",
    "                                                     id2label=itos, label2id=stoi).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28906364-d5f3-445e-a68d-29776f2ae10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7bd71c-80dc-47ec-8586-f70a2d66c1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(ft_model.classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d863626d-f06c-4589-8a7c-64a784c7be41",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_model.device, ft_model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c949a4-6922-4e4d-958b-a43facb9b41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(ft_model, input_size=(1, 3, 224, 224))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ccec813-a09a-42d2-9b76-aef7a40b11dc",
   "metadata": {},
   "source": [
    "### Hugging Face Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd51220d-5b5b-4912-acb5-593ad330de9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training hyperparameters\n",
    "batch_size = 32\n",
    "num_train_epochs = 5 # 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a15be2-dd0a-4dbe-b832-24d6a8c0b1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_steps_per_epoch = train_dataset.num_rows//batch_size\n",
    "steps_per_epoch = 200 \n",
    "# steps_per_epoch = max_steps_per_epoch \n",
    "steps_per_epoch, max_steps_per_epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1b924f-5e67-4fd2-9f5b-78cfeddf179a",
   "metadata": {},
   "source": [
    "### [trainer-callbacks](https://huggingface.co/docs/transformers/v4.52.3/en/trainer)\n",
    "- callbacks=[EarlyStoppingCallback()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952cf670-6f80-4f47-9ffd-7fa52acf5618",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    f\"aias-vit-cifar-10\", # output_dir\n",
    "    overwrite_output_dir=True,\n",
    "    # optimizer : optim = adamw_default(default)\n",
    "    learning_rate=2e-5, #adamw\n",
    "    #weight_decay=0.01,\n",
    "    # batch_size\n",
    "    per_device_train_batch_size=batch_size, # \n",
    "    per_device_eval_batch_size=batch_size,  # \n",
    "    #\n",
    "    eval_strategy=\"steps\", #\"epoch\", \n",
    "    eval_steps=steps_per_epoch, # 10 evaluation per this step\n",
    "    logging_steps=steps_per_epoch//2, # 10 \n",
    "    logging_dir='logs',\n",
    "    # num_train_epochs=3, # num_train_epoch or max_steps 둘 중 하나 사용\n",
    "    max_steps=steps_per_epoch * num_train_epochs, # train steps\n",
    "    remove_unused_columns=False,\n",
    "    # saving checkpoints\n",
    "    save_strategy=\"no\", # \"steps\", \"epoch\"\n",
    "    # save_total_limit=1, # keep only the last checkpoint\n",
    "    # load_best_model_at_end=True,\n",
    "    # metric_for_best_model=\"accuracy\",\n",
    ")\n",
    "\n",
    "# examples: single batch\n",
    "def collate_fn(examples):\n",
    "    pixels = torch.stack([example[\"pixels\"] for example in examples])\n",
    "    labels = torch.tensor([example[\"label\"] for example in examples])\n",
    "    return {\"pixel_values\": pixels, \"labels\": labels}\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return dict(accuracy=accuracy_score(predictions, labels))\n",
    "\n",
    "trainer = Trainer(\n",
    "    ft_model,\n",
    "    args, # TrainingArguments\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=collate_fn,\n",
    "    compute_metrics=compute_metrics,\n",
    "    # tokenizer=processor, # preprocessor\n",
    "    processing_class=processor, # preprocessor\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5976ad1e-7ab3-4cce-8302-cf34a1637460",
   "metadata": {},
   "outputs": [],
   "source": [
    "#?trainer.compute_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de27e80-11a2-480a-a894-06b2062be778",
   "metadata": {},
   "outputs": [],
   "source": [
    "#args"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60eb9cd1-f33e-4441-aa9d-c5deaff93be4",
   "metadata": {},
   "source": [
    "### Training the model for fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e172178-4392-4f61-a015-aa01d8c5f5f2",
   "metadata": {},
   "source": [
    " ## 1. last-layer 만 먼저 fine-tuning하고 싶다면, 해당 layer만 trainable로 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b5ea5b-56aa-4b6e-bcc9-b9c6deb0541a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    param.requires_grad = False\n",
    "for name, param in model.classifier.named_parameters():\n",
    "    param.requires_grad = True\n",
    "    \n",
    "# 각 파라미터 이름, shape, trainable 여부 출력\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name:55} | shape: {str(param.shape):28} | trainable: {param.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f12a4b-481e-4bc4-b0ba-e2aac998bf29",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "start_time = datetime.now()\n",
    "result = trainer.train()\n",
    "end_time = datetime.now()\n",
    "elapsed_time = end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb42e19c-c1de-45d7-bfd5-7a3015bfb341",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. full-train(default), 전체 layer의 parameters를 trainable로 변경하여 전체 weight 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea74f0a4-8b76-437e-962f-0ecc8355eab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name, param in model.named_parameters():\n",
    "#     param.requires_grad = True\n",
    "\n",
    "# for name, param in model.named_parameters():\n",
    "#     print(f\"{name:55} | shape: {str(param.shape):28} | trainable: {param.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4739d38d-35ba-4c27-ad6b-2ac40a2a1414",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# start_time = datetime.now()\n",
    "# result2 = trainer.train()\n",
    "# end_time = datetime.now()\n",
    "# elapsed_time = end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f22b4f-75be-456a-93b2-63e270fa0f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'training time: {str(elapsed_time)} sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63941b19-39c1-4516-800a-1a5009dcc758",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_result(result):\n",
    "    print(f\"Time: {result.metrics['train_runtime']:.2f}\")\n",
    "    print(f\"Samples/second: {result.metrics['train_samples_per_second']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59522db-ffdd-4507-8173-11267f592240",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_result(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a0df13-1b3e-4623-b3b7-b9ce2bc3082e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.state.log_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7303073-61a0-4967-8527-97f4adf6346e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer.state.log_history[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084ab11a-b089-483b-8346-adb5a219ac94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# history\n",
    "trainer.state.log_history\n",
    "import pandas as pd\n",
    "log_history = pd.DataFrame(trainer.state.log_history[:-1]) # to get log history except train results\n",
    "log_history1 = log_history[log_history['loss'].notna()].dropna(axis=1, how='any')\n",
    "log_history2 = log_history[log_history['eval_loss'].notna()].dropna(axis=1, how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479f846f-ccca-46b6-93e6-a3f3e553b0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# log_history1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d434a36-d65d-4000-8b7e-3b4805ad9a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# log_history2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642de54e-6de7-4f12-b2b1-0d2a7e854e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_history1[['step', 'loss']].plot(x='step', y='loss')\n",
    "plt.title('train loss vs steps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec15526-2b65-486f-bca9-dc54d5ccc5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_history2[['step', 'eval_loss']].plot(x='step', y='eval_loss' )\n",
    "log_history2[['step', 'eval_accuracy']].plot(x='step', y='eval_accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca595e4-b69b-4cf6-979f-da74fb5e2f86",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960ced4a-d41a-4a5a-bb8f-067595b4acec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation using trainer.predict API\n",
    "# [NOTE] there are labels informations in the test_dataset\n",
    "start_time = datetime.now()\n",
    "outputs = trainer.predict(test_dataset)\n",
    "end_time = datetime.now()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Accuracy at test dataset: {outputs.metrics['test_accuracy']}\")\n",
    "print(f\"Processing time to evaluate test dataset: {elapsed_time} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56060f47-d19a-4efb-ac11-16b5741b544f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# outputs에 대해 알아보기\n",
    "print(f'accuracy={outputs.metrics['test_accuracy']}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f62bcf8-3743-4eec-85e8-d292a8f5396d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(outputs.metrics) \n",
    "outputs.predictions.shape,outputs.label_ids.shape \n",
    "#dir(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96055b21-d6f6-4adf-9b2d-450afd56609d",
   "metadata": {},
   "source": [
    "### inference results for the selected index in the test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e42326-c918-4563-977c-524d95cae711",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 10\n",
    "ex = test_dataset[idx]['pixels']\n",
    "ex = (ex+1)/2\n",
    "\n",
    "exi = ToPILImage()(ex)\n",
    "display(exi)\n",
    "print(f'predicted: {itos[np.argmax(outputs.predictions[idx])]}, ground truth: {itos[outputs.label_ids[idx]]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570ece9b-fcf1-4a09-b826-dfb0148430ab",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68dce1db-5919-459e-91b7-b57f59dfcf7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = outputs.label_ids\n",
    "y_pred = outputs.predictions.argmax(1)\n",
    "\n",
    "labels = train_dataset.features['label'].names\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "disp.plot(xticks_rotation=45)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c486f2-6987-4acb-a555-dbf9e41e3188",
   "metadata": {},
   "source": [
    "## Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11439211-32bf-49e2-9e6a-23d0e23214e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir=\"vit-cifar10\"\n",
    "trainer.save_model(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7641f4-5dab-48f0-a417-45d71a2061c2",
   "metadata": {},
   "source": [
    "### Load the model and verify it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc467e1-0e22-47af-9441-842b474c3051",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor, AutoModelForImageClassification\n",
    "re_processor = AutoImageProcessor.from_pretrained(output_dir, use_fast=True)\n",
    "re_model = AutoModelForImageClassification.from_pretrained(output_dir, device_map=device)\n",
    "print(re_model.classifier) #The google/vit-base-patch16-224 model is originally fine tuned on imagenet-1K with 1000 output classes\n",
    "# or\n",
    "#re_model2 = ViTForImageClassification.from_pretrained(output_dir, device_map=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7e134c-c724-4b6b-9ab2-6b3c420a9ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "re_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc08663-a809-4536-828f-bb7eddb47cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "re_processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a32e299-7f77-4103-8c53-0e22a7668dd3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
